# Fine-tuning Binary Large Language Models

## Overview
This project investigates the fine-tuning of Large Language Models (LLMs) that have been converted from floating-point representations to a ternary network structure, where all parameters are restricted to the set $\{-1, 0, 1\}$.

## Goals
- **Efficiency:** Drastically reduce memory usage and computational cost for both training and inference.
- **Performance:** Maintain model quality despite the extreme quantization.
- **Methodology:** Develop and validate algorithms for effective fine-tuning of these discrete parameter networks.

## Structure
- `paper.tex`: The main LaTeX source file for the research paper.
