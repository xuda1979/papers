\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{geometry}
\geometry{margin=1in}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}

\title{Off-Policy Group Relative Policy Optimization: \\
Sample-Efficient Reinforcement Learning Through Experience Replay}

\author{Research Paper}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Sample efficiency is the critical bottleneck in reinforcement learning from verifiable rewards (RLVR), where rollout generation dominates computational costs. While Group Relative Policy Optimization (GRPO) eliminates the critic network overhead, its on-policy nature discards valuable experience after each update. In this paper, we present Off-Policy GRPO, a principled extension that enables sample reuse through importance-weighted experience replay. We derive theoretical bounds on policy improvement under off-policy corrections, analyze the optimal clipping threshold for importance weights, and establish sample complexity reductions proportional to replay buffer utilization. Our framework addresses key practical challenges including staleness management in distributed settings, memory-efficient buffer compression, and adaptive replay ratios. We prove that Off-Policy GRPO achieves multiplicative sample efficiency gains while maintaining GRPO's core advantage of critic-free operation, making it particularly suited for compute-constrained RLVR deployments.
\end{abstract}

\section{Introduction}

In reinforcement learning from verifiable rewards (RLVR), the computational cost is dominated by rollout generationâ€”sampling completions from large language models and evaluating them against verifiers. On-policy methods like standard GRPO discard these expensive samples after a single gradient update, leading to poor sample efficiency.

This inefficiency is particularly problematic in compute-constrained settings:
\begin{itemize}
    \item Generating a single completion from a 7B model requires significant GPU time
    \item Verification (compilation, test execution, proof checking) adds latency
    \item Distributed training amplifies communication overhead for fresh samples
\end{itemize}

\textbf{Key insight}: Off-policy methods can reuse samples multiple times, amortizing the generation cost across many gradient updates.

\subsection{Challenges in Off-Policy RLVR}

Extending GRPO to off-policy settings introduces several challenges:

\begin{enumerate}
    \item \textbf{Distribution shift}: Samples from old policies may poorly represent the current policy's behavior.
    \item \textbf{Importance weight variance}: Correcting for distribution shift via importance sampling can introduce high variance.
    \item \textbf{Staleness}: In distributed settings, samples may become outdated before use.
    \item \textbf{Memory management}: Storing completions requires significant memory.
\end{enumerate}

\subsection{Contributions}

\begin{enumerate}
    \item Principled derivation of Off-Policy GRPO with importance-weighted corrections
    \item Theoretical analysis including:
    \begin{itemize}
        \item Policy improvement bounds under bounded KL divergence
        \item Optimal clipping threshold derivation
        \item Sample complexity analysis with replay
    \end{itemize}
    \item Practical algorithms for compute-constrained deployments
    \item Guidelines for buffer management and staleness handling
\end{enumerate}

\section{Background}

\subsection{On-Policy GRPO}

Standard GRPO samples a group of completions from the current policy and computes normalized advantages:

\begin{equation}
    L_{\text{GRPO}}(\theta) = -\mathbb{E}_{x \sim \mathcal{D}, \{y_i\} \sim \pi_{\theta_{\text{old}}}}\left[\sum_{i=1}^G \hat{A}_i \log \pi_\theta(y_i | x)\right]
\end{equation}

where $\hat{A}_i = (r_i - \bar{r})/\sigma_r$ is the group-normalized advantage.

\textbf{Limitation}: Samples $\{y_i\}$ are from $\pi_{\theta_{\text{old}}}$, which equals $\pi_\theta$ only at the first gradient step. On-policy methods require fresh samples for each update.

\subsection{Off-Policy Policy Gradient}

Off-policy policy gradient uses importance sampling to correct for distribution mismatch:

\begin{equation}
    \nabla_\theta J(\theta) = \mathbb{E}_{y \sim \mu}\left[\frac{\pi_\theta(y|x)}{\mu(y|x)} A(y) \nabla_\theta \log \pi_\theta(y|x)\right]
\end{equation}

where $\mu$ is the behavior policy that generated the samples.

\section{Off-Policy GRPO}

\subsection{Replay Buffer Formulation}

We maintain a replay buffer $\mathcal{B}$ containing experience tuples:
\begin{equation}
    \mathcal{B} = \{(x_j, y_j, r_j, \mu_j)\}_{j=1}^{|\mathcal{B}|}
\end{equation}
where $\mu_j = \pi_{\theta_j}(y_j | x_j)$ is the log-probability under the behavior policy at collection time.

\subsection{Importance-Weighted GRPO}

\begin{definition}[Off-Policy GRPO Objective]
\begin{equation}
    L_{\text{Off-GRPO}}(\theta) = -\mathbb{E}_{(x,y,r,\mu) \sim \mathcal{B}}\left[\min\left(\rho \hat{A}, \text{clip}(\rho, 1-\epsilon, 1+\epsilon) \hat{A}\right)\right]
\end{equation}
where $\rho = \frac{\pi_\theta(y|x)}{\mu(y|x)}$ is the importance ratio.
\end{definition}

The clipping follows PPO's trust region approach, preventing excessively large updates when importance ratios are extreme.

\subsection{Group-Level Off-Policy Correction}

For GRPO's group-based advantage estimation, we apply corrections at the group level:

\begin{algorithm}[H]
\caption{Off-Policy GRPO with Replay Buffer}
\begin{algorithmic}[1]
\Require Replay buffer $\mathcal{B}$, current policy $\pi_\theta$, clip parameter $\epsilon$
\For{each update step}
    \State Sample batch of prompts $\{x_1, \ldots, x_B\}$
    \For{each prompt $x$}
        \State Retrieve group $\{(y_i, r_i, \mu_i)\}_{i=1}^G$ from buffer for prompt $x$
        \State Compute importance ratios: $\rho_i = \pi_\theta(y_i|x) / \mu_i$
        \State Compute effective sample size: $n_{\text{eff}} = \frac{(\sum_i \rho_i)^2}{\sum_i \rho_i^2}$
        \If{$n_{\text{eff}} < n_{\text{threshold}}$}
            \State Skip this group (too off-policy)
        \EndIf
        \State Compute weighted advantages: $\hat{A}_i = \frac{\rho_i (r_i - \bar{r}_\rho)}{\sigma_{r,\rho}}$
        \State where $\bar{r}_\rho = \frac{\sum_i \rho_i r_i}{\sum_i \rho_i}$
        \State Apply clipping: $\hat{A}_i^{\text{clip}} = \text{clip}(\rho_i, 1-\epsilon, 1+\epsilon) \cdot \hat{A}_i$
        \State Accumulate loss: $L += -\sum_i \hat{A}_i^{\text{clip}} \log \pi_\theta(y_i|x)$
    \EndFor
    \State Update $\theta$ using gradient of $L$
\EndFor
\end{algorithmic}
\end{algorithm}

\section{Theoretical Analysis}

\subsection{Policy Improvement Guarantee}

\begin{theorem}[Off-Policy Policy Improvement Bound]\label{thm:improvement}
Under binary verifiable rewards and bounded KL divergence $D_{\text{KL}}(\pi_\theta \| \mu) \leq \delta$:
\begin{equation}
    J(\pi_{\text{new}}) \geq J(\pi_{\text{old}}) - C\sqrt{\delta}
\end{equation}
where $C$ depends on the reward range and policy smoothness.
\end{theorem}

\begin{proof}
Using the performance difference lemma:
\begin{equation}
    J(\pi_\theta) - J(\mu) = \mathbb{E}_{s \sim d^{\pi_\theta}}\mathbb{E}_{a \sim \pi_\theta}[A^\mu(s,a)]
\end{equation}

The error from using $\mu$-sampled states instead of $\pi_\theta$-sampled states is bounded by the total variation distance, which is bounded by $\sqrt{D_{\text{KL}}/2}$ via Pinsker's inequality:

\begin{equation}
    \|d^{\pi_\theta} - d^\mu\|_1 \leq \sqrt{2 D_{\text{KL}}(\pi_\theta \| \mu)}
\end{equation}

Combined with bounded rewards, this gives the stated bound.
\end{proof}

\subsection{Optimal Clipping Threshold}

\begin{theorem}[Optimal Clipping]\label{thm:clipping}
The optimal clipping threshold $\epsilon^*$ that minimizes mean squared error of the gradient estimator is:
\begin{equation}
    \epsilon^* = \Theta\left(\sqrt{\frac{\text{Var}[r]}{\text{Var}[\rho]}}\right)
\end{equation}
\end{theorem}

\begin{proof}
The MSE of the importance-weighted estimator decomposes as:
\begin{equation}
    \text{MSE} = \text{Bias}^2 + \text{Var}
\end{equation}

Without clipping:
\begin{itemize}
    \item Bias = 0 (unbiased)
    \item Variance = $O(\text{Var}[\rho] \cdot \text{Var}[r \cdot \nabla \log \pi])$
\end{itemize}

With clipping at $\epsilon$:
\begin{itemize}
    \item Bias = $O(\mathbb{E}[|\rho - \text{clip}(\rho)|])$
    \item Variance reduced proportionally
\end{itemize}

Optimizing the bias-variance tradeoff:
\begin{equation}
    \epsilon^* = \arg\min_\epsilon \left[\text{Bias}(\epsilon)^2 + \text{Var}(\epsilon)\right]
\end{equation}

Taking derivatives and solving gives the stated result.
\end{proof}

\subsection{Sample Complexity with Replay}

\begin{theorem}[Sample Complexity Reduction]\label{thm:sample_complexity}
Off-Policy GRPO with replay buffer size $|\mathcal{B}|$ and replay ratio $R$ (gradient steps per new sample) achieves sample complexity:
\begin{equation}
    N_{\text{off-policy}} = \frac{N_{\text{on-policy}}}{R \cdot \eta_{\text{eff}}}
\end{equation}
where $\eta_{\text{eff}} = 1 - O(\delta)$ accounts for off-policy efficiency loss due to KL divergence $\delta$.
\end{theorem}

\begin{proof}
Each sample in the buffer can be reused $R$ times before becoming too stale (KL divergence exceeds threshold).

The effective number of gradient updates per sample is $R$.

However, off-policy updates are slightly less efficient due to importance sampling variance, captured by $\eta_{\text{eff}}$.

Total sample complexity is reduced by factor $R \cdot \eta_{\text{eff}}$.
\end{proof}

\subsection{Staleness Analysis}

\begin{definition}[Staleness]
The staleness of a sample $(x, y, r, \mu)$ with respect to current policy $\pi_\theta$ is:
\begin{equation}
    S = D_{\text{KL}}(\pi_\theta(\cdot|x) \| \mu(\cdot|x))
\end{equation}
\end{definition}

\begin{theorem}[Staleness Growth]\label{thm:staleness}
Under learning rate $\eta$ and gradient magnitude bounded by $G$:
\begin{equation}
    S_t \leq S_0 + t \cdot \eta^2 G^2
\end{equation}
Staleness grows quadratically in learning rate and linearly in time.
\end{theorem}

\section{Importance Weighting Schemes}

We compare several importance weighting approaches:

\begin{table}[h]
\centering
\begin{tabular}{lll}
\toprule
Scheme & Formula & Properties \\
\midrule
Standard IS & $\rho = \pi/\mu$ & Unbiased, high variance \\
Clipped IS & $\text{clip}(\rho, 1-\epsilon, 1+\epsilon)$ & Biased, low variance \\
Self-normalized IS & $\rho / \sum_j \rho_j$ & Lower variance, normalized \\
Per-decision IS & $\prod_t \pi(a_t)/\mu(a_t)$ & Token-level correction \\
V-trace & $\bar{\rho}_t = \min(c, \rho_t)$ & Truncated for stability \\
\bottomrule
\end{tabular}
\caption{Comparison of importance weighting schemes}
\end{table}

\subsection{Recommended: Clipped Self-Normalized IS}

We recommend combining clipping with self-normalization:

\begin{equation}
    w_i = \frac{\text{clip}(\rho_i, 1-\epsilon, 1+\epsilon)}{\sum_j \text{clip}(\rho_j, 1-\epsilon, 1+\epsilon)}
\end{equation}

This provides:
\begin{itemize}
    \item Bounded variance through clipping
    \item Proper probability distribution through normalization
    \item Reduced sensitivity to individual outliers
\end{itemize}

\section{Practical Considerations}

\subsection{Memory Management}

\subsubsection{Buffer Size Selection}

\begin{proposition}
The optimal buffer size balances sample diversity against staleness:
\begin{equation}
    |\mathcal{B}|^* = \Theta\left(\frac{R}{\eta^2 G^2} \cdot \delta_{\max}\right)
\end{equation}
where $R$ is target replay ratio and $\delta_{\max}$ is maximum acceptable KL divergence.
\end{proposition}

\subsubsection{Compression Strategies}

For memory-limited settings:
\begin{enumerate}
    \item \textbf{Token ID storage}: Store token indices instead of embeddings
    \item \textbf{Log-prob caching}: Store behavior log-probs, not full distributions
    \item \textbf{Priority-based eviction}: Remove oldest or lowest-advantage samples first
\end{enumerate}

\subsection{Distributed Training}

\subsubsection{Architecture Options}

\begin{enumerate}
    \item \textbf{Centralized buffer}: Single buffer, workers push/pull samples
    \begin{itemize}
        \item Pro: Consistent staleness management
        \item Con: Communication bottleneck
    \end{itemize}
    
    \item \textbf{Distributed buffers}: Each worker maintains local buffer
    \begin{itemize}
        \item Pro: Reduced communication
        \item Con: Sample diversity limited to local experience
    \end{itemize}
    
    \item \textbf{Hybrid}: Local buffers with periodic synchronization
    \begin{itemize}
        \item Pro: Balanced tradeoff
        \item Con: Implementation complexity
    \end{itemize}
\end{enumerate}

\subsubsection{Async Experience Collection}

\begin{algorithm}[H]
\caption{Async Off-Policy GRPO}
\begin{algorithmic}[1]
\State \textbf{Collector workers}: Generate samples, push to buffer
\State \textbf{Trainer workers}: Sample from buffer, compute gradients
\State \textbf{Synchronization}: Periodic parameter broadcast to collectors
\end{algorithmic}
\end{algorithm}

\subsection{Adaptive Replay Ratio}

\begin{proposition}[Adaptive Replay]
The optimal replay ratio $R^*$ decreases as training progresses:
\begin{equation}
    R^*(t) = R_0 \cdot \exp\left(-\frac{t}{\tau}\right)
\end{equation}
where $\tau$ controls the decay rate.
\end{proposition}

Intuition: Early in training, the policy changes slowly, so samples remain valid longer. Late in training, the policy is more refined and sensitive to distribution shift.

\section{Algorithm Summary}

\begin{algorithm}[H]
\caption{Complete Off-Policy GRPO}
\begin{algorithmic}[1]
\Require Initial policy $\pi_\theta$, buffer capacity $C$, clip $\epsilon$, staleness threshold $\delta_{\max}$
\State Initialize replay buffer $\mathcal{B} \leftarrow \emptyset$
\For{iteration $t = 1, 2, \ldots$}
    \State \textbf{Collection phase:}
    \For{each prompt $x$}
        \State Sample $\{y_i\}_{i=1}^G \sim \pi_\theta(\cdot|x)$
        \State Compute rewards $r_i = \text{Verifier}(x, y_i)$
        \State Store $\{(x, y_i, r_i, \pi_\theta(y_i|x))\}$ in $\mathcal{B}$
    \EndFor
    \State \textbf{Training phase:}
    \For{replay step $k = 1, \ldots, R$}
        \State Sample batch from $\mathcal{B}$
        \State Filter samples with $D_{\text{KL}} > \delta_{\max}$
        \State Compute importance-weighted GRPO loss
        \State Update $\theta$
    \EndFor
    \State \textbf{Buffer maintenance:}
    \State Evict samples exceeding staleness threshold
    \If{$|\mathcal{B}| > C$}
        \State Evict lowest-priority samples
    \EndIf
\EndFor
\end{algorithmic}
\end{algorithm}

\section{Experimental Framework}

\subsection{Efficiency Metrics}

\begin{table}[h]
\centering
\begin{tabular}{ll}
\toprule
Metric & Description \\
\midrule
Samples-to-convergence & Total samples needed to reach target performance \\
Wall-clock time & Real time including generation and training \\
GPU hours & Total compute resources consumed \\
Replay ratio & Gradient steps per new sample \\
Effective sample size & $n_{\text{eff}}$ accounting for importance weights \\
\bottomrule
\end{tabular}
\caption{Efficiency metrics for Off-Policy GRPO evaluation}
\end{table}

\subsection{Comparison Methods}

\begin{enumerate}
    \item \textbf{On-Policy GRPO}: Baseline, no replay
    \item \textbf{Off-Policy GRPO (Ours)}: With clipped self-normalized IS
    \item \textbf{PPO with replay}: Actor-critic baseline
    \item \textbf{V-trace}: Alternative off-policy correction
\end{enumerate}

\section{Discussion}

\subsection{When Off-Policy Helps Most}

Off-Policy GRPO is most beneficial when:
\begin{itemize}
    \item Generation cost dominates (large models)
    \item Verification is expensive (complex tests, theorem proving)
    \item Compute is limited but memory is available
    \item Training is distributed with communication overhead
\end{itemize}

\subsection{Limitations}

\begin{itemize}
    \item Memory overhead for buffer storage
    \item Hyperparameter sensitivity (clip threshold, staleness)
    \item Reduced effectiveness for rapidly changing policies
    \item Implementation complexity in distributed settings
\end{itemize}

\section{Conclusion}

We have presented Off-Policy GRPO, a principled approach to sample-efficient reinforcement learning from verifiable rewards. By enabling experience replay with importance-weighted corrections, our method achieves multiplicative sample efficiency gains while preserving GRPO's critic-free advantages. Our theoretical analysis establishes conditions for policy improvement, derives optimal clipping thresholds, and quantifies sample complexity reductions. The practical algorithms and guidelines we provide enable effective deployment in compute-constrained settings, making advanced RLVR techniques accessible to a broader range of practitioners and applications.

\bibliographystyle{plainnat}
\begin{thebibliography}{10}

\bibitem[DeepSeek-AI(2024)]{deepseekmath2024}
DeepSeek-AI.
\newblock DeepSeekMath: Pushing the limits of mathematical reasoning in open language models.
\newblock \emph{arXiv preprint arXiv:2402.03300}, 2024.

\bibitem[Schulman et al.(2017)]{ppo2017}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Munos et al.(2016)]{vtrace2016}
Munos, R., Stepleton, T., Harutyunyan, A., and Bellemare, M.
\newblock Safe and efficient off-policy reinforcement learning.
\newblock \emph{NeurIPS}, 2016.

\bibitem[Espeholt et al.(2018)]{impala2018}
Espeholt, L., et al.
\newblock IMPALA: Scalable distributed deep-rl with importance weighted actor-learner architectures.
\newblock \emph{ICML}, 2018.

\end{thebibliography}

\end{document}
