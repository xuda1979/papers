\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{geometry}
\geometry{margin=1in}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}

\title{Stabilizing GRPO Without a Reference Model: \\
Reference-Free Regularization for Critic-Free Reinforcement Learning}

\author{Research Paper}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Group Relative Policy Optimization (GRPO) achieves competitive performance without a critic network, but standard implementations rely on KL regularization against a frozen reference model---introducing memory overhead and hyperparameter sensitivity. In this paper, we present Stable GRPO (S-GRPO), a family of reference-free stabilization techniques that maintain training stability without requiring a reference model. We identify three key instability sources in GRPO: token-level gradient conflicts between successful and failed completions, policy collapse to narrow solution modes, and unreliable signals from high-entropy completions. Our proposed solutions include: (1) positive-only gradient updates that skip negative examples, (2) entropy floor constraints that prevent collapse without KL penalties, and (3) adaptive token conflict detection that down-weights ambiguous gradients. We derive theoretical conditions under which these techniques guarantee convergence and establish entropy floor thresholds sufficient to prevent degenerate solutions. Our approach reduces memory footprint by eliminating the reference model while maintaining or improving training stability.
\end{abstract}

\section{Introduction}

GRPO's critic-free design provides significant computational advantages, but practical implementations typically include KL regularization against a reference model:

\begin{equation}
    L_{\text{GRPO-KL}}(\theta) = L_{\text{GRPO}}(\theta) + \beta \cdot D_{\text{KL}}(\pi_\theta \| \pi_{\text{ref}})
\end{equation}

This introduces several costs:
\begin{itemize}
    \item \textbf{Memory}: Storing a frozen reference model (effectively doubling memory for policy)
    \item \textbf{Hyperparameter tuning}: The KL coefficient $\beta$ is sensitive and task-dependent
    \item \textbf{Computation}: Forward passes through reference model for KL computation
\end{itemize}

\textbf{Goal}: Achieve stable GRPO training \emph{without any reference model dependency}.

\subsection{Instability Sources in GRPO}

Recent work (GTPO) identifies key instability sources:

\begin{enumerate}
    \item \textbf{Token-level penalization}: Shared tokens appearing in both successful and failed completions receive conflicting gradients.
    
    \item \textbf{Policy collapse}: Without regularization, the policy can collapse to a narrow set of solutions, losing diversity.
    
    \item \textbf{High-entropy noise}: Completions generated with high entropy are often low-quality and provide unreliable training signals.
\end{enumerate}

\subsection{Contributions}

\begin{enumerate}
    \item Formal analysis of GRPO instability sources
    \item Three reference-free stabilization techniques:
    \begin{itemize}
        \item Positive-only gradient updates
        \item Entropy floor constraints
        \item Token conflict detection and resolution
    \end{itemize}
    \item Theoretical guarantees on convergence and collapse prevention
    \item Practical S-GRPO algorithm with reduced memory footprint
\end{enumerate}

\section{Background: GTPO and Instability Analysis}

\subsection{GTPO Insights}

Group-relative Trajectory-based Policy Optimization (GTPO) addresses GRPO instabilities through:

\begin{enumerate}
    \item \textbf{Skip negative updates}: Only amplify successful completions, don't suppress failures
    \item \textbf{Filter high-entropy completions}: Remove unreliable signals
    \item \textbf{No KL regularization}: Achieves stability without reference model
\end{enumerate}

\subsection{Token-Level Conflict Problem}

Consider a prompt where both successful and failed completions share a common prefix:

\begin{align}
    y^+ &= \text{"Let } x = 5\text{. Then } x + 3 = 8\text{."} \quad (r = 1) \\
    y^- &= \text{"Let } x = 5\text{. Then } x + 3 = 7\text{."} \quad (r = 0)
\end{align}

The shared prefix "Let $x = 5$" receives:
\begin{itemize}
    \item Positive gradient from $y^+$ (increase probability)
    \item Negative gradient from $y^-$ (decrease probability)
\end{itemize}

These conflicting signals cancel out or introduce noise.

\begin{definition}[Token Conflict]
Token $t$ at position $i$ has conflict if:
\begin{equation}
    \exists y^+, y^- : t = y^+_i = y^-_i \text{ and } r(y^+) = 1, r(y^-) = 0
\end{equation}
\end{definition}

\section{Proposed Methods}

\subsection{Method 1: Positive-Only Gradient Updates}

\begin{definition}[Positive-Only GRPO]
\begin{equation}
    L_{\text{PO-GRPO}}(\theta) = -\mathbb{E}_{x, \{y_i\}}\left[\sum_{i: r_i = 1} \hat{A}_i^+ \log \pi_\theta(y_i | x)\right]
\end{equation}
Only successful completions contribute to the loss.
\end{definition}

\begin{theorem}[Positive-Only Convergence]\label{thm:po_convergence}
Under binary rewards with success probability $p > 0$, positive-only GRPO converges to a policy that maximizes expected reward if:
\begin{equation}
    \eta \leq \frac{p}{(1-p) \cdot L}
\end{equation}
where $L$ is the policy's smoothness constant.
\end{theorem}

\begin{proof}
The positive-only gradient is:
\begin{equation}
    g_{\text{PO}} = \sum_{i: r_i = 1} \hat{A}_i^+ \nabla_\theta \log \pi_\theta(y_i | x)
\end{equation}

Taking expectation:
\begin{align}
    \mathbb{E}[g_{\text{PO}}] &= \mathbb{E}_{y \sim \pi_\theta}\left[\mathbf{1}[r(y) = 1] \cdot \hat{A}^+ \cdot \nabla \log \pi_\theta(y)\right] \\
    &= p \cdot \hat{A}^+ \cdot \nabla_\theta \log \pi_\theta(\mathcal{Y}^+ | x)
\end{align}

where $\mathcal{Y}^+$ is the set of correct completions.

This is a scaled version of the policy gradient for maximizing $\pi_\theta(\mathcal{Y}^+ | x)$.

The learning rate condition ensures the update doesn't overshoot, accounting for the asymmetric gradient (only positives).
\end{proof}

\subsubsection{When Positive-Only Suffices}

\begin{proposition}
Positive-only GRPO achieves the same asymptotic performance as standard GRPO when:
\begin{enumerate}
    \item Success rate $p$ is not too small ($p > 1/G$)
    \item Positive and negative completions are distinguishable (low overlap)
    \item The policy has sufficient capacity
\end{enumerate}
\end{proposition}

\subsection{Method 2: Entropy Floor Constraints}

\begin{definition}[Entropy Floor Regularization]
\begin{equation}
    L_{\text{EF}}(\theta) = L_{\text{GRPO}}(\theta) + \lambda_H \cdot \max(0, H_{\min} - H(\pi_\theta | x))
\end{equation}
where $H(\pi_\theta | x) = -\sum_y \pi_\theta(y|x) \log \pi_\theta(y|x)$ is the policy entropy.
\end{definition}

\begin{theorem}[Entropy Floor Sufficiency]\label{thm:entropy_floor}
Entropy floor $H_{\min}$ prevents collapse to deterministic policy if:
\begin{equation}
    H_{\min} \geq \log k^*
\end{equation}
where $k^*$ is the number of distinct correct solution types.
\end{theorem}

\begin{proof}
A deterministic policy concentrating on a single output $y^*$ has entropy $H = 0$.

For a policy with support size $k$ and uniform distribution over support:
\begin{equation}
    H = \log k
\end{equation}

If $H_{\min} = \log k^*$, the policy must maintain at least $k^*$ outputs with non-negligible probability.

If all $k^*$ correct solutions are maintained, the policy preserves diversity.
\end{proof}

\subsubsection{Practical Entropy Computation}

For sequence-level entropy (intractable for long sequences), we use a proxy:

\begin{definition}[Per-Token Entropy Proxy]
\begin{equation}
    \hat{H}(\pi_\theta | x) = \frac{1}{T} \sum_{t=1}^T H(\pi_\theta(\cdot | x, y_{<t}))
\end{equation}
Average entropy per token position.
\end{definition}

\subsection{Method 3: Token Conflict Detection}

\begin{definition}[Conflict Score]
For token $t$ at position $i$ in group $\{y_j\}$:
\begin{equation}
    C(t, i) = \min\left(\sum_{j: y_{j,i} = t, r_j = 1} 1, \sum_{j: y_{j,i} = t, r_j = 0} 1\right)
\end{equation}
Number of times $t$ appears in both successful and failed completions at position $i$.
\end{definition}

\begin{definition}[Conflict-Weighted Gradient]
\begin{equation}
    w_{\text{conflict}}(t, i) = \exp(-\gamma \cdot C(t, i))
\end{equation}
Tokens with high conflict receive lower weight.
\end{definition}

\begin{algorithm}[H]
\caption{Token Conflict Resolution}
\begin{algorithmic}[1]
\Require Group $\{(y_j, r_j)\}_{j=1}^G$, conflict discount $\gamma$
\For{each position $i$}
    \State $\text{tokens}_+ \leftarrow \{y_{j,i} : r_j = 1\}$
    \State $\text{tokens}_- \leftarrow \{y_{j,i} : r_j = 0\}$
    \State $\text{shared} \leftarrow \text{tokens}_+ \cap \text{tokens}_-$
    \For{each token $t \in \text{shared}$}
        \State $C(t, i) \leftarrow \min(|\{j : y_{j,i} = t, r_j = 1\}|, |\{j : y_{j,i} = t, r_j = 0\}|)$
        \State $w(t, i) \leftarrow \exp(-\gamma \cdot C(t, i))$
    \EndFor
\EndFor
\State Apply weights to gradient computation
\end{algorithmic}
\end{algorithm}

\subsection{Alternative Resolution Strategies}

\begin{table}[h]
\centering
\begin{tabular}{lll}
\toprule
Strategy & Description & Pros/Cons \\
\midrule
Skip & Don't update conflicting tokens & Simple, may lose signal \\
Weight & Weight by $P(\text{success}|\text{token})$ & Principled, requires estimation \\
Causal & Counterfactual analysis & Accurate, expensive \\
Aggregate & Majority vote & Robust, requires large groups \\
\bottomrule
\end{tabular}
\caption{Conflict resolution strategies}
\end{table}

\section{Stable GRPO (S-GRPO) Algorithm}

Combining all three techniques:

\begin{algorithm}[H]
\caption{S-GRPO: Stable GRPO without Reference Model}
\begin{algorithmic}[1]
\Require Prompts $\mathcal{D}$, policy $\pi_\theta$, group size $G$
\Require Entropy threshold $H_{\text{threshold}}$, entropy floor $H_{\min}$
\Require Conflict discount $\gamma$, positive-only flag $\text{po}$
\For{each prompt $x \in \mathcal{D}$}
    \State \textbf{// Sample group}
    \State $\{y_i\}_{i=1}^G \sim \pi_\theta(\cdot | x)$
    
    \State \textbf{// Compute per-completion entropy}
    \For{each $y_i$}
        \State $H_i \leftarrow -\frac{1}{|y_i|}\sum_t \sum_v \pi_\theta(v|x, y_{i,<t}) \log \pi_\theta(v|x, y_{i,<t})$
    \EndFor
    
    \State \textbf{// Filter high-entropy completions}
    \State $\text{valid} \leftarrow \{i : H_i < H_{\text{threshold}}\}$
    \If{$|\text{valid}| < 2$}
        \State \textbf{continue} \Comment{Skip degenerate groups}
    \EndIf
    
    \State \textbf{// Compute rewards}
    \For{each $i \in \text{valid}$}
        \State $r_i \leftarrow \text{Verifier}(x, y_i)$
    \EndFor
    
    \State \textbf{// Detect token conflicts}
    \For{each position $t$}
        \State Compute conflict scores $C(y_{i,t}, t)$ for all $i$
        \State Compute weights $w(y_{i,t}, t) \leftarrow \exp(-\gamma \cdot C)$
    \EndFor
    
    \State \textbf{// Compute advantages}
    \State $\bar{r} \leftarrow \text{mean}(\{r_i : i \in \text{valid}\})$
    \State $\sigma_r \leftarrow \text{std}(\{r_i : i \in \text{valid}\})$
    \For{each $i \in \text{valid}$}
        \State $\hat{A}_i \leftarrow (r_i - \bar{r}) / (\sigma_r + \epsilon)$
    \EndFor
    
    \State \textbf{// Compute loss}
    \If{$\text{po}$}
        \State $L \leftarrow -\sum_{i: r_i > 0} \hat{A}_i \sum_t w(y_{i,t}, t) \log \pi_\theta(y_{i,t} | x, y_{i,<t})$
    \Else
        \State $L \leftarrow -\sum_i \hat{A}_i \sum_t w(y_{i,t}, t) \log \pi_\theta(y_{i,t} | x, y_{i,<t})$
    \EndIf
    
    \State \textbf{// Entropy floor regularization}
    \State $H_{\text{policy}} \leftarrow \text{mean}(\{H_i\})$
    \State $L \leftarrow L + \lambda_H \cdot \max(0, H_{\min} - H_{\text{policy}})$
    
    \State \textbf{// Update}
    \State $\theta \leftarrow \theta - \eta \nabla_\theta L$
\EndFor
\end{algorithmic}
\end{algorithm}

\section{Theoretical Analysis}

\subsection{Entropy Threshold Derivation}

\begin{theorem}[Optimal Entropy Threshold]\label{thm:entropy_threshold}
The optimal entropy threshold for filtering unreliable completions is:
\begin{equation}
    H_{\text{threshold}} = f(\text{Var}[r], G, |\mathcal{V}|) = \bar{H} + \alpha \cdot \sigma_H
\end{equation}
where $\bar{H}$ and $\sigma_H$ are mean and std of per-completion entropy, and $\alpha \approx 1.5$ provides good filtering.
\end{theorem}

\begin{proof}[Proof Sketch]
High-entropy completions correlate with:
\begin{enumerate}
    \item Model uncertainty (low confidence)
    \item Exploration mode (random sampling artifacts)
    \item Out-of-distribution inputs
\end{enumerate}

Empirically, completions with entropy more than 1.5 standard deviations above mean have lower correlation between policy log-prob and reward.

The threshold adapts to the current policy's entropy distribution.
\end{proof}

\subsection{Convergence with Reference-Free Regularization}

\begin{theorem}[S-GRPO Convergence]\label{thm:sgrpo_convergence}
S-GRPO with entropy floor $H_{\min}$ and positive-only updates converges to a local optimum of:
\begin{equation}
    \max_\theta \mathbb{E}[r(x, y)] \quad \text{s.t.} \quad H(\pi_\theta | x) \geq H_{\min}
\end{equation}
at rate $O(1/\sqrt{T})$.
\end{theorem}

\begin{proof}
The entropy floor constraint creates a feasible set $\Theta_H = \{\theta : H(\pi_\theta) \geq H_{\min}\}$.

The positive-only gradient points in a direction that increases expected reward among successful completions.

The entropy regularization projects back onto $\Theta_H$ when the constraint is violated.

Together, these implement projected gradient descent on the constrained problem, with convergence rate inherited from standard SGD analysis.
\end{proof}

\subsection{Comparison: KL vs Entropy Floor}

\begin{table}[h]
\centering
\begin{tabular}{lll}
\toprule
Aspect & KL Regularization & Entropy Floor \\
\midrule
Reference model & Required & Not required \\
Memory & $2\times$ policy size & No overhead \\
Hyperparameter & $\beta$ (sensitive) & $H_{\min}$ (interpretable) \\
Effect & Pull toward reference & Prevent collapse only \\
Flexibility & Anchored to reference & Allows arbitrary diversity \\
\bottomrule
\end{tabular}
\caption{Comparison of regularization approaches}
\end{table}

\section{Experimental Comparison}

\subsection{Methods to Compare}

\begin{enumerate}
    \item \textbf{GRPO}: Standard with KL regularization
    \item \textbf{GTPO}: Skip negatives + entropy filter
    \item \textbf{S-GRPO}: Our full method
    \item \textbf{GRPO-EF}: Entropy floor only
    \item \textbf{GRPO-TC}: Token conflict handling only
    \item \textbf{GRPO-PO}: Positive-only updates only
\end{enumerate}

\subsection{Stability Metrics}

\begin{table}[h]
\centering
\begin{tabular}{ll}
\toprule
Metric & Description \\
\midrule
Gradient variance & $\text{Var}[\nabla L]$ over training \\
Policy entropy & $H(\pi_\theta)$ trajectory \\
Collapse rate & Frequency of entropy dropping below threshold \\
Performance variance & Std of eval metrics across seeds \\
Training stability & Absence of sudden performance drops \\
\bottomrule
\end{tabular}
\caption{Stability evaluation metrics}
\end{table}

\subsection{Ablation Studies}

\begin{enumerate}
    \item \textbf{Entropy threshold}: Vary $H_{\text{threshold}}$ from 0.5× to 2× mean
    \item \textbf{Entropy floor}: Vary $H_{\min}$ from $\log 2$ to $\log 10$
    \item \textbf{Conflict discount}: Vary $\gamma$ from 0.1 to 2.0
    \item \textbf{Positive-only}: Compare with/without negative updates
\end{enumerate}

\section{Discussion}

\subsection{When to Use Reference-Free Methods}

Reference-free stabilization is preferred when:
\begin{itemize}
    \item Memory is constrained (can't store reference model)
    \item Reference model is unavailable or inappropriate
    \item Flexibility to diverge from base model is desired
    \item Simpler hyperparameter tuning is preferred
\end{itemize}

\subsection{Practical Recommendations}

\begin{enumerate}
    \item \textbf{Start with GTPO defaults}: Skip negatives, filter high-entropy
    \item \textbf{Monitor entropy}: Track policy entropy during training
    \item \textbf{Adaptive thresholds}: Adjust entropy threshold based on task difficulty
    \item \textbf{Combine techniques}: Use all three methods for best stability
\end{enumerate}

\subsection{Limitations}

\begin{itemize}
    \item Positive-only may slow convergence when negatives carry useful signal
    \item Entropy floor may be too restrictive for some tasks
    \item Token conflict detection adds computational overhead
    \item Optimal hyperparameters are task-dependent
\end{itemize}

\section{Conclusion}

We have presented S-GRPO, a family of reference-free stabilization techniques for GRPO that eliminate the need for a frozen reference model while maintaining training stability. Our analysis identifies key instability sources---token conflicts, policy collapse, and high-entropy noise---and provides targeted solutions for each. The entropy floor constraint offers an interpretable alternative to KL regularization, positive-only updates simplify gradient computation, and token conflict detection reduces gradient noise. Together, these techniques enable stable GRPO training with reduced memory footprint and fewer hyperparameters, making critic-free RLVR more accessible for compute-constrained settings.

\bibliographystyle{plainnat}
\begin{thebibliography}{10}

\bibitem[DeepSeek-AI(2024)]{deepseekmath2024}
DeepSeek-AI.
\newblock DeepSeekMath: Pushing the limits of mathematical reasoning in open language models.
\newblock \emph{arXiv preprint arXiv:2402.03300}, 2024.

\bibitem[Wang et al.(2024)]{gtpo2024}
Wang, Y., et al.
\newblock GTPO: Fine-tuning-free scalable test-time policy optimization.
\newblock \emph{arXiv preprint arXiv:2508.03772}, 2024.

\bibitem[Schulman et al.(2017)]{ppo2017}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Jaques et al.(2019)]{jaques2019}
Jaques, N., et al.
\newblock Way off-policy batch deep reinforcement learning of implicit human preferences in dialog.
\newblock \emph{arXiv preprint arXiv:1907.00456}, 2019.

\end{thebibliography}

\end{document}
