\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{geometry}
\geometry{margin=1in}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}

\title{TreeRPO: Step-Level Credit Assignment via Tree Sampling \\
Without Training a Reward Model}

\author{Research Paper}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Reinforcement learning from verifiable rewards (RLVR) typically provides only terminal, binary feedback---the answer is correct or incorrect---leaving intermediate reasoning steps without direct training signal. This coarse credit assignment limits learning efficiency, particularly for long-horizon reasoning tasks. We present Tree Relative Policy Optimization (TreeRPO), a method that uses tree-structured sampling to estimate step-level rewards without training a separate reward model. By sampling multiple continuations from intermediate reasoning states, TreeRPO estimates the expected value of each step and provides dense training signals that identify which reasoning steps are pivotal. We derive theoretical bounds on the bias and variance of tree-based value estimates, prove that TreeRPO achieves variance reduction proportional to the branching factor, and establish conditions under which step-level credit improves sample complexity by a factor of $O(H^2)$ where $H$ is the horizon length. Our compute-aware variants introduce adaptive branching, early cutoff via partial verification, and subtree caching for efficient deployment.
\end{abstract}

\section{Introduction}

The credit assignment problem is fundamental in reinforcement learning: when a trajectory succeeds or fails, which actions deserve credit or blame? In RLVR for reasoning tasks, this problem is acute:

\begin{itemize}
    \item A math solution may have 20 steps, but only the final answer is verified
    \item A correct early step followed by a wrong step gets no credit
    \item A wrong early step followed by luck gets full credit
\end{itemize}

Terminal-only rewards lead to:
\begin{enumerate}
    \item High variance gradient estimates
    \item Slow convergence on long-horizon tasks
    \item Difficulty distinguishing good-start-bad-end from bad-start-bad-end
\end{enumerate}

\subsection{TreeRPO Solution}

TreeRPO addresses credit assignment by \textbf{tree sampling}:
\begin{enumerate}
    \item From each intermediate reasoning step, sample multiple continuations
    \item Evaluate all terminal leaves with the verifier
    \item Backpropagate values up the tree to estimate step-level rewards
    \item Use step-level advantages for dense policy gradients
\end{enumerate}

\textbf{Key insight}: The expected future reward from a step can be estimated by Monte Carlo sampling of its subtree---no learned reward model needed.

\subsection{Contributions}

\begin{enumerate}
    \item TreeRPO algorithm for step-level credit assignment via tree sampling
    \item Theoretical analysis:
    \begin{itemize}
        \item Bias bounds for tree-based value estimation
        \item Variance reduction analysis
        \item Sample complexity improvements
    \end{itemize}
    \item Compute-aware variants:
    \begin{itemize}
        \item Adaptive branching factor
        \item Early cutoff with partial verification
        \item Subtree caching for off-policy reuse
    \end{itemize}
\end{enumerate}

\section{Background}

\subsection{Credit Assignment in RL}

The goal of credit assignment is to determine the contribution of each action to the final outcome. Standard approaches:

\begin{enumerate}
    \item \textbf{Monte Carlo}: Use full return, high variance
    \item \textbf{Temporal Difference}: Bootstrap from learned value, introduces bias
    \item \textbf{Advantage Actor-Critic}: Combine value function with policy gradient
\end{enumerate}

In RLVR, we want step-level credit \emph{without} training a separate value network.

\subsection{Tree Search in Reasoning}

Tree search methods explore multiple reasoning paths:
\begin{itemize}
    \item \textbf{Beam search}: Keep top-k hypotheses at each step
    \item \textbf{Monte Carlo Tree Search}: Estimate values via rollouts
    \item \textbf{Best-first search}: Expand most promising nodes
\end{itemize}

TreeRPO uses tree sampling not for search but for \textbf{credit assignment}: the tree structure reveals which steps lead to success.

\section{TreeRPO Method}

\subsection{Tree Structure}

\begin{definition}[Reasoning Tree]
A reasoning tree $\mathcal{T}$ for prompt $x$ is a directed tree where:
\begin{itemize}
    \item Root: Initial prompt $x$
    \item Internal nodes: Partial reasoning states $s_i = (x, y_{1:i})$
    \item Edges: Single reasoning step (token/sentence)
    \item Leaves: Complete responses with verifier labels
\end{itemize}
\end{definition}

\subsection{Tree Sampling}

\begin{algorithm}[H]
\caption{Tree Sampling for Credit Assignment}
\begin{algorithmic}[1]
\Require Prompt $x$, policy $\pi_\theta$, branching factor $B$, max depth $D$
\State Initialize tree: $\mathcal{T} \leftarrow \{x\}$
\State Initialize queue: $Q \leftarrow [(x, 0)]$ \Comment{(node, depth)}
\While{$Q$ not empty}
    \State $(s, d) \leftarrow Q.\text{pop}()$
    \If{$d = D$ or $s$ is complete}
        \State $s.\text{reward} \leftarrow \text{Verifier}(x, s)$
    \Else
        \State Sample $B$ continuations: $\{c_1, \ldots, c_B\} \sim \pi_\theta(\cdot | s)$
        \For{each continuation $c_i$}
            \State $s' \leftarrow s \circ c_i$ \Comment{Extend state}
            \State Add edge $(s, s')$ to $\mathcal{T}$
            \State $Q.\text{push}((s', d+1))$
        \EndFor
    \EndIf
\EndWhile
\State \Return $\mathcal{T}$
\end{algorithmic}
\end{algorithm}

\subsection{Value Backpropagation}

After generating the tree, backpropagate values from leaves to root:

\begin{algorithm}[H]
\caption{Value Backpropagation}
\begin{algorithmic}[1]
\Require Tree $\mathcal{T}$ with leaf rewards
\For{depth $d = D-1$ down to $0$}
    \For{each node $s$ at depth $d$}
        \State $s.\text{value} \leftarrow \frac{1}{|\text{children}(s)|} \sum_{c \in \text{children}(s)} c.\text{value}$
    \EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

\subsection{Step-Level Advantage}

\begin{definition}[Tree-Based Step Advantage]
For step $i$ transitioning from state $s_i$ to $s_{i+1}$:
\begin{equation}
    \hat{A}_i = \hat{V}(s_{i+1}) - \hat{V}(s_i)
\end{equation}
where $\hat{V}(s)$ is the estimated value from tree backpropagation.
\end{definition}

\subsection{TreeRPO Loss}

\begin{equation}
    L_{\text{TreeRPO}}(\theta) = -\mathbb{E}_{\mathcal{T} \sim \pi_\theta}\left[\sum_{s \to s' \in \mathcal{T}} \hat{A}(s \to s') \log \pi_\theta(s' | s)\right]
\end{equation}

Each step transition receives a gradient weighted by its estimated advantage.

\section{Theoretical Analysis}

\subsection{Bias Analysis}

\begin{theorem}[Tree Value Estimation Bias]\label{thm:bias}
Let $V^*(s)$ be the true value of state $s$ under policy $\pi$. The tree-based estimate $\hat{V}(s)$ with branching factor $B$ satisfies:
\begin{equation}
    |\mathbb{E}[\hat{V}(s)] - V^*(s)| \leq O\left(\frac{1}{B}\right) + \epsilon_{\text{policy}}
\end{equation}
where $\epsilon_{\text{policy}}$ accounts for policy shift during tree construction.
\end{theorem}

\begin{proof}
The tree estimate is:
\begin{equation}
    \hat{V}(s) = \frac{1}{B} \sum_{j=1}^B r(\text{leaf}_j)
\end{equation}

where each $\text{leaf}_j$ is reached by sampling from $\pi_\theta$.

By the law of large numbers:
\begin{equation}
    \mathbb{E}[\hat{V}(s)] = \mathbb{E}_{y \sim \pi_\theta(\cdot | s)}[r(y)] = V^{\pi_\theta}(s)
\end{equation}

The bias comes from:
\begin{enumerate}
    \item Finite sample effects: $O(1/B)$
    \item Policy shift: If $\pi_\theta$ changes during tree construction, $\epsilon_{\text{policy}} = O(\|\theta_{\text{end}} - \theta_{\text{start}}\|)$
\end{enumerate}
\end{proof}

\subsection{Variance Analysis}

\begin{theorem}[Variance Reduction]\label{thm:variance}
TreeRPO with branching factor $B$ reduces gradient variance by factor $O(B)$ compared to terminal-only advantage:
\begin{equation}
    \text{Var}[\nabla L_{\text{TreeRPO}}] = \frac{1}{B} \cdot \text{Var}[\nabla L_{\text{terminal}}]
\end{equation}
\end{theorem}

\begin{proof}
For terminal-only advantage with binary reward:
\begin{equation}
    \text{Var}[\hat{A}_{\text{terminal}}] = \text{Var}[r] = p(1-p)
\end{equation}

For tree-based advantage:
\begin{equation}
    \text{Var}[\hat{A}_{\text{tree}}] = \text{Var}\left[\frac{1}{B}\sum_{j=1}^B r_j\right] = \frac{p(1-p)}{B}
\end{equation}

The variance reduction propagates to the gradient variance.
\end{proof}

\subsection{Sample Complexity Improvement}

\begin{theorem}[Sample Complexity with Step-Level Credit]\label{thm:sample_complexity}
For horizon length $H$, TreeRPO achieves sample complexity:
\begin{equation}
    N_{\text{TreeRPO}} = O\left(\frac{1}{(1-\gamma)^2 \epsilon^2}\right)
\end{equation}
compared to terminal-only:
\begin{equation}
    N_{\text{terminal}} = O\left(\frac{H^2}{(1-\gamma)^2 \epsilon^2}\right)
\end{equation}
Improvement factor: $O(H^2)$.
\end{theorem}

\begin{proof}
Terminal-only credit assignment has variance scaling with horizon:
\begin{equation}
    \text{Var}[\hat{A}_{\text{terminal}}] = O(H) \cdot \text{Var}[r]
\end{equation}
because a single terminal reward is attributed to all $H$ steps.

Step-level credit assignment attributes reward to individual steps:
\begin{equation}
    \text{Var}[\hat{A}_{\text{step}}] = O(1) \cdot \text{Var}[r]
\end{equation}

Sample complexity scales with variance, giving the $O(H^2)$ improvement.
\end{proof}

\section{Compute-Aware TreeRPO}

\subsection{Adaptive Branching Factor}

\begin{definition}[Adaptive Branching]
\begin{equation}
    B(s) = f(H(s), \text{Var}[\hat{V}(s)], d(s))
\end{equation}
where $H(s)$ is policy entropy at state $s$, $\text{Var}[\hat{V}(s)]$ is estimated value variance, and $d(s)$ is depth.
\end{definition}

Branch more when:
\begin{itemize}
    \item High entropy: Model is uncertain
    \item High value variance: Step is pivotal
    \item Early depth: Errors compound more
\end{itemize}

\begin{algorithm}[H]
\caption{Adaptive Branching Factor}
\begin{algorithmic}[1]
\Require State $s$, base branching $B_0$, depth $d$, max depth $D$
\State $H \leftarrow -\sum_a \pi(a|s) \log \pi(a|s)$ \Comment{Policy entropy}
\State $\alpha_H \leftarrow H / H_{\max}$ \Comment{Normalized entropy}
\State $\alpha_d \leftarrow 1 - d/D$ \Comment{Depth discount}
\State $B(s) \leftarrow B_0 \cdot (1 + \alpha_H) \cdot (1 + \alpha_d)$
\State \Return $\lfloor B(s) \rfloor$
\end{algorithmic}
\end{algorithm}

\subsection{Early Cutoff via Partial Verification}

For some domains, partial verification is possible:

\begin{itemize}
    \item \textbf{Math}: Check equation validity at each step
    \item \textbf{Code}: Run type checker, linter, partial tests
    \item \textbf{Proofs}: Check tactic validity
\end{itemize}

\begin{definition}[Partial Verifier]
$V_{\text{partial}}(s) \in [0, 1]$: Estimated probability that state $s$ leads to success.
\end{definition}

\begin{algorithm}[H]
\caption{Early Cutoff with Partial Verification}
\begin{algorithmic}[1]
\Require State $s$, partial verifier $V_{\text{partial}}$, threshold $\tau$
\State $p \leftarrow V_{\text{partial}}(s)$
\If{$p < \tau$}
    \State Prune subtree (don't expand)
    \State $s.\text{value} \leftarrow p$ \Comment{Use partial estimate}
\ElsIf{$p > 1 - \tau$}
    \State Prune subtree (high confidence success)
    \State $s.\text{value} \leftarrow p$
\Else
    \State Continue expansion
\EndIf
\end{algorithmic}
\end{algorithm}

\subsection{Subtree Caching}

\begin{definition}[Subtree Cache]
Store computed subtrees for reuse:
\begin{equation}
    \text{Cache}[(s, \pi_\theta)] \leftarrow (\mathcal{T}_s, \hat{V}(s))
\end{equation}
\end{definition}

\textbf{Cache invalidation}: When policy changes significantly:
\begin{equation}
    \text{Invalidate if } D_{\text{KL}}(\pi_{\theta_{\text{new}}} \| \pi_{\theta_{\text{cached}}}) > \delta
\end{equation}

\textbf{Importance weighting}: For cached subtrees with mild policy shift:
\begin{equation}
    \hat{V}_{\text{corrected}}(s) = \frac{\sum_{\text{leaf}} \rho_{\text{leaf}} \cdot r_{\text{leaf}}}{\sum_{\text{leaf}} \rho_{\text{leaf}}}
\end{equation}
where $\rho_{\text{leaf}} = \prod_{t} \frac{\pi_{\theta_{\text{new}}}(a_t | s_t)}{\pi_{\theta_{\text{cached}}}(a_t | s_t)}$.

\section{Efficient TreeRPO Algorithm}

\begin{algorithm}[H]
\caption{E-TreeRPO: Efficient TreeRPO}
\begin{algorithmic}[1]
\Require Prompt $x$, policy $\pi_\theta$, branching schedule $B(\cdot)$
\Require Partial verifier $V_{\text{partial}}$, cache $\mathcal{C}$, cutoff $\tau$
\State \textbf{// Check cache}
\If{$(x, \pi_\theta) \in \mathcal{C}$ and not invalidated}
    \State \Return corrected cached tree
\EndIf
\State \textbf{// Generate tree with adaptive branching and early cutoff}
\State $\mathcal{T} \leftarrow \{x\}$, $Q \leftarrow [(x, 0)]$
\While{$Q$ not empty}
    \State $(s, d) \leftarrow Q.\text{pop}()$
    \State $p \leftarrow V_{\text{partial}}(s)$
    \If{$p < \tau$ or $p > 1 - \tau$}
        \State $s.\text{value} \leftarrow p$ \Comment{Early cutoff}
        \State \textbf{continue}
    \EndIf
    \If{$s$ is complete}
        \State $s.\text{reward} \leftarrow \text{Verifier}(x, s)$
    \Else
        \State $b \leftarrow B(s)$ \Comment{Adaptive branching}
        \State Sample $b$ continuations
        \State Add children to tree and queue
    \EndIf
\EndWhile
\State \textbf{// Backpropagate values}
\State Backpropagate from leaves to root
\State \textbf{// Compute step-level advantages}
\For{each edge $s \to s'$}
    \State $\hat{A}(s \to s') \leftarrow \hat{V}(s') - \hat{V}(s)$
\EndFor
\State \textbf{// Update cache}
\State $\mathcal{C}[(x, \pi_\theta)] \leftarrow (\mathcal{T}, \hat{V}(x))$
\State \Return $\mathcal{T}$, $\{\hat{A}\}$
\end{algorithmic}
\end{algorithm}

\section{When Does TreeRPO Help?}

\begin{table}[h]
\centering
\begin{tabular}{lll}
\toprule
Condition & Terminal-Only & TreeRPO \\
\midrule
Short problems ($H < 5$) & Fine & Overkill \\
Long reasoning ($H > 10$) & Poor & Essential \\
High reward variance & Struggling & Better \\
Low success rate & Very noisy & More signal \\
Partial verification available & No advantage & Significant speedup \\
\bottomrule
\end{tabular}
\caption{When TreeRPO provides benefits over terminal-only credit}
\end{table}

\section{Experimental Framework}

\subsection{Datasets}

\begin{itemize}
    \item \textbf{GSM8K}: Multi-step math (5-10 steps)
    \item \textbf{MATH}: Competition math (10-30 steps)
    \item \textbf{HumanEval}: Code generation (variable length)
    \item \textbf{MiniF2F}: Theorem proving (long proofs)
\end{itemize}

\subsection{Baselines}

\begin{enumerate}
    \item GRPO (terminal-only)
    \item TreeRPO (uniform branching)
    \item E-TreeRPO (adaptive branching + early cutoff)
    \item Process Reward Model (learned step-level rewards)
\end{enumerate}

\subsection{Metrics}

\begin{table}[h]
\centering
\begin{tabular}{ll}
\toprule
Metric & Description \\
\midrule
Pass@1 & Final accuracy \\
Sample efficiency & Samples to reach target accuracy \\
Compute overhead & Additional cost vs terminal-only \\
Advantage quality & Correlation with oracle step rewards \\
Variance reduction & Empirical gradient variance \\
\bottomrule
\end{tabular}
\caption{Evaluation metrics}
\end{table}

\section{Discussion}

\subsection{TreeRPO vs Process Reward Models}

\begin{table}[h]
\centering
\begin{tabular}{lll}
\toprule
Aspect & TreeRPO & Process Reward Model \\
\midrule
Training & No separate training & Requires step-level labels \\
Accuracy & Monte Carlo estimate & Learned approximation \\
Compute & Per-prompt tree cost & One-time training cost \\
Adaptability & Adapts to policy & May not generalize \\
\bottomrule
\end{tabular}
\caption{TreeRPO vs learned process reward models}
\end{table}

\subsection{Practical Recommendations}

\begin{enumerate}
    \item Use TreeRPO for tasks with $H > 10$ reasoning steps
    \item Implement partial verification where possible for efficiency
    \item Start with $B = 4$ branching, adjust based on compute budget
    \item Use caching for iterative training on same prompts
\end{enumerate}

\subsection{Limitations}

\begin{itemize}
    \item Computational overhead scales with branching factor and depth
    \item Partial verification not always available
    \item Cache invalidation may discard useful information
    \item Tree structure assumes decomposable reasoning
\end{itemize}

\section{Conclusion}

We have presented TreeRPO, a method for step-level credit assignment that uses tree-structured sampling to estimate intermediate rewards without training a separate model. Our theoretical analysis establishes bias bounds, variance reduction guarantees, and sample complexity improvements of $O(H^2)$ for long-horizon tasks. The compute-aware variants---adaptive branching, early cutoff, and subtree caching---make TreeRPO practical for resource-constrained settings. TreeRPO provides a principled approach to dense credit assignment that preserves GRPO's critic-free advantages while dramatically improving learning efficiency on multi-step reasoning tasks.

\bibliographystyle{plainnat}
\begin{thebibliography}{10}

\bibitem[DeepSeek-AI(2024)]{deepseekmath2024}
DeepSeek-AI.
\newblock DeepSeekMath: Pushing the limits of mathematical reasoning in open language models.
\newblock \emph{arXiv preprint arXiv:2402.03300}, 2024.

\bibitem[Lightman et al.(2023)]{prm2023}
Lightman, H., et al.
\newblock Let's verify step by step.
\newblock \emph{arXiv preprint arXiv:2305.20050}, 2023.

\bibitem[Silver et al.(2016)]{alphago2016}
Silver, D., et al.
\newblock Mastering the game of Go with deep neural networks and tree search.
\newblock \emph{Nature}, 529(7587):484--489, 2016.

\bibitem[Kocsis \& Szepesv{\'a}ri(2006)]{uct2006}
Kocsis, L. and Szepesv{\'a}ri, C.
\newblock Bandit based Monte-Carlo planning.
\newblock \emph{ECML}, 2006.

\end{thebibliography}

\end{document}
