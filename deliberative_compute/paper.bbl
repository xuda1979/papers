\begin{thebibliography}{10}

\bibitem{cobbe2021gsm8k}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz
  Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
  et~al.
\newblock Training verifiers to solve math word problems.
\newblock {\em arXiv preprint}, 2021.

\bibitem{gittins1979}
John~C Gittins.
\newblock Bandit processes and dynamic allocation indices.
\newblock {\em Journal of the Royal Statistical Society: Series B},
  41(2):148--164, 1979.

\bibitem{hao2023rap}
Shibo Hao, Yi~Gu, Haodi Ma, Joshua~Jiahua Hong, Zhen Wang, Daisy~Zhe Wang, and
  Zhiting Hu.
\newblock Reasoning with language model is planning with world model.
\newblock {\em arXiv preprint}, 2023.

\bibitem{horvitz1989}
Eric~J Horvitz.
\newblock Reasoning under varying and uncertain resource constraints.
\newblock In {\em AAAI Workshop on Limited Rationality}, 1989.

\bibitem{howardmatheson1972}
Ronald~A Howard and James~E Matheson.
\newblock Risk-sensitive markov decision processes.
\newblock {\em Management Science}, 18(7):356--369, 1972.

\bibitem{ji2025survey}
Yixin Ji, Juntao Li, Yang Xiang, Hai Ye, Kaixin Wu, Kai Yao, Jia Xu, Linjian
  Mo, and Min Zhang.
\newblock A survey of test-time compute: From intuitive inference to deliberate
  reasoning.
\newblock {\em arXiv preprint}, 2025.

\bibitem{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon
  Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models.
\newblock {\em arXiv preprint}, 2020.

\bibitem{liedergriffiths2020}
Falk Lieder and Thomas~L Griffiths.
\newblock Resource-rational analysis: Understanding human cognition as the
  optimal use of limited computational resources.
\newblock {\em Behavioral and Brain Sciences}, 43:e1, 2020.

\bibitem{lightman2023prm}
Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy
  Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe.
\newblock Let's verify step by step.
\newblock {\em arXiv preprint}, 2023.

\bibitem{madaan2023selfrefine}
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah
  Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et~al.
\newblock Self-refine: Iterative refinement with self-feedback.
\newblock {\em NeurIPS}, 2023.

\bibitem{nemhauser1978}
George~L Nemhauser, Laurence~A Wolsey, and Marshall~L Fisher.
\newblock An analysis of approximations for maximizing submodular set
  functions.
\newblock {\em Mathematical Programming}, 14(1):265--294, 1978.

\bibitem{russellwefald1991}
Stuart Russell and Eric Wefald.
\newblock {\em Do the Right Thing: Studies in Limited Rationality}.
\newblock MIT Press, 1991.

\bibitem{shinn2023reflexion}
Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu
  Yao.
\newblock Reflexion: Language agents with verbal reinforcement learning.
\newblock {\em NeurIPS}, 2023.

\bibitem{snell2024scaling}
Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar.
\newblock Scaling llm test-time compute optimally can be more effective than
  scaling model parameters.
\newblock {\em arXiv preprint}, 2024.

\bibitem{wald1945sprt}
Abraham Wald.
\newblock Sequential tests of statistical hypotheses.
\newblock {\em The Annals of Mathematical Statistics}, 16(2):117--186, 1945.

\bibitem{wang2024mathshepherd}
Peiyi Wang, Lei Li, Zhihong Shao, R.X. Xu, Damai Dai, Yifei Li, Deli Chen,
  Yu~Wu, and Zhifang Sui.
\newblock Math-shepherd: Verify and reinforce llms step-by-step without human
  annotations.
\newblock {\em ACL}, 2024.

\bibitem{wang2023selfconsistency}
Xuezhi Wang, Jason Wei, Dale Schuurmans, et~al.
\newblock Self-consistency improves chain of thought reasoning in language
  models.
\newblock {\em arXiv preprint}, 2023.

\bibitem{wei2022cot}
Jason Wei, Xuezhi Wang, Dale Schuurmans, et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language
  models.
\newblock {\em arXiv preprint}, 2022.

\bibitem{wu2024inference}
Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang.
\newblock Inference scaling laws: An empirical analysis of compute-optimal
  inference for problem-solving with language models.
\newblock {\em arXiv preprint}, 2024.

\bibitem{yao2023tot}
Shunyu Yao, Dian Yu, Jeffrey Zhao, et~al.
\newblock Tree of thoughts: Deliberate problem solving with large language
  models.
\newblock {\em arXiv preprint}, 2023.

\bibitem{zhou2024lats}
Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang.
\newblock Language agent tree search unifies reasoning acting and planning in
  language models.
\newblock {\em arXiv preprint}, 2024.

\end{thebibliography}
