@article{chen2023frugalgpt,
  title={FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance},
  author={Chen, Lingjiao and Zaharia, Matei and Zou, James},
  journal={arXiv preprint arXiv:2305.05176},
  year={2023}
}

@inproceedings{dao2022flashattention,
  title={FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  author={Dao, Tri and Fu, Daniel Y and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  booktitle={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16344--16359},
  year={2022}
}

@article{frantar2022gptq,
  title={GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2210.17323},
  year={2022}
}

@article{leviathan2023fast,
  title={Fast Inference from Transformers via Speculative Decoding},
  author={Leviathan, Yaniv and Kalman, Matan and Matias, Yossi},
  journal={arXiv preprint arXiv:2211.17192},
  year={2023}
}

@article{pope2022efficiently,
  title={Efficiently Scaling Transformer Inference},
  author={Pope, Reiner and Li, Sholto and Firoozi, Yasmin and Dehghani, Mostafa and Gehrmann, Sebastian and Agrawal, Shrestha and Al-Huraibi, Omar and Depersin, Jonathan and Grygiel, Michal and Kamaev, Teven and others},
  journal={arXiv preprint arXiv:2211.05102},
  year={2022}
}

@inproceedings{sharma2023caching,
  title={Context-Aware Caching for LLM-based Applications},
  author={Sharma, A and Gadiraju, Ujwal and Prakhya, S and Tiwari, V},
  booktitle={Proceedings of the ACM Symposium on Cloud Computing},
  year={2023}
}
