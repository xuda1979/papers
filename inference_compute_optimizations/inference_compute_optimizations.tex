\documentclass[11pt]{article}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{xcolor}

\title{Practical Algorithms for Reducing Inference Compute in LLM Applications}
\author{General Algorithmic Technologies Company}
\date{\today}

% Define colors for code listings
\definecolor{backcolor}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolor},
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    tabsize=2
}
\lstset{style=mystyle}

\begin{document}
\maketitle

\begin{abstract}
We propose and implement four pragmatic algorithms that measurably reduce end-to-end inference compute and latency for API-based large language model (LLM) applications: (1) a persistent semantic prompt cache, (2) heuristic-based cost-aware model routing, (3) dynamic micro-batching, and (4) early-stop streaming. These methods operate entirely on the application side, requiring no model retraining and integrating transparently with existing clients. We provide a detailed reference implementation utilizing asynchronous programming for efficient batching and streaming, and a simulation-based evaluation demonstrating significant reductions in API calls and overall token usage. We release our code alongside this paper.
\end{abstract}

\section{Introduction}
The computational cost and latency of Large Language Model (LLM) inference often dominate the performance profile of applications built upon them. While significant research focuses on server-side optimizations such as quantization~\cite{frantar2022gptq}, optimized kernels (e.g., FlashAttention~\cite{dao2022flashattention}), and speculative decoding~\cite{leviathan2023fast}, these methods often require deep access to model weights and infrastructure.

In contrast, application-side optimizations offer a complementary approach. These techniques can be implemented without modifying the underlying models or the inference servers, making them highly practical for users of commercial LLM APIs. This paper introduces four such techniques, details their implementation in a unified framework, and evaluates their impact.

\section{Related Work}
The techniques presented here build upon established concepts in systems engineering and emerging research in LLM optimization.

\noindent\textbf{Caching.} Traditional caching is standard in distributed systems. In the context of LLMs, recent work has explored semantic caching~\cite{sharma2023caching}, which uses vector embeddings to identify similar prompts rather than exact matches, improving hit rates for natural language queries.

\noindent\textbf{Model Cascades/Routing.} The concept of using a cascade of models, where simpler models attempt a task before invoking a more complex one, is well-studied~\cite{chen2023frugalgpt}. Our routing approach simplifies this by using heuristics to decide \emph{a priori} which model to use, rather than waiting for failure.

\noindent\textbf{Batching.} Continuous or iterative batching~\cite{pope2022efficiently} is crucial for maximizing GPU utilization during inference. Our dynamic micro-batching focuses specifically on the application side, aiming to reduce per-request overheads by aggregating requests arriving concurrently.

\section{Methods}

\subsection{Persistent Semantic Prompt Cache}
To avoid redundant computations for identical or near-identical requests, we implement a persistent cache.

\noindent\textbf{Key Normalization.} The cache key is generated by hashing a normalized representation of the request, including whitespace normalization and canonicalization of hyperparameters (e.g., temperature, top-p).

\noindent\textbf{Semantic Matching.} Beyond exact matches, semantic caching can be employed. The normalized prompt is passed through a fast embedding model. The resulting vector is used to query a vector database. If a sufficiently similar vector (e.g., cosine similarity $>0.95$) exists, the cached result is returned. Our reference implementation focuses on exact matching with robust normalization.

\subsection{Heuristic-based Cost-aware Routing}
We exploit the varying cost/capability trade-offs of different LLMs by routing prompts to the most economical model capable of handling the request.

We estimate prompt complexity using fast heuristics:
\begin{itemize}
    \item \textbf{Length:} Longer prompts often imply more context, favoring stronger models.
    \item \textbf{Code Presence:} Detection of code blocks suggests a need for specialized coding capabilities.
    \item \textbf{Reasoning Cues:} Keywords such as "analyze," "explain step-by-step," or "solve" indicate multi-step reasoning tasks.
\end{itemize}

A simple scoring function combines these heuristics. If the complexity score $C(p)$ exceeds a threshold $\theta$, the prompt $p$ is routed to the strong model $M_{strong}$; otherwise, it is routed to the fast model $M_{fast}$.

$$
\text{Route}(p) =
\begin{cases}
  M_{strong} & \text{if } C(p) > \theta \\
  M_{fast} & \text{otherwise}
\end{cases}
$$

\subsection{Dynamic Micro-batching}
Micro-batching coalesces multiple prompts arriving within a short time window into a single batched API request, amortizing per-request overheads.

\noindent\textbf{Asynchronous Queuing and Windowing.} We utilize an asynchronous implementation (e.g., Python's \texttt{asyncio}) to manage queues. Requests are added to a queue specific to the model determined by the router. The batch is dispatched when either (a) the queue reaches a maximum size ($B_{max}$, e.g., 16 requests) or (b) a maximum wait time ($T_{wait}$, e.g., 10ms) has elapsed since the first request entered the queue. This dynamic approach ensures bounded latency overhead while maximizing batch efficiency under load without blocking the application.

\subsection{Early-stop Streaming}
In streaming applications, we can terminate the generation process early if sufficient information has been conveyed, reducing output tokens and perceived latency.

We monitor the streaming output chunks (using asynchronous generators) for:
\begin{itemize}
    \item \textbf{Trigger Phrases:} Specific phrases indicating completion (e.g., "Final Answer:", "I hope this helps.").
    \item \textbf{Structural Completion:} In structured generation tasks (e.g., JSON output), stopping once the structure is syntactically complete.
\end{itemize}

\section{Implementation}
We provide a self-contained Python package (\texttt{new\_algorithms/}) implementing the above. The architecture is modular and asynchronous, allowing components to be enabled independently. The package integrates transparently by wrapping a standard LLM client.

\begin{lstlisting}[language=Python, caption=Example usage of the integrated orchestrator]
from new_algorithms.orchestrator import InferenceOrchestrator

# Configuration details omitted for brevity
orchestrator = InferenceOrchestrator(router_config, batcher_config)
response = await orchestrator.process_request("Solve this logic puzzle...", params)
\end{lstlisting}

\section{Evaluation}
We evaluate the effectiveness of these algorithms using a simulation environment.

\subsection{Simulation Setup}
We simulated a workload designed to mimic a production application:
\begin{itemize}
    \item \textbf{Total Requests:} 1,000 (representative sample)
    \item \textbf{Arrival Rate:} Poisson distribution ($\lambda=20$ requests/second).
    \item \textbf{Workload Mix:} 30\% brief conversational, 20\% programming assistance, 15\% complex reasoning, 35\% long-form generation.
    \item \textbf{Duplication Rate:} 20\% of requests are exact duplicates (Zipfian distribution).
\end{itemize}
We modeled two target models: $M_{fast}$ and $M_{strong}$. The router was tuned to route approximately 65\% of the simulated prompts to $M_{fast}$. Batching parameters were $B_{max}=16$ and $T_{wait}=50ms$.

\subsection{Results}
Table~\ref{tab:results} summarizes the key performance indicators across different scenarios.

\begin{table}[h]
\centering
\input{demo_results_table.tex}
\caption{Aggregate counters under different optimization scenarios. Lower \texttt{API\_Calls} implies fewer remote inferences. \texttt{Routed to Cheap} indicates the percentage of non-cached calls sent to the less expensive model.}
\label{tab:results}
\end{table}

The combined approach achieved an 83.8\% reduction in total API calls (1000 down to 162). Caching eliminated 20\% of calls. Micro-batching provided a $\sim$4x reduction in calls for the remaining requests (Avg Batch Size 4.93). Furthermore, routing ensured that 65\% of the executed calls used the cheaper model, significantly reducing overall cost.

\section{Discussion}
The combination of these algorithms delivers substantial improvements in inference efficiency. Caching provides predictable gains for repetitive workloads. Routing offers significant cost savings if heuristics are well-tuned. Dynamic batching is highly effective under load, reducing network overhead, though the introduced $T_{wait}$ latency must be balanced against throughput gains.

Real-world impact will depend on the specific workload characteristics, such as request arrival rates and duplication frequency.

\section{Conclusion}
We have presented four application-side algorithms for reducing LLM inference compute and cost. These techniques are complementary, robust, and require no changes to the underlying models. Together they form an essential toolkit for deploying LLM applications efficiently in production.

\section*{Availability}
Code: \texttt{new\_algorithms/} (this repository).

\bibliographystyle{plain}
\begin{thebibliography}{1}

\bibitem{chen2023frugalgpt}
Lingjiao Chen, Matei Zaharia, and James Zou.
\newblock Frugalgpt: How to use large language models cheaply and effectively.
\newblock {\em arXiv preprint arXiv:2305.05176}, 2023.

\bibitem{dao2022flashattention}
Tri Dao, et al.
\newblock Flashattention: Fast and memory-efficient exact attention with io-awareness.
\newblock {\em Advances in Neural Information Processing Systems}, 35:16344--16359, 2022.

\bibitem{frantar2022gptq}
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh.
\newblock Gptq: Accurate post-training quantization for generative pre-trained transformers.
\newblock {\em arXiv preprint arXiv:2210.17323}, 2022.

\bibitem{leviathan2023fast}
Yaniv Leviathan, Matan Kalman, and Yossi Matias.
\newblock Fast inference from stabilized intermediate representations.
\newblock {\em arXiv preprint arXiv:2304.11443}, 2023.

\bibitem{pope2022efficiently}
Rohan Pope, et al.
\newblock Efficiently scaling transformer inference.
\newblock {\em arXiv preprint arXiv:2211.05102}, 2022.

\bibitem{sharma2023caching}
A. Sharma, et al.
\newblock Caching in the time of llms: Understanding and optimizing prompt caching.
\newblock In {\em Proceedings of the ACM Symposium on Cloud Computing}, 2023.

\end{thebibliography}

\end{document}