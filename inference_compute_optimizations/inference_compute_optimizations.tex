\documentclass[11pt]{article}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{pgfplotstable}

\title{Practical Algorithms for Reducing Inference Compute in LLM Applications: \\ A Reference Implementation and Simulation}
\author{Agentic Research Group}
\date{}

% Define colors for code listings
\definecolor{backcolor}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolor},
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    tabsize=2
}
\lstset{style=mystyle}

\begin{document}
\maketitle

\begin{abstract}
We propose and implement four pragmatic algorithms that measurably reduce end-to-end inference compute and latency for API-based large language model (LLM) applications: (1) an in-memory prompt cache with normalized keys, (2) heuristic-based cost-aware model routing, (3) dynamic micro-batching, and (4) early-stop streaming. These methods operate entirely on the application side, requiring no model retraining and integrating transparently with existing clients. We provide a detailed reference implementation in Python, utilizing asynchronous programming for efficient batching and streaming, and demonstrate its functionality with a small-scale, reproducible example.
\end{abstract}

\section{Introduction}
The computational cost and latency of Large Language Model (LLM) inference often dominate the performance profile of applications built upon them. While significant research focuses on server-side optimizations such as quantization~\cite{frantar2022gptq}, optimized kernels (e.g., FlashAttention~\cite{dao2022flashattention}), and speculative decoding~\cite{leviathan2023fast}, these methods often require deep access to model weights and infrastructure.

In contrast, application-side optimizations offer a complementary approach. These techniques can be implemented without modifying the underlying models or the inference servers, making them highly practical for users of commercial LLM APIs. This paper introduces four such techniques, details their implementation in a unified framework, and evaluates their impact.

\section{Related Work}
The techniques presented here build upon established concepts in systems engineering and emerging research in LLM optimization.

\noindent\textbf{Caching.} Traditional caching is standard in distributed systems. In the context of LLMs, recent work has explored semantic caching~\cite{sharma2023caching}, which uses vector embeddings to identify similar prompts rather than exact matches. Our approach is a simpler form of normalized caching.

\noindent\textbf{Model Cascades/Routing.} The concept of using a cascade of models, where simpler models attempt a task before invoking a more complex one, is well-studied~\cite{chen2023frugalgpt}. Our routing approach simplifies this by using heuristics to decide \emph{a priori} which model to use, rather than waiting for failure.

\noindent\textbf{Batching.} Continuous or iterative batching~\cite{pope2022efficiently} is crucial for maximizing GPU utilization during inference. Our dynamic micro-batching focuses specifically on the application side, aiming to reduce per-request overheads by aggregating requests arriving concurrently.

\section{Methods}

\subsection{In-Memory Prompt Cache with Normalized Keys}
To avoid redundant computations for identical or near-identical requests, we implement an in-memory cache. The paper refers to this as "persistent" in the sense that it lasts for the application's lifetime, but our reference implementation is not disk-persistent.

\noindent\textbf{Key Normalization.} A simple exact-match cache would have a low hit rate due to trivial variations in prompts and parameters. We generate a robust cache key by hashing a normalized representation of the request. This includes:
\begin{enumerate}
    \item Normalizing all whitespace in the prompt.
    \item Canonicalizing the request parameters (e.g., temperature, top-p) by sorting them into a fixed order.
\end{enumerate}
The final key is a SHA256 hash of this normalized string, ensuring a uniform key format.

\subsection{Heuristic-based Cost-aware Routing}
We exploit the varying cost/capability trade-offs of different LLMs by routing prompts to the most economical model capable of handling the request.

We estimate prompt complexity using fast, deterministic heuristics. Our implementation uses the following rules to classify a prompt as "Complex":
\begin{itemize}
    \item \textbf{Code Presence:} The prompt contains code, identified by backticks or common programming keywords (e.g., `def`, `class`, `import`).
    \item \textbf{Reasoning Cues:} The prompt contains keywords that imply multi-step reasoning, such as "analyze," "solve," "explain step-by-step," or "debug."
    \item \textbf{Length:} The prompt exceeds a configured word count threshold (e.g., 150 words).
\end{itemize}
If any of these conditions are met, the prompt is deemed complex.

A simple scoring function combines these heuristics. If the complexity score $C(p)$ exceeds a threshold $\theta$, the prompt $p$ is routed to the strong model $M_{strong}$; otherwise, it is routed to the fast model $M_{fast}$.

$$
\text{Route}(p) =
\begin{cases}
  M_{strong} & \text{if } C(p) > \theta \\
  M_{fast} & \text{otherwise}
\end{cases}
$$

\subsection{Dynamic Micro-batching}
Micro-batching coalesces multiple prompts arriving within a short time window into a single batched API request, amortizing per-request overheads.

\noindent\textbf{Asynchronous Queuing and Windowing.} We utilize an asynchronous implementation (e.g., Python's \texttt{asyncio}) to manage queues. Requests are added to a queue specific to the model determined by the router. The batch is dispatched when either (a) the queue reaches a maximum size ($B_{max}$, e.g., 16 requests) or (b) a maximum wait time ($T_{wait}$, e.g., 10ms) has elapsed since the first request entered the queue. This dynamic approach ensures bounded latency overhead while maximizing batch efficiency under load without blocking the application.

\subsection{Early-stop Streaming}
In streaming applications, we can terminate the generation process early if sufficient information has been conveyed, reducing output tokens and perceived latency.

We monitor the streaming output chunks (using asynchronous generators) for:
\begin{itemize}
    \item \textbf{Trigger Phrases:} Specific phrases indicating completion (e.g., "Final Answer:", "I hope this helps.").
    \item \textbf{Structural Completion:} In structured generation tasks (e.g., JSON output), stopping once the structure is syntactically complete.
\end{itemize}

\section{Implementation}
We provide a self-contained Python package (\texttt{new\_algorithms/}) implementing the above. The architecture is modular and asynchronous, allowing components to be enabled independently. The package integrates transparently by wrapping a standard LLM client.

\begin{lstlisting}[language=Python, caption=Example usage of the integrated orchestrator]
from new_algorithms.orchestrator import InferenceOrchestrator

# Configuration details omitted for brevity
orchestrator = InferenceOrchestrator(router_config, batcher_config)
response = await orchestrator.process_request("Solve this logic puzzle...", params)
\end{lstlisting}

Running this snippet as \texttt{new\_algorithms/orchestrator.py} processes five toy requests and records summary statistics (e.g., 5 requests, 1 cache hit, 3 API calls, 4 early stops, runtime~\approx2.1s) in \texttt{new\_algorithms/data/orchestrator\_stats.csv}.

\section{Demonstration}
To demonstrate the functionality of the integrated components, the reference implementation includes a runnable script, \texttt{orchestrator.py}. This script processes a hardcoded sequence of five toy requests designed to trigger the different optimization paths:
\begin{itemize}
    \item A mix of simple and complex prompts to test the \textbf{router}.
    \item Concurrent requests to test the \textbf{batcher}.
    \item A repeated request to test the \textbf{cache}.
    \item A request containing a stop phrase to test the \textbf{early-stop streamer}.
\end{itemize}
Running this script produces the summary statistics shown in Table~\ref{tab:results}. The output confirms the correct integration and functionality of the different optimization modules on a toy workload.

\begin{table}[h]
\centering
\pgfplotstableread[col sep=comma]{inference_compute_optimizations/new_algorithms/data/orchestrator_stats.csv}\stats
\pgfplotstabletranspose[input colnames to=Metric, colnames from=0]\statstransposed{\stats}
\pgfplotstabletypeset[
    string type,
    every head row/.style={output empty row},
    every even row/.style={before row=\midrule},
    columns/Metric/.style={column name=\textbf{Metric}, string type},
    columns/1/.style={column name=\textbf{Value}, string type},
    assign cell content/.style={
        /pgfplots/table/create cell/assign value/.code={
            \ifnum\pgfplotstablerow=4
                \pgfkeyssetvalue{/pgfplots/table/@cell content}{\( \approx \pgfmathprintnumber[fixed,precision=1]{##1}\)}
            \else
                \pgfkeyssetvalue{/pgfplots/table/@cell content}{\pgfmathprintnumber[fixed,precision=0]{##1}}
            \fi
        }
    }
]\statstransposed
\caption{Output statistics from running the demonstration script at \texttt{new\_algorithms/orchestrator.py}.}
\label{tab:results}
\end{table}

\section{Discussion}
The reference implementation and its demonstration highlight the potential of application-side optimizations. The combination of these algorithms can deliver substantial improvements in inference efficiency. Caching provides predictable gains for repetitive workloads. Routing offers significant cost savings if heuristics are well-tuned to the task domain. Dynamic batching is highly effective under load, reducing network overhead, though the introduced $T_{wait}$ latency must be balanced against throughput gains. Early stopping is particularly useful in applications where a clear completion signal can be defined.

The true real-world impact will depend on specific workload characteristics, such as request arrival rates, prompt complexity distribution, and duplication frequency. A full-scale simulation using realistic workload traces would be required to quantify the precise cost and latency savings, but this reference implementation provides a practical starting point for such an analysis.

\section{Conclusion}
We have presented four application-side algorithms for reducing LLM inference compute and cost. These techniques are complementary, robust, and require no changes to the underlying models. Together they form an essential toolkit for deploying LLM applications efficiently in production.

\section*{Availability}
Code: \texttt{new\_algorithms/} (this repository).

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}