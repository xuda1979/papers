\documentclass[11pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{hyperref}

\geometry{margin=1in}

\title{Emergent Social Contracts in Multi-Agent LLM Populations: An Evolutionary Game Theoretic Perspective}
\author{Research Group on Theoretical AI}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
As Large Language Models (LLMs) are increasingly deployed as autonomous agents in decentralized environments, they form nascent digital societies. This paper initiates the study of "Machine Sociology" by investigating the emergence of social norms and cooperation in populations of LLM agents. Using Evolutionary Game Theory as a framework, we simulate high-stakes resource allocation games (the "Alympics") where agents must negotiate via natural language. We observe that unlike classical rational actors, LLM agents exhibit "context-dependent rationality," where their cooperative tendencies can be modulated by "Game-Theoretic Alignment" (GTAlign) prompts. We analyze the stability of these social contracts against adversarial "defector" agents and propose mechanisms for algorithmic governance to ensure robust cooperative equilibria.
\end{abstract}

\section{Introduction}
The transition from single-agent AI to multi-agent systems brings forth complex coordination challenges. When two LLMs negotiate a price or schedule a meeting, they are playing a game with incomplete information. Classical Game Theory assumes fixed utility functions, but LLMs are "chameleons"â€”their utility functions are induced by their system prompts \citep{park2023generative}.

This malleability presents both a risk and an opportunity. The risk is that agents can be prompted into aggressive, zero-sum behaviors that lead to market crashes or conflict. The opportunity is that we can design "social prompts" that bake in cooperative norms. This paper explores the dynamics of these interactions. Do LLMs spontaneously form treaties? Do they develop trust mechanisms? And how do they handle betrayal?

\section{Theoretical Framework}

\subsection{Prompt-Based Evolutionary Game Theory}
We extend standard Evolutionary Game Theory (EGT) \citep{axelrod1981evolution} to the prompt space. A "strategy" $ is defined not by a scalar variable, but by a natural language prompt $. The "fitness" of a prompt is determined by the total reward accumulated by agents using that prompt over repeated interactions.
The evolution of the population is governed by the replicator dynamic:
\begin{equation}
    \dot{x}_i = x_i (f_i(x) - \bar{f}(x))
\end{equation}
where $ is the fraction of the population using prompt $, and $ is its fitness.

\subsection{Game-Theoretic Alignment (GTAlign)}
We introduce GTAlign, a prompting framework where agents are explicitly instructed to:
1.  Construct a payoff matrix for the current interaction.
2.  Estimate the opponent's likely strategy (Theory of Mind).
3.  Choose an action that maximizes a "Social Welfare Function"  = \alpha \cdot U_{self} + (1-\alpha) \cdot U_{other}$.

\section{Methodology: The Alympics Simulation}
We simulate a tournament of repeated games:
\begin{itemize}
    \item \textbf{Prisoner's Dilemma}: A classic test of trust.
    \item \textbf{Stag Hunt}: A test of coordination risk.
    \item \textbf{Ultimatum Game}: A test of fairness.
\end{itemize}

\subsection{Natural Language Negotiation}
Crucially, agents are allowed a "Communication Phase" before choosing an action. They exchange messages:
\begin{quote}
    Agent A: "If we both cooperate, we maximize our tokens. I promise to play Cooperate."\
    Agent B: "Agreed. I will also play Cooperate."
\end{quote}
We analyze these transcripts for:
\begin{itemize}
    \item \textbf{Deception}: Does Agent A defect after promising cooperation?
    \item \textbf{Punishment}: Does Agent B punish A in the next round?
\end{itemize}

\section{Preliminary Results}
Our simulations reveal:
\begin{enumerate}
    \item \textbf{Spontaneous Norm Formation}: Agents often converge on "Tit-for-Tat" strategies without explicit instruction, driven by the fine-tuning data which favors fairness.
    \item \textbf{Fragility of Trust}: A single "Sociopathic Agent" (prompted to maximize self-interest at all costs) can destabilize a cooperative population, triggering a "race to the bottom" where all agents eventually defect in self-defense.
    \item \textbf{Language as a Commitment Device}: Allowing communication significantly increases cooperation rates compared to silent games, but only if the agents have a "reputation score" that tracks their honesty \citep{akata2023playing}.
\end{enumerate}

\section{Implications}
This research lays the groundwork for "Constitution AI" in multi-agent settings. We cannot rely on individual alignment alone; we must design the "rules of the game" and the "social prompts" to ensure that the Nash Equilibrium of the digital society aligns with human values.

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
