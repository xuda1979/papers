\contentsline {section}{\numberline {1}Introduction}{4}{section.1}%
\contentsline {subsection}{\numberline {1.1}The Long-Context Challenge}{4}{subsection.1.1}%
\contentsline {subsection}{\numberline {1.2}Our Contribution: Spectral Sparse Attention (SSA)}{4}{subsection.1.2}%
\contentsline {paragraph}{What SSA does (in brief).}{4}{section*.2}%
\contentsline {paragraph}{Where SSA beats baselines.}{4}{section*.3}%
\contentsline {paragraph}{Key idea.}{4}{section*.4}%
\contentsline {paragraph}{Main theoretical result.}{5}{section*.5}%
\contentsline {subsection}{\numberline {1.3}Related Work}{5}{subsection.1.3}%
\contentsline {paragraph}{Sparse attention mechanisms.}{5}{section*.6}%
\contentsline {paragraph}{Cluster-based attention (Routing Transformer).}{5}{section*.7}%
\contentsline {paragraph}{State-space models.}{5}{section*.8}%
\contentsline {paragraph}{Spectral graph sparsification.}{5}{section*.9}%
\contentsline {subsection}{\numberline {1.4}Paper Organization}{5}{subsection.1.4}%
\contentsline {section}{\numberline {2}Spectral Sparse Attention: Algorithm and Complexity}{6}{section.2}%
\contentsline {subsection}{\numberline {2.1}Algorithm Specification}{6}{subsection.2.1}%
\contentsline {subsubsection}{\numberline {2.1.1}Notation}{6}{subsubsection.2.1.1}%
\contentsline {subsubsection}{\numberline {2.1.2}SSA Pseudocode}{6}{subsubsection.2.1.2}%
\contentsline {subsection}{\numberline {2.2}Complexity Analysis}{7}{subsection.2.2}%
\contentsline {subsection}{\numberline {2.3}Theory-Implementation Mapping}{7}{subsection.2.3}%
\contentsline {paragraph}{Objects in the theory (Section~\ref {sec:spectral_theory}).}{8}{section*.10}%
\contentsline {paragraph}{Objects in the algorithm (Algorithm~\ref {alg:ssa}).}{8}{section*.11}%
\contentsline {paragraph}{Mapping the algorithm to the theory.}{8}{section*.12}%
\contentsline {paragraph}{Key assumptions for the mapping to hold.}{8}{section*.13}%
\contentsline {section}{\numberline {3}Spectral Theory of Attention Graphs}{9}{section.3}%
\contentsline {subsection}{\numberline {3.1}The Attention Graph and Its Laplacian}{9}{subsection.3.1}%
\contentsline {subsection}{\numberline {3.2}Graph-Theoretic Formulation}{9}{subsection.3.2}%
\contentsline {subsection}{\numberline {3.3}The Fundamental Spectral Correspondence}{11}{subsection.3.3}%
\contentsline {subsection}{\numberline {3.4}Riemannian Structure}{13}{subsection.3.4}%
\contentsline {subsection}{\numberline {3.5}Spectral Gap and Information Propagation}{14}{subsection.3.5}%
\contentsline {subsection}{\numberline {3.6}Sharp Eigenvalue Perturbation Theory}{14}{subsection.3.6}%
\contentsline {subsection}{\numberline {3.7}Regularity Assumptions}{16}{subsection.3.7}%
\contentsline {subsection}{\numberline {3.8}Information Propagation and Mixing Time}{17}{subsection.3.8}%
\contentsline {subsection}{\numberline {3.9}Markov Chain Interpretation}{17}{subsection.3.9}%
\contentsline {subsection}{\numberline {3.10}Main Approximation Theorem}{19}{subsection.3.10}%
\contentsline {subsubsection}{\numberline {3.10.1}The Sparsification Problem}{19}{subsubsection.3.10.1}%
\contentsline {subsection}{\numberline {3.11}The Main Approximation Theorem}{19}{subsection.3.11}%
\contentsline {subsection}{\numberline {3.12}Johnson-Lindenstrauss Projection for Efficient Similarity}{21}{subsection.3.12}%
\contentsline {section}{\numberline {4}Experimental Validation}{22}{section.4}%
\contentsline {paragraph}{Experimental scope.}{22}{section*.15}%
\contentsline {subsection}{\numberline {4.1}Baseline Methods}{22}{subsection.4.1}%
\contentsline {subsection}{\numberline {4.2}Real-World Language Modeling Benchmarks}{22}{subsection.4.2}%
\contentsline {paragraph}{Implementation details.}{23}{section*.17}%
\contentsline {paragraph}{Key observations.}{23}{section*.20}%
\contentsline {subsection}{\numberline {4.3}Long-Range Dependency Preservation (Synthetic)}{24}{subsection.4.3}%
\contentsline {paragraph}{Task setup.}{24}{section*.21}%
\contentsline {subsection}{\numberline {4.4}Spectral Fidelity Verification}{25}{subsection.4.4}%
\contentsline {subsection}{\numberline {4.5}Validation of Clusterability Assumption}{25}{subsection.4.5}%
\contentsline {paragraph}{Key observations.}{26}{section*.26}%
\contentsline {paragraph}{Sensitivity to symmetrization.}{26}{section*.27}%
\contentsline {subsection}{\numberline {4.6}Energy Evaluation}{27}{subsection.4.6}%
\contentsline {subsection}{\numberline {4.7}Pareto Frontier}{27}{subsection.4.7}%
\contentsline {subsection}{\numberline {4.8}Ablation Studies}{27}{subsection.4.8}%
\contentsline {subsection}{\numberline {4.9}Memory Efficiency}{28}{subsection.4.9}%
\contentsline {section}{\numberline {5}Discussion}{28}{section.5}%
\contentsline {subsection}{\numberline {5.1}Energy Scaling and Practical Efficiency}{28}{subsection.5.1}%
\contentsline {paragraph}{Dense vs sparse scaling.}{28}{section*.33}%
\contentsline {paragraph}{Memory bandwidth.}{29}{section*.34}%
\contentsline {paragraph}{Quantization synergy.}{29}{section*.35}%
\contentsline {subsection}{\numberline {5.2}Limitations and Assumptions}{29}{subsection.5.2}%
\contentsline {section}{\numberline {6}Conclusion}{29}{section.6}%
\contentsline {paragraph}{Principal contributions.}{30}{section*.36}%
\contentsline {paragraph}{Relationship to prior work.}{30}{section*.37}%
\contentsline {paragraph}{Key insights.}{30}{section*.38}%
\contentsline {paragraph}{Future directions.}{30}{section*.39}%
\contentsline {paragraph}{Broader perspective.}{30}{section*.40}%
\contentsline {section}{\numberline {A}Variational and Thermodynamic Foundations of Attention}{31}{appendix.A}%
\contentsline {section}{\numberline {B}Computational Complexity Theory of Attention}{31}{appendix.B}%
\contentsline {section}{\numberline {C}Ternary Quantization and BitNet Integration}{32}{appendix.C}%
\contentsline {paragraph}{Energy savings intuition.}{32}{section*.43}%
\contentsline {paragraph}{Approximation error.}{32}{section*.44}%
\contentsline {section}{\numberline {D}Generalization Theory for Sparse Attention}{33}{appendix.D}%
\contentsline {section}{\numberline {E}Sharp Estimates and Concentration Inequalities}{33}{appendix.E}%
\contentsline {section}{\numberline {F}Axiomatic Characterization of Attention}{33}{appendix.F}%
\contentsline {section}{\numberline {G}Independence of the Axiom System}{33}{appendix.G}%
\contentsline {section}{\numberline {H}Logical Dependencies of Main Results}{34}{appendix.H}%
\contentsline {section}{\numberline {I}Proof of Technical Lemmas}{35}{appendix.I}%
\contentsline {subsection}{\numberline {I.1}Rademacher Complexity Reduction via Sparsity}{35}{subsection.I.1}%
\contentsline {subsection}{\numberline {I.2}Matrix Bernstein Inequality}{35}{subsection.I.2}%
\contentsline {section}{\numberline {J}Glossary of Notation}{36}{appendix.J}%
\contentsline {section}{\numberline {K}Axiom Summary}{36}{appendix.K}%
\contentsline {subsection}{\numberline {K.1}Attention Axioms (A1--A7)}{36}{subsection.K.1}%
\contentsline {subsection}{\numberline {K.2}Energy Axioms (E1--E3)}{36}{subsection.K.2}%
\contentsline {section}{\numberline {L}Reproducibility Statement}{36}{appendix.L}%
