
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{merity2017pointer,
  title={Pointer sentinel mixture models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={arXiv preprint arXiv:1609.07843},
  year={2017}
}

@article{patterson2021carbon,
  title={Carbon emissions and large neural network training},
  author={Patterson, David and Gonzalez, Joseph and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},
  journal={arXiv preprint arXiv:2104.10350},
  year={2021}
}

@article{gu2023mamba,
  title={Mamba: Linear-time sequence modeling with selective state spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2023}
}

@article{gu2022efficiently,
  title={Efficiently modeling long sequences with structured state spaces},
  author={Gu, Albert and Goel, Karan and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2111.00396},
  year={2022}
}

@article{welford1962note,
  title={Note on a method for calculating corrected sums of squares and products},
  author={Welford, B. P.},
  journal={Technometrics},
  volume={4},
  number={3},
  pages={419--420},
  year={1962},
  publisher={Taylor \& Francis}
}

@inproceedings{shen2021efficient,
  title={Efficient attention: Attention with linear complexities},
  author={Shen, Zhuoran and Zhang, Mingyuan and Zhao, Haiyu and Yi, Shuai and Li, Hongsheng},
  booktitle={Proceedings of the IEEE/CVF winter conference on applications of computer vision},
  pages={3531--3539},
  year={2021}
}

@inproceedings{choromanski2020rethinking,
  title={Rethinking attention with performers},
  author={Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and others},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{arjovsky2016unitary,
  title={Unitary evolution recurrent neural networks},
  author={Arjovsky, Martin and Shah, Amar and Bengio, Yoshua},
  booktitle={International Conference on Machine Learning},
  pages={1120--1128},
  year={2016},
  organization={PMLR}
}

@inproceedings{liu2022ecoformer,
  title={Ecoformer: Energy-saving attention with linear complexity},
  author={Liu, Jing and Pan, Zizheng and He, Haoyu and Cai, Jianfei and Zhuang, Bohan},
  booktitle={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16950--16963},
  year={2022}
}

@article{wang2020cluster,
  title={Cluster-former: Clustering-based sparse transformer for long-range dependency encoding},
  author={Wang, Shuohang and Zhou, Luowei and Gan, Zhe and Chen, Yen-Chun and Fang, Yuwei and Parmar, Jayant and Wang, Yu and Zhou, Ming},
  journal={arXiv preprint arXiv:2009.06097},
  year={2020}
}

@inproceedings{wang2022clusterformer,
  title={ClusterFormer: Neural clustering attention for efficient and effective transformer},
  author={Wang, Ningyu and Gan, Guozheng and Zhang, Peng and Zhang, Shuai and Wei, Junchi and Lu, Xiangang and Shen, Huajun},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics},
  pages={2390--2402},
  year={2022}
}

@inproceedings{vyas2020fast,
  title={Fast transformers with clustered attention},
  author={Vyas, Apoorv and Katharopoulos, Angelos and Fleuret, Fran{\c{c}}ois},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={21665--21674},
  year={2020}
}

@inproceedings{katharopoulos2020transformers,
  title={Transformers are rnns: Fast autoregressive transformers with linear attention},
  author={Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  booktitle={International conference on machine learning},
  pages={5156--5165},
  year={2020},
  organization={PMLR}
}

@inproceedings{jang2017categorical,
  title={Categorical reparameterization with gumbel-softmax},
  author={Jang, Eric and Gu, Shixiang and Poole, Ben},
  booktitle={International Conference on Learning Representations},
  year={2017}
}

@article{child2019generating,
  title={Generating Long Sequences with Sparse Transformers},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1904.10509},
  year={2019}
}

@article{beltagy2020longformer,
  title={Longformer: The Long-Document Transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}

@article{dao2022flashattention,
  title={Flashattention: Fast and memory-efficient exact attention with io-awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16344--16359},
  year={2022}
}

@article{dao2023flashattention2,
  title={Flashattention-2: Faster attention with better parallelism and work partitioning},
  author={Dao, Tri},
  journal={arXiv preprint arXiv:2307.08691},
  year={2023}
}

@article{wang2020linformer,
  title={Linformer: Self-Attention with Linear Complexity},
  author={Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}

@article{kitaev2020reformer,
  title={Reformer: The Efficient Transformer},
  author={Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  journal={arXiv preprint arXiv:2001.04451},
  year={2020}
}

@inproceedings{yun2020,
  title={Are Transformers universal approximators of sequence-to-sequence functions?},
  author={Yun, Chulhee and Bhojanapalli, Srinadh and Rawat, Ankit Singh and Reddi, Sashank J and Kumar, Sanjiv},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{davis1970rotation,
  title={The rotation of eigenvectors by a perturbation. III},
  author={Davis, Chandler and Kahan, William M},
  journal={SIAM Journal on Numerical Analysis},
  volume={7},
  number={1},
  pages={1--46},
  year={1970},
  publisher={SIAM}
}

@article{tropp2012user,
  title={User-friendly tail bounds for sums of random matrices},
  author={Tropp, Joel A},
  journal={Foundations of computational mathematics},
  volume={12},
  number={4},
  pages={389--434},
  year={2012},
  publisher={Springer}
}

@book{chung1997spectral,
  title={Spectral graph theory},
  author={Chung, Fan RK},
  volume={92},
  year={1997},
  publisher={American Mathematical Soc.}
}

@article{spielman2011graph,
  title={Graph sparsification by effective resistances},
  author={Spielman, Daniel A and Srivastava, Nikhil},
  journal={SIAM Journal on Computing},
  volume={40},
  number={6},
  pages={1913--1926},
  year={2011},
  publisher={SIAM}
}

@article{roy2021mixture,
    title={Mixture-of-Depths: Dynamically allocating compute in transformer-based language models},
    author={Roy, Aurko and Saffar, Mohammad and Vaswani, Ashish and Grangier, David},
    journal={arXiv preprint arXiv:2104.04561},
    year={2021}
}

@article{dao2022flashattention,
  title={FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  author={Dao, Tri and Fu, Daniel Y and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  year={2022}
}

@article{li2025constant,
  title={Constant Bit-size Transformers Are Turing Complete},
  author={Li, Qian and Wang, Yuyi},
  journal={arXiv preprint arXiv:2506.12027},
  year={2025}
}

@article{jiang2025softmax,
  title={Softmax Transformers are Turing-Complete},
  author={Jiang, Hongjian and Hahn, Michael and Zetzsche, Georg and Lin, Anthony W},
  journal={arXiv preprint arXiv:2511.20038},
  year={2025}
}

@article{wang2024bitnet,
  title={BitNet: Scaling 1-bit Transformers for Large Language Models},
  author={Wang, Hongyu and Ma, Shuming and Dong, Li and Huang, Shaohan and Zhang, Dongdong and Wei, Furu},
  journal={arXiv preprint arXiv:2310.11453},
  year={2023}
}

@book{parberry1994circuit,
  title={Circuit Complexity and Neural Networks},
  author={Parberry, Ian},
  year={1994},
  publisher={MIT Press}
}

@article{landauer1961irreversibility,
  title={Irreversibility and heat generation in the computing process},
  author={Landauer, Rolf},
  journal={IBM Journal of Research and Development},
  volume={5},
  number={3},
  pages={183--191},
  year={1961},
  publisher={IBM}
}

@article{bartlett2002rademacher,
  title={Rademacher and Gaussian complexities: Risk bounds and structural results},
  author={Bartlett, Peter L and Mendelson, Shahar},
  journal={Journal of Machine Learning Research},
  volume={3},
  pages={463--482},
  year={2002}
}

@article{perez2019turing,
  title={On the Turing Completeness of Modern Neural Network Architectures},
  author={P{\'e}rez, Jorge and Barcel{\'o}, Pablo and Marinkovic, Javier},
  journal={International Conference on Learning Representations},
  year={2019}
}

@article{wei2022statistically,
  title={Statistically Meaningful Approximation: a Case Study on Approximating Turing Machines with Transformers},
  author={Wei, Colin and Chen, Yining and Ma, Tengyu},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  year={2022}
}

@inproceedings{batson2012twiceramanujan,
  title={Twice-Ramanujan Sparsifiers},
  author={Batson, Joshua and Spielman, Daniel A and Srivastava, Nikhil},
  booktitle={SIAM Journal on Computing},
  volume={41},
  number={6},
  pages={1704--1721},
  year={2012}
}

@article{von2007tutorial,
  title={A tutorial on spectral clustering},
  author={Von Luxburg, Ulrike},
  journal={Statistics and computing},
  volume={17},
  number={4},
  pages={395--416},
  year={2007},
  publisher={Springer}
}

@article{lei2015consistency,
  title={Consistency of spectral clustering in stochastic block models},
  author={Lei, Jing and Rinaldo, Alessandro},
  journal={The Annals of Statistics},
  volume={43},
  number={1},
  pages={215--237},
  year={2015},
  publisher={Institute of Mathematical Statistics}
}

@article{xiao2023attention,
  title={Efficient Streaming Language Models with Attention Sinks},
  author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  journal={arXiv preprint arXiv:2309.17453},
  year={2023}
}

@article{roy2021routing,
  title={Efficient Content-Based Sparse Attention with Routing Transformers},
  author={Roy, Aurko and Saffar, Mohammad and Vaswani, Ashish and Grangier, David},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={53--68},
  year={2021}
}

@article{gu2020hippo,
  title={HiPPO: Recurrent Memory with Optimal Polynomial Projections},
  author={Gu, Albert and Dao, Tri and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1474--1487},
  year={2020}
}

@article{gu2021efficiently,
  title={Efficiently Modeling Long Sequences with Structured State Spaces},
  author={Gu, Albert and Goel, Karan and R{\'e}, Christopher},
  journal={International Conference on Learning Representations},
  year={2022}
}

@article{peng2023rwkv,
  title={RWKV: Reinventing RNNs for the Transformer Era},
  author={Peng, Bo and Alcaide, Eric and Anthony, Quentin and Albalak, Alon and Arcadinho, Samuel and Cao, Huanqi and others},
  journal={arXiv preprint arXiv:2305.13048},
  year={2023}
}

% Note: choromanski2020rethinking is already defined above

@article{tay2020long,
  title={Long Range Arena: A Benchmark for Efficient Transformers},
  author={Tay, Yi and Dehghani, Mostafa and Abnar, Samira and Shen, Yikang and Bahri, Dara and Pham, Philip and Rao, Jinfeng and Yang, Liu and Ruder, Sebastian and Metzler, Donald},
  journal={International Conference on Learning Representations},
  year={2021}
}

@book{trefethen2019approximation,
  title={Approximation Theory and Approximation Practice},
  author={Trefethen, Lloyd N},
  year={2019},
  publisher={SIAM}
}

@inproceedings{raz1999super,
  title={Super-logarithmic depth lower bounds via the direct sum in communication complexity},
  author={Raz, Ran and others},
  booktitle={Computational Complexity},
  volume={6},
  number={3},
  pages={191--204},
  year={1999}
}
