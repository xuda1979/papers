\begin{thebibliography}{10}

\bibitem{beltagy2020longformer}
Iz~Beltagy, Matthew~E Peters, and Arman Cohan.
\newblock Longformer: The long-document transformer.
\newblock {\em arXiv preprint arXiv:2004.05150}, 2020.

\bibitem{child2019generating}
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
\newblock Generating long sequences with sparse transformers.
\newblock {\em arXiv preprint arXiv:1904.10509}, 2019.

\bibitem{chung1997spectral}
Fan~RK Chung.
\newblock {\em Spectral graph theory}, volume~92.
\newblock American Mathematical Soc., 1997.

\bibitem{dao2022flashattention}
Tri Dao, Daniel~Y Fu, Stefano Ermon, Atri Rudra, and Christopher R{\'e}.
\newblock Flashattention: Fast and memory-efficient exact attention with
  io-awareness.
\newblock {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{davis1970rotation}
Chandler Davis and William~M Kahan.
\newblock The rotation of eigenvectors by a perturbation. iii.
\newblock {\em SIAM Journal on Numerical Analysis}, 7(1):1--46, 1970.

\bibitem{gu2023mamba}
Albert Gu and Tri Dao.
\newblock Mamba: Linear-time sequence modeling with selective state spaces.
\newblock {\em arXiv preprint arXiv:2312.00752}, 2023.

\bibitem{jiang2025softmax}
Hongjian Jiang, Michael Hahn, Georg Zetzsche, and Anthony~W Lin.
\newblock Softmax transformers are turing-complete.
\newblock {\em arXiv preprint arXiv:2511.20038}, 2025.

\bibitem{kitaev2020reformer}
Nikita Kitaev, {\L}ukasz Kaiser, and Anselm Levskaya.
\newblock Reformer: The efficient transformer.
\newblock {\em arXiv preprint arXiv:2001.04451}, 2020.

\bibitem{landauer1961irreversibility}
Rolf Landauer.
\newblock Irreversibility and heat generation in the computing process.
\newblock {\em IBM Journal of Research and Development}, 5(3):183--191, 1961.

\bibitem{li2025constant}
Qian Li and Yuyi Wang.
\newblock Constant bit-size transformers are turing complete.
\newblock {\em arXiv preprint arXiv:2506.12027}, 2025.

\bibitem{tropp2012user}
Joel~A Tropp.
\newblock User-friendly tail bounds for sums of random matrices.
\newblock {\em Foundations of computational mathematics}, 12(4):389--434, 2012.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{wang2024bitnet}
Hongyu Wang, Shuming Ma, Li~Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei.
\newblock Bitnet: Scaling 1-bit transformers for large language models.
\newblock {\em arXiv preprint arXiv:2310.11453}, 2023.

\bibitem{wang2020linformer}
Sinong Wang, Belinda~Z Li, Madian Khabsa, Han Fang, and Hao Ma.
\newblock Linformer: Self-attention with linear complexity.
\newblock {\em arXiv preprint arXiv:2006.04768}, 2020.

\end{thebibliography}
