\begin{thebibliography}{10}

\bibitem{arjovsky2016unitary}
Martin Arjovsky, Amar Shah, and Yoshua Bengio.
\newblock Unitary evolution recurrent neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  1120--1128. PMLR, 2016.

\bibitem{beltagy2020longformer}
Iz~Beltagy, Matthew~E Peters, and Arman Cohan.
\newblock Longformer: The long-document transformer.
\newblock {\em arXiv preprint arXiv:2004.05150}, 2020.

\bibitem{child2019generating}
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
\newblock Generating long sequences with sparse transformers.
\newblock {\em arXiv preprint arXiv:1904.10509}, 2019.

\bibitem{choromanski2020rethinking}
Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,
  Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin,
  Lukasz Kaiser, et~al.
\newblock Rethinking attention with performers.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{dao2023flashattention2}
Tri Dao.
\newblock Flashattention-2: Faster attention with better parallelism and work
  partitioning.
\newblock {\em arXiv preprint arXiv:2307.08691}, 2023.

\bibitem{gu2023mamba}
Albert Gu and Tri Dao.
\newblock Mamba: Linear-time sequence modeling with selective state spaces.
\newblock {\em arXiv preprint arXiv:2312.00752}, 2023.

\bibitem{gu2022efficiently}
Albert Gu, Karan Goel, and Christopher R{\'e}.
\newblock Efficiently modeling long sequences with structured state spaces.
\newblock {\em arXiv preprint arXiv:2111.00396}, 2022.

\bibitem{jang2017categorical}
Eric Jang, Shixiang Gu, and Ben Poole.
\newblock Categorical reparameterization with gumbel-softmax.
\newblock In {\em International Conference on Learning Representations}, 2017.

\bibitem{katharopoulos2020transformers}
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran{\c{c}}ois
  Fleuret.
\newblock Transformers are rnns: Fast autoregressive transformers with linear
  attention.
\newblock In {\em International conference on machine learning}, pages
  5156--5165. PMLR, 2020.

\bibitem{liu2022ecoformer}
Jing Liu, Zizheng Pan, Haoyu He, Jianfei Cai, and Bohan Zhuang.
\newblock Ecoformer: Energy-saving attention with linear complexity.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~35, pages 16950--16963, 2022.

\bibitem{merity2017pointer}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
\newblock Pointer sentinel mixture models.
\newblock {\em arXiv preprint arXiv:1609.07843}, 2017.

\bibitem{patterson2021carbon}
David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia,
  Daniel Rothchild, David So, Maud Texier, and Jeff Dean.
\newblock Carbon emissions and large neural network training.
\newblock {\em arXiv preprint arXiv:2104.10350}, 2021.

\bibitem{shen2021efficient}
Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li.
\newblock Efficient attention: Attention with linear complexities.
\newblock In {\em Proceedings of the IEEE/CVF winter conference on applications
  of computer vision}, pages 3531--3539, 2021.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{wang2022clusterformer}
Ningyu Wang, Guozheng Gan, Peng Zhang, Shuai Zhang, Junchi Wei, Xiangang Lu,
  and Huajun Shen.
\newblock Clusterformer: Neural clustering attention for efficient and
  effective transformer.
\newblock In {\em Proceedings of the 60th Annual Meeting of the Association for
  Computational Linguistics}, pages 2390--2402, 2022.

\bibitem{wang2020cluster}
Shuohang Wang, Luowei Zhou, Zhe Gan, Yen-Chun Chen, Yuwei Fang, Jayant Parmar,
  Yu~Wang, and Ming Zhou.
\newblock Cluster-former: Clustering-based sparse transformer for long-range
  dependency encoding.
\newblock {\em arXiv preprint arXiv:2009.06097}, 2020.

\bibitem{welford1962note}
B.~P. Welford.
\newblock Note on a method for calculating corrected sums of squares and
  products.
\newblock {\em Technometrics}, 4(3):419--420, 1962.

\end{thebibliography}
