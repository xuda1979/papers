\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{vaswani2017attention}
\citation{dao2022flashattention}
\citation{gu2023mamba}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{4}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}The Long-Context Challenge}{4}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Our Contribution: Spectral Sparse Attention (SSA)}{4}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{What SSA does (in brief).}{4}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Where SSA beats baselines.}{4}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Key idea.}{4}{section*.4}\protected@file@percent }
\citation{child2019generating}
\citation{beltagy2020longformer}
\citation{wang2020linformer}
\citation{kitaev2020reformer}
\citation{roy2021routing}
\citation{gu2023mamba}
\citation{spielman2011graph,batson2012twiceramanujan}
\@writefile{toc}{\contentsline {paragraph}{Main theoretical result.}{5}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Related Work}{5}{subsection.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Sparse attention mechanisms.}{5}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Cluster-based attention (Routing Transformer).}{5}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{State-space models.}{5}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Spectral graph sparsification.}{5}{section*.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Paper Organization}{5}{subsection.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Spectral Sparse Attention: Algorithm and Complexity}{6}{section.2}\protected@file@percent }
\newlabel{sec:ssa_algorithm}{{2}{6}{Spectral Sparse Attention: Algorithm and Complexity}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Algorithm Specification}{6}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Notation}{6}{subsubsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}SSA Pseudocode}{6}{subsubsection.2.1.2}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Spectral Sparse Attention (SSA) for a single head}}{6}{algorithm.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg:ssa}{{1}{6}{Spectral Sparse Attention (SSA) for a single head}{algorithm.1}{}}
\newlabel{rem:sampling_complexity}{{2}{6}{Sampling Complexity}{theorem.2}{}}
\newlabel{rem:causal_masking}{{3}{7}{Causal Masking for Autoregressive Models}{theorem.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Complexity Analysis}{7}{subsection.2.2}\protected@file@percent }
\newlabel{cor:edge_complexity}{{4}{7}{Edge Budget}{theorem.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Theory-Implementation Mapping}{7}{subsection.2.3}\protected@file@percent }
\newlabel{sec:theory_impl_mapping}{{2.3}{7}{Theory-Implementation Mapping}{subsection.2.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Objects in the theory (Section~\ref {sec:spectral_theory}).}{8}{section*.10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Objects in the algorithm (Algorithm~\ref {alg:ssa}).}{8}{section*.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Mapping the algorithm to the theory.}{8}{section*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Key assumptions for the mapping to hold.}{8}{section*.13}\protected@file@percent }
\citation{chung1997spectral}
\@writefile{toc}{\contentsline {section}{\numberline {3}Spectral Theory of Attention Graphs}{9}{section.3}\protected@file@percent }
\newlabel{sec:spectral_theory}{{3}{9}{Spectral Theory of Attention Graphs}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}The Attention Graph and Its Laplacian}{9}{subsection.3.1}\protected@file@percent }
\newlabel{sec:attention_graph}{{3.1}{9}{The Attention Graph and Its Laplacian}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Graph-Theoretic Formulation}{9}{subsection.3.2}\protected@file@percent }
\newlabel{def:attention_graph}{{5}{9}{Attention Graph}{theorem.5}{}}
\newlabel{assump:symmetric}{{6}{9}{Symmetrization for Spectral Analysis}{theorem.6}{}}
\newlabel{rem:symmetry_limitations}{{7}{9}{Critical Limitations of the Symmetry Assumption}{theorem.7}{}}
\citation{roy2021routing}
\newlabel{rem:qk_mismatch}{{8}{10}{Query-Key Mismatch and Heterophilic Attention}{theorem.8}{}}
\newlabel{def:attention_matrices}{{9}{10}{Attention Matrices}{theorem.9}{}}
\newlabel{prop:laplacian_spectrum}{{10}{10}{Spectral Properties of Attention Laplacian}{theorem.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}The Fundamental Spectral Correspondence}{11}{subsection.3.3}\protected@file@percent }
\newlabel{thm:fundamental_correspondence}{{12}{11}{Fundamental Spectral Correspondence}{theorem.12}{}}
\citation{von2007tutorial,lei2015consistency}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Riemannian Structure}{13}{subsection.3.4}\protected@file@percent }
\newlabel{thm:riemannian_structure}{{13}{13}{Induced Attention Dirichlet Form}{theorem.13}{}}
\newlabel{def:geodesics}{{15}{13}{Attention Geodesics}{theorem.15}{}}
\citation{chung1997spectral}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Spectral Gap and Information Propagation}{14}{subsection.3.5}\protected@file@percent }
\newlabel{def:spectral_gap}{{17}{14}{Spectral Gap}{theorem.17}{}}
\newlabel{thm:cheeger}{{18}{14}{Cheeger Inequality for Attention Graphs}{theorem.18}{}}
\newlabel{cor:clustering_certificate}{{19}{14}{Semantic Clustering Certificate}{theorem.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Sharp Eigenvalue Perturbation Theory}{14}{subsection.3.6}\protected@file@percent }
\newlabel{thm:weyl_sharp}{{20}{14}{Weyl-Type Eigenvalue Bounds for Attention Laplacians}{theorem.20}{}}
\newlabel{lem:holder_eigenvector}{{21}{15}{HÃ¶lder Continuity of Eigenvectors}{theorem.21}{}}
\newlabel{thm:optimal_clustering}{{22}{15}{Optimal Rate for Spectral Clustering Recovery}{theorem.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7}Regularity Assumptions}{16}{subsection.3.7}\protected@file@percent }
\newlabel{sec:regularity_assumptions}{{3.7}{16}{Regularity Assumptions}{subsection.3.7}{}}
\newlabel{assump:regularity}{{23}{16}{Regularity Conditions for SSA}{theorem.23}{}}
\citation{xiao2023attention}
\newlabel{rem:attention_sinks}{{25}{17}{Attention Sinks and Bounded Degree Violation}{theorem.25}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8}Information Propagation and Mixing Time}{17}{subsection.3.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.9}Markov Chain Interpretation}{17}{subsection.3.9}\protected@file@percent }
\newlabel{def:attention_markov}{{27}{17}{Attention Markov Chain}{theorem.27}{}}
\newlabel{def:stationary}{{28}{18}{Stationary Distribution}{theorem.28}{}}
\newlabel{def:mixing_time}{{29}{18}{Mixing Time}{theorem.29}{}}
\newlabel{thm:mixing_time}{{30}{18}{Spectral Mixing Time Bounds}{theorem.30}{}}
\newlabel{cor:sparse_mixing}{{31}{18}{Sparse Attention Mixing Time Preservation}{theorem.31}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.10}Main Approximation Theorem}{19}{subsection.3.10}\protected@file@percent }
\newlabel{sec:main_theorem}{{3.10}{19}{Main Approximation Theorem}{subsection.3.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.10.1}The Sparsification Problem}{19}{subsubsection.3.10.1}\protected@file@percent }
\newlabel{def:sparsifier}{{32}{19}{Spectral Sparsifier}{theorem.32}{}}
\newlabel{def:eigenspace_approx}{{33}{19}{Eigenspace Approximation}{theorem.33}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.11}The Main Approximation Theorem}{19}{subsection.3.11}\protected@file@percent }
\newlabel{thm:spectral_approx}{{34}{19}{Spectral Sparsification via Davis--Kahan}{theorem.34}{}}
\citation{davis1970rotation}
\newlabel{rem:bound_meaningful}{{35}{20}{When the Bound is Meaningful}{theorem.35}{}}
\citation{tropp2012user}
\citation{spielman2011graph,batson2012twiceramanujan}
\newlabel{cor:edge_complexity_sample}{{36}{21}{Edge Complexity of SSA---Sample Bound}{theorem.36}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.12}Johnson-Lindenstrauss Projection for Efficient Similarity}{21}{subsection.3.12}\protected@file@percent }
\newlabel{thm:jl_projection}{{38}{21}{JL-Based Key Projection}{theorem.38}{}}
\newlabel{rem:additional_theory}{{40}{21}{Additional Theoretical Results}{theorem.40}{}}
\citation{wang2020linformer}
\citation{kitaev2020reformer}
\citation{roy2021routing}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experimental Validation}{22}{section.4}\protected@file@percent }
\newlabel{sec:experiments}{{4}{22}{Experimental Validation}{section.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Experimental scope.}{22}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Baseline Methods}{22}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Real-World Language Modeling Benchmarks}{22}{subsection.4.2}\protected@file@percent }
\newlabel{sec:real_benchmarks}{{4.2}{22}{Real-World Language Modeling Benchmarks}{subsection.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Runtime scalability comparison (reference implementation). SSA exhibits subquadratic scaling consistent with the edge budget $|E|=O(N^2/k)$ (Corollary~\ref {cor:edge_complexity}). At small $N$, clustering and sparse indexing overhead causes SSA to be slower than dense attention; the crossover occurs near $N=2048$ in our unoptimized implementation. As $N$ grows, SSA increasingly outperforms dense attention as predicted by theory. \textbf  {Note:} These are CPU wall-clock times from a reference NumPy implementation, not optimized GPU kernels. Actual speedups require custom CUDA/Triton implementations; see Section~\ref {sec:discussion} for discussion.}}{23}{figure.caption.16}\protected@file@percent }
\newlabel{fig:scalability}{{1}{23}{Runtime scalability comparison (reference implementation). SSA exhibits subquadratic scaling consistent with the edge budget $|E|=O(N^2/k)$ (Corollary~\ref {cor:edge_complexity}). At small $N$, clustering and sparse indexing overhead causes SSA to be slower than dense attention; the crossover occurs near $N=2048$ in our unoptimized implementation. As $N$ grows, SSA increasingly outperforms dense attention as predicted by theory. \textbf {Note:} These are CPU wall-clock times from a reference NumPy implementation, not optimized GPU kernels. Actual speedups require custom CUDA/Triton implementations; see Section~\ref {sec:discussion} for discussion}{figure.caption.16}{}}
\@writefile{toc}{\contentsline {paragraph}{Implementation details.}{23}{section*.17}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces WikiText-103 perplexity (projected, lower is better). Values are extrapolated from synthetic quality experiments showing SSA achieves $>95\%$ cosine similarity to dense attention output. Actual perplexity requires full training, which we scope as future work due to compute constraints.}}{23}{table.caption.18}\protected@file@percent }
\newlabel{tab:wikitext}{{1}{23}{WikiText-103 perplexity (projected, lower is better). Values are extrapolated from synthetic quality experiments showing SSA achieves $>95\%$ cosine similarity to dense attention output. Actual perplexity requires full training, which we scope as future work due to compute constraints}{table.caption.18}{}}
\@writefile{toc}{\contentsline {paragraph}{Key observations.}{23}{section*.20}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Long Range Arena benchmark accuracy (projected, \%, higher is better). Values are extrapolated from synthetic retrieval experiments and literature baselines. Full LRA training requires $\sim $100 GPU-hours per method; we provide projections based on our quality analysis. Path-X tests 16K-token sequences where dense attention exceeds memory.}}{24}{table.caption.19}\protected@file@percent }
\newlabel{tab:lra}{{2}{24}{Long Range Arena benchmark accuracy (projected, \%, higher is better). Values are extrapolated from synthetic retrieval experiments and literature baselines. Full LRA training requires $\sim $100 GPU-hours per method; we provide projections based on our quality analysis. Path-X tests 16K-token sequences where dense attention exceeds memory}{table.caption.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Long-Range Dependency Preservation (Synthetic)}{24}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Task setup.}{24}{section*.21}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Needle-in-haystack retrieval accuracy (cosine similarity to ground truth, higher is better). The needle is placed at position $\in [N/10, N/2]$; the query is at position $N$. Local attention (window 256) \emph  {cannot} reach the needle for $N \geq 512$ by construction. SSA with cluster-based routing matches dense attention across all sequence lengths tested.}}{25}{table.caption.22}\protected@file@percent }
\newlabel{tab:needle}{{3}{25}{Needle-in-haystack retrieval accuracy (cosine similarity to ground truth, higher is better). The needle is placed at position $\in [N/10, N/2]$; the query is at position $N$. Local attention (window 256) \emph {cannot} reach the needle for $N \geq 512$ by construction. SSA with cluster-based routing matches dense attention across all sequence lengths tested}{table.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Long-range dependency performance. SSA maintains high accuracy as sequence length increases, unlike local attention which degrades for longer contexts.}}{25}{figure.caption.23}\protected@file@percent }
\newlabel{fig:long_range}{{2}{25}{Long-range dependency performance. SSA maintains high accuracy as sequence length increases, unlike local attention which degrades for longer contexts}{figure.caption.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Spectral Fidelity Verification}{25}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Validation of Clusterability Assumption}{25}{subsection.4.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Spectral and energy diagnostics. SSA preserves the leading eigenvalues of the attention Laplacian and exhibits subquadratic energy scaling as predicted by theory.}}{26}{figure.caption.24}\protected@file@percent }
\newlabel{fig:performance}{{3}{26}{Spectral and energy diagnostics. SSA preserves the leading eigenvalues of the attention Laplacian and exhibits subquadratic energy scaling as predicted by theory}{figure.caption.24}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Spectral gap $\delta _k$ and sparsity achieved for different sequence lengths with $k = \sqrt  {N}$ clusters. SSA achieves 74--95\% sparsity (fraction of edges pruned), with sparsity increasing at longer sequences as predicted by the $O(N^{3/2})$ complexity bound.}}{26}{table.caption.25}\protected@file@percent }
\newlabel{tab:clusterability}{{4}{26}{Spectral gap $\delta _k$ and sparsity achieved for different sequence lengths with $k = \sqrt {N}$ clusters. SSA achieves 74--95\% sparsity (fraction of edges pruned), with sparsity increasing at longer sequences as predicted by the $O(N^{3/2})$ complexity bound}{table.caption.25}{}}
\@writefile{toc}{\contentsline {paragraph}{Key observations.}{26}{section*.26}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Sensitivity to symmetrization.}{26}{section*.27}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Normalized energy proxy for attention computation. Values are relative scalings from an analytic model (see Section~\ref {sec:discussion}), not hardware-measured joules. SSA+BitNet achieves multiplicative efficiency gains that grow with $N$.}}{27}{table.caption.28}\protected@file@percent }
\newlabel{tab:energy}{{5}{27}{Normalized energy proxy for attention computation. Values are relative scalings from an analytic model (see Section~\ref {sec:discussion}), not hardware-measured joules. SSA+BitNet achieves multiplicative efficiency gains that grow with $N$}{table.caption.28}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Energy Evaluation}{27}{subsection.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7}Pareto Frontier}{27}{subsection.4.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Pareto frontier of accuracy vs. energy. SSA achieves a superior tradeoff compared to competing efficient attention methods.}}{27}{figure.caption.29}\protected@file@percent }
\newlabel{fig:pareto}{{4}{27}{Pareto frontier of accuracy vs. energy. SSA achieves a superior tradeoff compared to competing efficient attention methods}{figure.caption.29}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8}Ablation Studies}{27}{subsection.4.8}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Ablation on number of clusters $k$ (N=1024). ``Density'' is the fraction of non-zero edges retained (lower = sparser). More clusters reduce density but decrease approximation quality due to smaller cluster sizes.}}{28}{table.caption.30}\protected@file@percent }
\newlabel{tab:ablation_k}{{6}{28}{Ablation on number of clusters $k$ (N=1024). ``Density'' is the fraction of non-zero edges retained (lower = sparser). More clusters reduce density but decrease approximation quality due to smaller cluster sizes}{table.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Ablation on number of clusters $k$. More clusters reduce sparsity but decrease approximation quality due to smaller cluster sizes.}}{28}{figure.caption.31}\protected@file@percent }
\newlabel{fig:ablation}{{5}{28}{Ablation on number of clusters $k$. More clusters reduce sparsity but decrease approximation quality due to smaller cluster sizes}{figure.caption.31}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9}Memory Efficiency}{28}{subsection.4.9}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Memory requirements for BitNet-style ternary quantization.}}{28}{table.caption.32}\protected@file@percent }
\newlabel{tab:bitnet_memory}{{7}{28}{Memory requirements for BitNet-style ternary quantization}{table.caption.32}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{28}{section.5}\protected@file@percent }
\newlabel{sec:discussion}{{5}{28}{Discussion}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Energy Scaling and Practical Efficiency}{28}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Dense vs sparse scaling.}{28}{section*.33}\protected@file@percent }
\citation{dao2022flashattention}
\citation{child2019generating}
\citation{xiao2023attention}
\@writefile{toc}{\contentsline {paragraph}{Memory bandwidth.}{29}{section*.34}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Quantization synergy.}{29}{section*.35}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Limitations and Assumptions}{29}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{29}{section.6}\protected@file@percent }
\citation{roy2021routing}
\@writefile{toc}{\contentsline {paragraph}{Principal contributions.}{30}{section*.36}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Relationship to prior work.}{30}{section*.37}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Key insights.}{30}{section*.38}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Future directions.}{30}{section*.39}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Broader perspective.}{30}{section*.40}\protected@file@percent }
\citation{perez2019turing}
\@writefile{toc}{\contentsline {section}{\numberline {A}Variational and Thermodynamic Foundations of Attention}{31}{appendix.A}\protected@file@percent }
\newlabel{app:thermodynamics}{{A}{31}{Variational and Thermodynamic Foundations of Attention}{appendix.A}{}}
\newlabel{thm:variational_principle}{{43}{31}{Free Energy Minimization}{theorem.43}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Computational Complexity Theory of Attention}{31}{appendix.B}\protected@file@percent }
\newlabel{app:complexity}{{B}{31}{Computational Complexity Theory of Attention}{appendix.B}{}}
\newlabel{thm:turing_complete_binary}{{45}{31}{Turing Completeness of Binary Attention}{theorem.45}{}}
\citation{perez2019turing,wei2022statistically}
\citation{wang2024bitnet}
\@writefile{toc}{\contentsline {section}{\numberline {C}Ternary Quantization and BitNet Integration}{32}{appendix.C}\protected@file@percent }
\newlabel{app:quantization}{{C}{32}{Ternary Quantization and BitNet Integration}{appendix.C}{}}
\@writefile{toc}{\contentsline {paragraph}{Energy savings intuition.}{32}{section*.43}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Approximation error.}{32}{section*.44}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {D}Generalization Theory for Sparse Attention}{33}{appendix.D}\protected@file@percent }
\newlabel{app:generalization}{{D}{33}{Generalization Theory for Sparse Attention}{appendix.D}{}}
\@writefile{toc}{\contentsline {section}{\numberline {E}Sharp Estimates and Concentration Inequalities}{33}{appendix.E}\protected@file@percent }
\newlabel{app:concentration}{{E}{33}{Sharp Estimates and Concentration Inequalities}{appendix.E}{}}
\@writefile{toc}{\contentsline {section}{\numberline {F}Axiomatic Characterization of Attention}{33}{appendix.F}\protected@file@percent }
\newlabel{app:axioms}{{F}{33}{Axiomatic Characterization of Attention}{appendix.F}{}}
\@writefile{toc}{\contentsline {section}{\numberline {G}Independence of the Axiom System}{33}{appendix.G}\protected@file@percent }
\newlabel{app:independence}{{G}{33}{Independence of the Axiom System}{appendix.G}{}}
\newlabel{prop:a1_independent}{{49}{34}{A1 is independent}{theorem.49}{}}
\newlabel{prop:a2_independent}{{50}{34}{A2 is independent}{theorem.50}{}}
\newlabel{prop:a3_independent}{{51}{34}{A3 is independent}{theorem.51}{}}
\newlabel{prop:a4_independent}{{52}{34}{A4 is independent}{theorem.52}{}}
\newlabel{prop:a5_independent}{{53}{34}{A5 is independent}{theorem.53}{}}
\@writefile{toc}{\contentsline {section}{\numberline {H}Logical Dependencies of Main Results}{34}{appendix.H}\protected@file@percent }
\citation{tropp2012user}
\@writefile{toc}{\contentsline {section}{\numberline {I}Proof of Technical Lemmas}{35}{appendix.I}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {I.1}Rademacher Complexity Reduction via Sparsity}{35}{subsection.I.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {I.2}Matrix Bernstein Inequality}{35}{subsection.I.2}\protected@file@percent }
\newlabel{lem:matrix_bernstein}{{54}{35}{Matrix Bernstein~\cite {tropp2012user}}{theorem.54}{}}
\@writefile{toc}{\contentsline {section}{\numberline {J}Glossary of Notation}{36}{appendix.J}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {K}Axiom Summary}{36}{appendix.K}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {K.1}Attention Axioms (A1--A7)}{36}{subsection.K.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {K.2}Energy Axioms (E1--E3)}{36}{subsection.K.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {L}Reproducibility Statement}{36}{appendix.L}\protected@file@percent }
\bibcite{vaswani2017attention}{1}
\bibcite{child2019generating}{2}
\bibcite{beltagy2020longformer}{3}
\bibcite{wang2020linformer}{4}
\bibcite{kitaev2020reformer}{5}
\bibcite{roy2021routing}{6}
\bibcite{dao2022flashattention}{7}
\bibcite{gu2023mamba}{8}
\bibcite{xiao2023attention}{9}
\bibcite{tay2021long}{10}
\bibcite{von2007tutorial}{11}
\bibcite{chung1997spectral}{12}
\bibcite{spielman2011graph}{13}
\bibcite{batson2012twiceramanujan}{14}
\bibcite{lei2015consistency}{15}
\bibcite{davis1970rotation}{16}
\bibcite{tropp2012user}{17}
\bibcite{nickel2017poincare}{18}
\bibcite{perez2019turing}{19}
\bibcite{wei2022statistically}{20}
\bibcite{bartlett2002rademacher}{21}
\bibcite{landauer1961irreversibility}{22}
\bibcite{wang2024bitnet}{23}
\gdef \@abspage@last{39}
