improve the paper and output diff for the latex code:
\documentclass[10pt,twocolumn,letterpaper]{article}

% --- Packages ---
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}        % \mathbb
\usepackage{microtype}       % better kerning & justification
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{caption}
\usepackage{authblk}
\usepackage{abstract}

% --- Configuration ---

% Define geometry for a typical conference paper layout
\geometry{
letterpaper,
top=1in,
bottom=1in,
left=0.75in,
right=0.75in,
columnsep=0.25in
}

% Configuration for code listings (PyTorch/Python)
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.98,0.98,0.98}

\lstdefinestyle{pytorchstyle}{
language=Python,
backgroundcolor=\color{backcolour},
commentstyle=\color{codegreen},
keywordstyle=\color{blue},
numberstyle=\tiny\color{codegray},
stringstyle=\color{codepurple},
basicstyle=\ttfamily\footnotesize,
breaklines=true,
captionpos=b,
frame=single,
keepspaces=true,
numbers=left,
numbersep=5pt,
showstringspaces=false,
tabsize=4
}

\title{\textbf{The Latent State Transformer (LST): Sub-Quadratic Sequence Modeling via SSM-Informed Latent Attention}}

% Placeholder authors
\author[1]{Anonymous Author(s)}
\affil[1]{Research Institution}

\date{}

\begin{document}

% Abstract and title in twocolumn mode
\twocolumn[
\begin{@twocolumnfalse}
\maketitle
\begin{abstract}
\noindent The quadratic complexity of the Transformer architecture has motivated the search for more efficient sequence models. State Space Models (SSMs), such as Mamba, offer linear scaling but can lack the rich content‑aware reasoning of attention mechanisms. We introduce the \textbf{Latent State Transformer (LST)}, a hybrid architecture that marries SSM efficiency with attention expressiveness. LST first scans the input sequence with a selective SSM, then compresses the resulting features into a fixed‑size latent \emph{bottleneck}. A lightweight \emph{State‑Informed Attention} (SIA) layer subsequently performs cross‑attention between the full sequence and this latent space. The attention cost is therefore reduced from \(O(N^{2})\) to \(O(NK)\), where \(K\!\ll\!N\). Synthetic experiments show that LST scales \emph{nearly linearly} with sequence length while preserving the strong modeling performance typical of full attention.
\end{abstract}
\vspace{1em}
\end{@twocolumnfalse}
]

\section{Introduction}

The trade-off between performance and efficiency remains a central challenge in sequence modeling. The Transformer architecture~\cite{vaswani2017attention} excels at capturing complex dependencies but incurs a quadratic \(O(N^{2})\) cost in sequence length, limiting its practicality for very long contexts such as genomics or long‑document analysis.

Recent developments in State Space Models (SSMs), particularly the selective SSM architecture Mamba~\cite{gu2023mamba}, have demonstrated efficient sequence modeling with linear O(N) complexity. However, SSMs process information through a recurrent-like mechanism which can sometimes limit their ability to perform complex associative recall compared to the direct access provided by attention.

We propose the Latent State Transformer (LST) to bridge this gap. Inspired by latent bottleneck architectures such as the Perceiver~\cite{jaegle2021perceiver}, LST decouples the sequence length from the attention complexity by integrating an efficient SSM backbone with a compressed attention mechanism.

\section{The LST Architecture}

The LST architecture modifies the traditional Transformer block by replacing the standard Multi-Head Self-Attention (MHSA) with a sequential processing block (SSM) followed by a State-Informed Attention (SIA) block.

\subsection{Sequential Feature Extraction via SSM}

The first stage of an LST block processes the input sequence \(X\in\mathbb{R}^{N\times D}\) with a selective State Space Model (e.g.\ Mamba). This scan is \(O(N)\) and efficiently integrates information across the entire sequence.

\begin{equation}
H = \text{SSM}(X)
\end{equation}

The output H is a sequence representation enriched with historical context at each time step.

\subsection{State-Informed Attention (SIA)}

The core innovation of LST is the SIA mechanism, which operates through a compressed latent bottleneck.

\subsubsection{Latent Space Compression}

We generate a fixed‑size latent representation \(L\in\mathbb{R}^{K\times D}\) with \(K\ll N\) (e.g.\ \(K{=}128\)). Learnable query vectors \(Q_{\text{latent}}\in\mathbb{R}^{K\times D}\) aggregate information from \(H\) via attention:

\begin{equation}
L = \text{Attention}(Q=Q_{\text{latent}}, K=H, V=H)
\end{equation}

This step summarizes the entire sequence into K vectors and has a complexity of O(NK).

\subsubsection{Latent Cross-Attention}

The sequence representation H then performs cross-attention against the global latent summary L.

\begin{equation}
Y = \text{Attention}(Q=H, K=L, V=L)
\end{equation}

This allows every element in the sequence to access the compressed global context summarized in L. This step is also O(NK).

\subsection{Overall Complexity}

The total complexity per block is \(O(N)\) for the SSM pass plus \(2\times O(NK)\) for SIA. For \(K\ll N\), the dominant term is \(O(NK)\), yielding quasi‑linear scaling— a substantial improvement over the \(O(N^{2})\) cost of standard Transformers.

\section{Conceptual Implementation}

The following PyTorch-style pseudocode illustrates the LST block structure, including essential components like residual connections.

\begin{lstlisting}[style=pytorchstyle, caption={PyTorch-style pseudocode for the LST Block.}, label=lst:code_block]
import torch
import torch.nn as nn

# Placeholder for the SSM/Mamba component
class MockMambaBlock(nn.Module):
    """Minimal placeholder for the O(N) SSM scan."""
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return x  # no‑op for illustration


class LSTBlock(nn.Module):
    def __init__(self, d_model: int, num_heads: int, k_latent: int) -> None:
        super().__init__()
        self.ssm = MockMambaBlock()

        # 1) Latent queries (K×D) – the bottleneck parameters
        self.latent_queries = nn.Parameter(torch.randn(k_latent, d_model))

        # 2) Cross‑attention layers
        self.attn_compress  = nn.MultiheadAttention(d_model, num_heads, batch_first=True)
        self.attn_retrieve  = nn.MultiheadAttention(d_model, num_heads, batch_first=True)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: (B, N, D) input sequence
        Returns:
            (B, N, D) output sequence
        """
        # Step 1: SSM processing — O(N)
        h = self.ssm(x) + x  # residual

        # Step 2: Latent bottleneck compression — O(NK)
        q_lat = self.latent_queries.unsqueeze(0).expand(x.size(0), -1, -1)
        L, _   = self.attn_compress(q_lat, h, h)

        # Step 3: State‑Informed Attention — O(NK)
        out, _ = self.attn_retrieve(h, L, L)
        return h + out  # residual
\end{lstlisting}

% Figure spanning both columns
\begin{figure*}[t]
\centering
\includegraphics[width=0.9\textwidth]{LST_Simulation_Results.png}
\caption{Simulated trade‑off between perplexity (lower is better) and throughput (tokens/s, log‑log scale) as sequence length grows. LST approaches Transformer‑level perplexity while retaining the near‑linear efficiency of Mamba.}
\label{fig:simulation}
\end{figure*}

\section{Simulated Performance Evaluation}

To validate the LST design, we simulate the expected computational efficiency (throughput) and modeling performance (perplexity) compared to a standard Transformer (\(O(N^{2})\)) and a pure Mamba model (\(O(N)\)). We hypothesise that LST will match Mamba's efficiency while approaching the Transformer's performance.

We use a fixed latent dimension K=128 and a model dimension D=768 for the simulation, evaluating sequence lengths N from 512 to 65536.

\subsection{Analysis of Simulated Results}

The simulation results (Figure~\ref{fig:simulation}) illustrate the theoretical advantages of the LST architecture.

\textbf{Performance (Figure~\ref{fig:simulation}A):} As hypothesized, the Transformer achieves the lowest perplexity, benefiting from full N-to-N attention. Mamba exhibits higher (worse) perplexity. LST effectively bridges this gap. At the maximum simulated length (N=65536), LST achieves a perplexity of 2.09, significantly better than Mamba's 3.40 and close to the Transformer's 1.85. This suggests that the SSM-informed latent bottleneck retains sufficient global context for effective sequence modeling.

\textbf{Efficiency (Figure~\ref{fig:simulation}B):} The log-log throughput plot clearly demonstrates the impact of computational complexity. As the sequence length increases, the Transformer's throughput drops sharply due to its O(N 
2
 ) scaling. At N=65536, the Transformer's throughput is severely limited (approx. 9.9 tokens/sec). Both Mamba (O(N)) and LST (O(NK)) exhibit excellent, near-linear scaling. While Mamba is the most efficient (approx. 54253 tokens/sec), LST maintains strong efficiency (approx. 2429 tokens/sec), providing significantly higher throughput than the Transformer while offering superior performance to Mamba.

\section{Conclusion and Future Work}

We introduced the Latent State Transformer (LST), a hybrid architecture that combines the linear efficiency of State Space Models with the global reasoning capabilities of attention. By utilizing an SSM to inform a fixed-size latent bottleneck, LST avoids the quadratic complexity of standard Transformers, operating in O(NK) time. Theoretical simulations suggest that LST offers a highly favorable trade-off between performance and efficiency for long-context modeling. Future work will focus on empirical validation of the LST architecture on established language modeling benchmarks (e.g., Pile, PG-19) to confirm these theoretical advantages.

\section*{References}

\small
\bibliographystyle{plain}

% Manually formatted bibliography
\begin{thebibliography}{9}

\bibitem{vaswani2017attention}
A. Vaswani, et al. "Attention is all you need." \textit{Advances in Neural Information Processing Systems}. 2017.

\bibitem{gu2023mamba}
A. Gu, & T. Dao. "Mamba: Linear-time sequence modeling with selective state spaces." \textit{arXiv preprint arXiv:2312.00752}. 2023.

\bibitem{jaegle2021perceiver}
A. Jaegle, et al. "Perceiver: General perception with iterative attention." \textit{International Conference on Machine Learning}. 2021.

\end{thebibliography}

\end{document}