\frametitle{GRPO算法 - DeepSeek的创新}
\textbf{Group Relative Policy Optimization}：去除Critic网络，简化训练

\vspace{0.3cm}
\removelatexerror
\begin{algorithmic}[1]
\FOR{每个问题 $q$}
    \STATE 采样一组回答 $\{o_1, o_2, ..., o_G\}$ 从 $\pi_{\theta_{old}}$
    \STATE 计算奖励 $\{r_1, r_2, ..., r_G\}$
    \STATE 归一化优势: $\hat{A}_i = \frac{r_i - \text{mean}(\mathbf{r})}{\text{std}(\mathbf{r})}$
    \STATE 更新策略最大化:
\ENDFOR
\end{algorithmic}

$$\mathcal{J}_{GRPO}(\theta) = \mathbb{E}_{q, \{o_i\}} \left[ \frac{1}{G}\sum_{i=1}^{G} \min\left( \frac{\pi_\theta(o_i|q)}{\pi_{\theta_{old}}(o_i|q)} \hat{A}_i, \text{clip}(\cdot) \hat{A}_i \right) \right]$$

\vspace{0.2cm}
$$- \beta \cdot \mathbb{D}_{KL}[\pi_\theta || \pi_{ref}]$$
