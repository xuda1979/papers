\contentsline {section}{\numberline {1}引言}{8}{section.1}%
\contentsline {subsection}{\numberline {1.1}研究背景与动机}{8}{subsection.1.1}%
\contentsline {subsection}{\numberline {1.2}研究进展时间线}{8}{subsection.1.2}%
\contentsline {subsection}{\numberline {1.3}本报告结构}{9}{subsection.1.3}%
\contentsline {section}{\numberline {2}强化学习基础理论}{10}{section.2}%
\contentsline {subsection}{\numberline {2.1}马尔可夫决策过程}{10}{subsection.2.1}%
\contentsline {subsection}{\numberline {2.2}部分可观测马尔可夫决策过程（POMDP）}{11}{subsection.2.2}%
\contentsline {subsection}{\numberline {2.3}策略梯度方法}{11}{subsection.2.3}%
\contentsline {subsection}{\numberline {2.4}值函数估计}{12}{subsection.2.4}%
\contentsline {subsection}{\numberline {2.5}广义优势估计（GAE）}{12}{subsection.2.5}%
\contentsline {subsection}{\numberline {2.6}信任域方法}{12}{subsection.2.6}%
\contentsline {subsection}{\numberline {2.7}重要性采样与Off-Policy学习}{13}{subsection.2.7}%
\contentsline {subsection}{\numberline {2.8}自然梯度与Fisher信息矩阵}{13}{subsection.2.8}%
\contentsline {section}{\numberline {3}核心算法详解}{13}{section.3}%
\contentsline {subsection}{\numberline {3.1}PPO算法}{13}{subsection.3.1}%
\contentsline {subsubsection}{\numberline {3.1.1}算法动机}{13}{subsubsection.3.1.1}%
\contentsline {subsubsection}{\numberline {3.1.2}数学推导}{14}{subsubsection.3.1.2}%
\contentsline {subsubsection}{\numberline {3.1.3}完整目标函数}{14}{subsubsection.3.1.3}%
\contentsline {subsubsection}{\numberline {3.1.4}算法流程}{15}{subsubsection.3.1.4}%
\contentsline {subsubsection}{\numberline {3.1.5}PPO的变体与改进}{15}{subsubsection.3.1.5}%
\contentsline {subsubsection}{\numberline {3.1.6}PPO在LLM中的特殊考虑}{16}{subsubsection.3.1.6}%
\contentsline {subsection}{\numberline {3.2}GRPO算法}{16}{subsection.3.2}%
\contentsline {subsubsection}{\numberline {3.2.1}算法动机}{16}{subsubsection.3.2.1}%
\contentsline {subsubsection}{\numberline {3.2.2}数学推导}{16}{subsubsection.3.2.2}%
\contentsline {subsubsection}{\numberline {3.2.3}与PPO的对比}{17}{subsubsection.3.2.3}%
\contentsline {subsubsection}{\numberline {3.2.4}算法流程}{18}{subsubsection.3.2.4}%
\contentsline {subsubsection}{\numberline {3.2.5}GRPO的理论分析}{18}{subsubsection.3.2.5}%
\contentsline {subsubsection}{\numberline {3.2.6}GRPO变体}{18}{subsubsection.3.2.6}%
\contentsline {subsection}{\numberline {3.3}Direct Preference Optimization (DPO)}{19}{subsection.3.3}%
\contentsline {subsubsection}{\numberline {3.3.1}核心思想}{19}{subsubsection.3.3.1}%
\contentsline {subsubsection}{\numberline {3.3.2}DPO目标函数}{19}{subsubsection.3.3.2}%
\contentsline {subsubsection}{\numberline {3.3.3}DPO vs RLHF}{20}{subsubsection.3.3.3}%
\contentsline {subsection}{\numberline {3.4}Reinforcement Learning from AI Feedback (RLAIF)}{20}{subsection.3.4}%
\contentsline {subsubsection}{\numberline {3.4.1}核心框架}{20}{subsubsection.3.4.1}%
\contentsline {subsubsection}{\numberline {3.4.2}评判模型设计}{20}{subsubsection.3.4.2}%
\contentsline {subsubsection}{\numberline {3.4.3}RLAIF的变体}{21}{subsubsection.3.4.3}%
\contentsline {subsection}{\numberline {3.5}奖励函数设计}{21}{subsection.3.5}%
\contentsline {subsubsection}{\numberline {3.5.1}可验证奖励}{21}{subsubsection.3.5.1}%
\contentsline {subsubsection}{\numberline {3.5.2}格式奖励}{22}{subsubsection.3.5.2}%
\contentsline {subsubsection}{\numberline {3.5.3}长度惩罚}{22}{subsubsection.3.5.3}%
\contentsline {subsubsection}{\numberline {3.5.4}复合奖励}{22}{subsubsection.3.5.4}%
\contentsline {subsubsection}{\numberline {3.5.5}过程奖励与结果奖励的数学分析}{22}{subsubsection.3.5.5}%
\contentsline {subsubsection}{\numberline {3.5.6}奖励塑形（Reward Shaping）}{23}{subsubsection.3.5.6}%
\contentsline {section}{\numberline {4}DeepSeek-R1技术详解}{23}{section.4}%
\contentsline {subsection}{\numberline {4.1}项目概述}{23}{subsection.4.1}%
\contentsline {subsection}{\numberline {4.2}训练流程}{23}{subsection.4.2}%
\contentsline {subsubsection}{\numberline {4.2.1}Stage 1: 冷启动SFT}{24}{subsubsection.4.2.1}%
\contentsline {subsubsection}{\numberline {4.2.2}Stage 2: 推理RL}{24}{subsubsection.4.2.2}%
\contentsline {subsubsection}{\numberline {4.2.3}Stage 3: 拒绝采样SFT}{25}{subsubsection.4.2.3}%
\contentsline {subsubsection}{\numberline {4.2.4}Stage 4: 全场景RL}{25}{subsubsection.4.2.4}%
\contentsline {subsection}{\numberline {4.3}DeepSeek-R1-Zero实验}{25}{subsection.4.3}%
\contentsline {subsubsection}{\numberline {4.3.1}实验设置}{25}{subsubsection.4.3.1}%
\contentsline {subsubsection}{\numberline {4.3.2}涌现现象}{26}{subsubsection.4.3.2}%
\contentsline {subsubsection}{\numberline {4.3.3}局限性}{26}{subsubsection.4.3.3}%
\contentsline {subsection}{\numberline {4.4}性能表现}{26}{subsection.4.4}%
\contentsline {subsection}{\numberline {4.5}蒸馏到小模型}{26}{subsection.4.5}%
\contentsline {subsubsection}{\numberline {4.5.1}蒸馏方法}{27}{subsubsection.4.5.1}%
\contentsline {subsubsection}{\numberline {4.5.2}蒸馏效果}{27}{subsubsection.4.5.2}%
\contentsline {section}{\numberline {5}OpenAI o1技术分析}{27}{section.5}%
\contentsline {subsection}{\numberline {5.1}官方披露信息}{27}{subsection.5.1}%
\contentsline {subsection}{\numberline {5.2}技术推测}{27}{subsection.5.2}%
\contentsline {subsubsection}{\numberline {5.2.1}过程奖励模型（PRM）}{28}{subsubsection.5.2.1}%
\contentsline {subsubsection}{\numberline {5.2.2}大规模RL训练}{28}{subsubsection.5.2.2}%
\contentsline {subsubsection}{\numberline {5.2.3}推理时搜索}{28}{subsubsection.5.2.3}%
\contentsline {subsection}{\numberline {5.3}推理时Scaling}{29}{subsection.5.3}%
\contentsline {subsection}{\numberline {5.4}与DeepSeek-R1的对比}{29}{subsection.5.4}%
\contentsline {section}{\numberline {6}其他主要研究方法}{29}{section.6}%
\contentsline {subsection}{\numberline {6.1}Constitutional AI（Anthropic）}{29}{subsection.6.1}%
\contentsline {subsubsection}{\numberline {6.1.1}核心思想}{29}{subsubsection.6.1.1}%
\contentsline {subsubsection}{\numberline {6.1.2}宪法原则示例}{30}{subsubsection.6.1.2}%
\contentsline {subsubsection}{\numberline {6.1.3}优势与局限}{30}{subsubsection.6.1.3}%
\contentsline {subsection}{\numberline {6.2}Self-Play与迭代改进}{30}{subsection.6.2}%
\contentsline {subsubsection}{\numberline {6.2.1}基本流程}{30}{subsubsection.6.2.1}%
\contentsline {subsubsection}{\numberline {6.2.2}代表工作}{30}{subsubsection.6.2.2}%
\contentsline {subsection}{\numberline {6.3}Google的推理研究}{31}{subsection.6.3}%
\contentsline {subsubsection}{\numberline {6.3.1}Chain-of-Thought Prompting}{31}{subsubsection.6.3.1}%
\contentsline {subsubsection}{\numberline {6.3.2}Self-Consistency}{31}{subsubsection.6.3.2}%
\contentsline {subsection}{\numberline {6.4}STaR: Self-Taught Reasoner}{31}{subsection.6.4}%
\contentsline {subsubsection}{\numberline {6.4.1}核心方法}{31}{subsubsection.6.4.1}%
\contentsline {subsubsection}{\numberline {6.4.2}Rationalization的作用}{32}{subsubsection.6.4.2}%
\contentsline {subsection}{\numberline {6.5}Quiet-STaR: 静默思考}{32}{subsection.6.5}%
\contentsline {subsubsection}{\numberline {6.5.1}核心创新}{32}{subsubsection.6.5.1}%
\contentsline {subsubsection}{\numberline {6.5.2}训练目标}{32}{subsubsection.6.5.2}%
\contentsline {subsection}{\numberline {6.6}ReST：强化自我训练}{33}{subsection.6.6}%
\contentsline {subsubsection}{\numberline {6.6.1}核心方法}{33}{subsubsection.6.6.1}%
\contentsline {subsubsection}{\numberline {6.6.2}与其他方法的关系}{33}{subsubsection.6.6.2}%
\contentsline {subsubsection}{\numberline {6.6.3}理论分析}{33}{subsubsection.6.6.3}%
\contentsline {subsection}{\numberline {6.7}ReST-EM：期望最大化自我训练}{34}{subsection.6.7}%
\contentsline {subsubsection}{\numberline {6.7.1}EM框架}{34}{subsubsection.6.7.1}%
\contentsline {subsubsection}{\numberline {6.7.2}与RLHF的联系}{34}{subsubsection.6.7.2}%
\contentsline {subsection}{\numberline {6.8}Expert Iteration (ExIt)}{34}{subsection.6.8}%
\contentsline {subsubsection}{\numberline {6.8.1}算法思想}{34}{subsubsection.6.8.1}%
\contentsline {subsubsection}{\numberline {6.8.2}在LLM中的应用}{35}{subsubsection.6.8.2}%
\contentsline {subsubsection}{\numberline {6.8.3}搜索策略}{35}{subsubsection.6.8.3}%
\contentsline {subsection}{\numberline {6.9}Self-Rewarding Language Models}{35}{subsection.6.9}%
\contentsline {subsubsection}{\numberline {6.9.1}核心创新}{35}{subsubsection.6.9.1}%
\contentsline {subsubsection}{\numberline {6.9.2}自评Prompt设计}{35}{subsubsection.6.9.2}%
\contentsline {subsubsection}{\numberline {6.9.3}迭代训练}{36}{subsubsection.6.9.3}%
\contentsline {subsection}{\numberline {6.10}REINFORCE及其变体}{36}{subsection.6.10}%
\contentsline {subsubsection}{\numberline {6.10.1}基础REINFORCE}{36}{subsubsection.6.10.1}%
\contentsline {subsubsection}{\numberline {6.10.2}RLOO (REINFORCE Leave-One-Out)}{36}{subsubsection.6.10.2}%
\contentsline {subsubsection}{\numberline {6.10.3}与GRPO的关系}{37}{subsubsection.6.10.3}%
\contentsline {subsection}{\numberline {6.11}RAFT：奖励排序微调}{37}{subsection.6.11}%
\contentsline {subsubsection}{\numberline {6.11.1}算法流程}{37}{subsubsection.6.11.1}%
\contentsline {subsubsection}{\numberline {6.11.2}与拒绝采样的区别}{37}{subsubsection.6.11.2}%
\contentsline {subsection}{\numberline {6.12}Rejection Sampling Fine-tuning (RSF)}{37}{subsection.6.12}%
\contentsline {subsubsection}{\numberline {6.12.1}方法细节}{37}{subsubsection.6.12.1}%
\contentsline {subsubsection}{\numberline {6.12.2}在数学推理中的应用}{38}{subsubsection.6.12.2}%
\contentsline {subsection}{\numberline {6.13}V-STaR：验证器增强的自我训练}{38}{subsection.6.13}%
\contentsline {subsubsection}{\numberline {6.13.1}方法改进}{38}{subsubsection.6.13.1}%
\contentsline {subsubsection}{\numberline {6.13.2}验证器训练}{38}{subsubsection.6.13.2}%
\contentsline {subsubsection}{\numberline {6.13.3}联合训练}{38}{subsubsection.6.13.3}%
\contentsline {subsection}{\numberline {6.14}过程奖励模型（PRM）与结果奖励模型（ORM）}{39}{subsection.6.14}%
\contentsline {subsubsection}{\numberline {6.14.1}Let's Verify Step by Step}{39}{subsubsection.6.14.1}%
\contentsline {subsubsection}{\numberline {6.14.2}Math-Shepherd}{39}{subsubsection.6.14.2}%
\contentsline {subsubsection}{\numberline {6.14.3}PRM vs ORM对比}{39}{subsubsection.6.14.3}%
\contentsline {subsection}{\numberline {6.15}PRIME：隐式过程奖励}{40}{subsection.6.15}%
\contentsline {subsubsection}{\numberline {6.15.1}核心思想}{40}{subsubsection.6.15.1}%
\contentsline {subsubsection}{\numberline {6.15.2}实现方法}{40}{subsubsection.6.15.2}%
\contentsline {subsection}{\numberline {6.16}Online DPO与迭代偏好优化}{40}{subsection.6.16}%
\contentsline {subsubsection}{\numberline {6.16.1}Online DPO}{40}{subsubsection.6.16.1}%
\contentsline {subsubsection}{\numberline {6.16.2}IPO (Identity Preference Optimization)}{40}{subsubsection.6.16.2}%
\contentsline {subsection}{\numberline {6.17}WizardMath与WizardCoder系列}{40}{subsection.6.17}%
\contentsline {subsubsection}{\numberline {6.17.1}Evol-Instruct方法}{40}{subsubsection.6.17.1}%
\contentsline {subsubsection}{\numberline {6.17.2}RLEIF (RL from Evol-Instruct Feedback)}{41}{subsubsection.6.17.2}%
\contentsline {subsection}{\numberline {6.18}Qwen系列数学与代码模型}{41}{subsection.6.18}%
\contentsline {subsubsection}{\numberline {6.18.1}Qwen2.5-Math}{41}{subsubsection.6.18.1}%
\contentsline {subsubsection}{\numberline {6.18.2}训练流程}{41}{subsubsection.6.18.2}%
\contentsline {subsection}{\numberline {6.19}Marco-o1与MCTS增强推理}{42}{subsection.6.19}%
\contentsline {subsubsection}{\numberline {6.19.1}MCTS在LLM中的应用}{42}{subsubsection.6.19.1}%
\contentsline {subsubsection}{\numberline {6.19.2}与其他方法结合}{42}{subsubsection.6.19.2}%
\contentsline {subsection}{\numberline {6.20}Kimi k1.5与长上下文RL}{42}{subsection.6.20}%
\contentsline {subsubsection}{\numberline {6.20.1}长上下文强化学习}{42}{subsubsection.6.20.1}%
\contentsline {subsubsection}{\numberline {6.20.2}技术特点}{42}{subsubsection.6.20.2}%
\contentsline {subsection}{\numberline {6.21}方法对比总结}{43}{subsection.6.21}%
\contentsline {subsection}{\numberline {6.22}其他重要方法与研究}{43}{subsection.6.22}%
\contentsline {subsubsection}{\numberline {6.22.1}SPIN: Self-Play Fine-Tuning}{43}{subsubsection.6.22.1}%
\contentsline {subsubsection}{\numberline {6.22.2}Iterative DPO}{43}{subsubsection.6.22.2}%
\contentsline {subsubsection}{\numberline {6.22.3}KTO: Kahneman-Tversky Optimization}{44}{subsubsection.6.22.3}%
\contentsline {subsubsection}{\numberline {6.22.4}ORPO: Odds Ratio Preference Optimization}{44}{subsubsection.6.22.4}%
\contentsline {subsubsection}{\numberline {6.22.5}SimPO: Simple Preference Optimization}{44}{subsubsection.6.22.5}%
\contentsline {subsubsection}{\numberline {6.22.6}Code Generation: AlphaCode与竞赛编程}{44}{subsubsection.6.22.6}%
\contentsline {subsubsection}{\numberline {6.22.7}Tool Use与Agent RL}{45}{subsubsection.6.22.7}%
\contentsline {subsubsection}{\numberline {6.22.8}OpenR: 开源推理框架}{45}{subsubsection.6.22.8}%
\contentsline {subsubsection}{\numberline {6.22.9}rStar: 自博弈推理}{45}{subsubsection.6.22.9}%
\contentsline {subsubsection}{\numberline {6.22.10}Inference-Time Compute Scaling}{45}{subsubsection.6.22.10}%
\contentsline {section}{\numberline {7}创新算法框架}{46}{section.7}%
\contentsline {subsection}{\numberline {7.1}自适应分层奖励优化（AHRO）}{46}{subsection.7.1}%
\contentsline {subsubsection}{\numberline {7.1.1}动机与背景}{46}{subsubsection.7.1.1}%
\contentsline {subsubsection}{\numberline {7.1.2}算法框架}{46}{subsubsection.7.1.2}%
\contentsline {subsubsection}{\numberline {7.1.3}理论分析}{47}{subsubsection.7.1.3}%
\contentsline {subsection}{\numberline {7.2}认知架构引导策略学习（CAPL）}{47}{subsection.7.2}%
\contentsline {subsubsection}{\numberline {7.2.1}核心思想}{47}{subsubsection.7.2.1}%
\contentsline {subsubsection}{\numberline {7.2.2}架构设计}{48}{subsubsection.7.2.2}%
\contentsline {subsubsection}{\numberline {7.2.3}训练方法}{48}{subsubsection.7.2.3}%
\contentsline {subsection}{\numberline {7.3}动态思维链自演化（DCoT-SE）}{49}{subsection.7.3}%
\contentsline {subsubsection}{\numberline {7.3.1}动机}{49}{subsubsection.7.3.1}%
\contentsline {subsubsection}{\numberline {7.3.2}推理模式库}{49}{subsubsection.7.3.2}%
\contentsline {subsubsection}{\numberline {7.3.3}模式选择策略}{49}{subsubsection.7.3.3}%
\contentsline {subsubsection}{\numberline {7.3.4}自演化机制}{50}{subsubsection.7.3.4}%
\contentsline {subsection}{\numberline {7.4}多粒度推理一致性强化（MGRCR）}{50}{subsection.7.4}%
\contentsline {subsubsection}{\numberline {7.4.1}核心思想}{50}{subsubsection.7.4.1}%
\contentsline {subsubsection}{\numberline {7.4.2}粒度定义}{50}{subsubsection.7.4.2}%
\contentsline {subsubsection}{\numberline {7.4.3}一致性奖励设计}{51}{subsubsection.7.4.3}%
\contentsline {subsubsection}{\numberline {7.4.4}一致性检测方法}{51}{subsubsection.7.4.4}%
\contentsline {subsubsection}{\numberline {7.4.5}训练框架}{51}{subsubsection.7.4.5}%
\contentsline {subsection}{\numberline {7.5}元认知反馈回路优化（MFLO）}{52}{subsection.7.5}%
\contentsline {subsubsection}{\numberline {7.5.1}元认知框架}{52}{subsubsection.7.5.1}%
\contentsline {subsubsection}{\numberline {7.5.2}元认知组件}{52}{subsubsection.7.5.2}%
\contentsline {subsubsection}{\numberline {7.5.3}反馈回路设计}{53}{subsubsection.7.5.3}%
\contentsline {subsubsection}{\numberline {7.5.4}训练方法}{53}{subsubsection.7.5.4}%
\contentsline {subsection}{\numberline {7.6}对抗推理鲁棒性增强（ARRE）}{54}{subsection.7.6}%
\contentsline {subsubsection}{\numberline {7.6.1}动机}{54}{subsubsection.7.6.1}%
\contentsline {subsubsection}{\numberline {7.6.2}对抗样本生成}{54}{subsubsection.7.6.2}%
\contentsline {subsubsection}{\numberline {7.6.3}对抗训练目标}{54}{subsubsection.7.6.3}%
\contentsline {subsubsection}{\numberline {7.6.4}渐进式对抗训练}{55}{subsubsection.7.6.4}%
\contentsline {subsection}{\numberline {7.7}课程强化推理学习（CRRL）}{55}{subsection.7.7}%
\contentsline {subsubsection}{\numberline {7.7.1}核心思想}{55}{subsubsection.7.7.1}%
\contentsline {subsubsection}{\numberline {7.7.2}多维度课程设计}{55}{subsubsection.7.7.2}%
\contentsline {subsubsection}{\numberline {7.7.3}自适应课程调度}{55}{subsubsection.7.7.3}%
\contentsline {subsubsection}{\numberline {7.7.4}里程碑机制}{56}{subsubsection.7.7.4}%
\contentsline {subsection}{\numberline {7.8}推理能力迁移学习框架（RCTL）}{56}{subsection.7.8}%
\contentsline {subsubsection}{\numberline {7.8.1}动机}{56}{subsubsection.7.8.1}%
\contentsline {subsubsection}{\numberline {7.8.2}推理能力分解}{56}{subsubsection.7.8.2}%
\contentsline {subsubsection}{\numberline {7.8.3}能力表示学习}{57}{subsubsection.7.8.3}%
\contentsline {subsubsection}{\numberline {7.8.4}迁移机制}{57}{subsubsection.7.8.4}%
\contentsline {subsection}{\numberline {7.9}因果推理增强学习（CREL）}{57}{subsection.7.9}%
\contentsline {subsubsection}{\numberline {7.9.1}核心思想}{57}{subsubsection.7.9.1}%
\contentsline {subsubsection}{\numberline {7.9.2}因果图建模}{57}{subsubsection.7.9.2}%
\contentsline {subsubsection}{\numberline {7.9.3}因果奖励设计}{57}{subsubsection.7.9.3}%
\contentsline {subsubsection}{\numberline {7.9.4}反事实推理训练}{58}{subsubsection.7.9.4}%
\contentsline {subsection}{\numberline {7.10}分布式自博弈推理优化（DSPRO）}{58}{subsection.7.10}%
\contentsline {subsubsection}{\numberline {7.10.1}大规模自博弈框架}{58}{subsubsection.7.10.1}%
\contentsline {subsubsection}{\numberline {7.10.2}多样性促进机制}{58}{subsubsection.7.10.2}%
\contentsline {subsubsection}{\numberline {7.10.3}异步训练协议}{59}{subsubsection.7.10.3}%
\contentsline {section}{\numberline {8}实现细节与训练技巧}{59}{section.8}%
\contentsline {subsection}{\numberline {8.1}数据准备}{59}{subsection.8.1}%
\contentsline {subsubsection}{\numberline {8.1.1}数学数据来源}{59}{subsubsection.8.1.1}%
\contentsline {subsubsection}{\numberline {8.1.2}代码数据来源}{60}{subsubsection.8.1.2}%
\contentsline {subsubsection}{\numberline {8.1.3}数据质量控制}{60}{subsubsection.8.1.3}%
\contentsline {subsubsection}{\numberline {8.1.4}数据增强}{60}{subsubsection.8.1.4}%
\contentsline {subsection}{\numberline {8.2}采样策略}{61}{subsection.8.2}%
\contentsline {subsubsection}{\numberline {8.2.1}温度采样}{61}{subsubsection.8.2.1}%
\contentsline {subsubsection}{\numberline {8.2.2}Top-p采样（Nucleus Sampling）}{61}{subsubsection.8.2.2}%
\contentsline {subsubsection}{\numberline {8.2.3}Top-k采样}{61}{subsubsection.8.2.3}%
\contentsline {subsubsection}{\numberline {8.2.4}Best-of-N（Rejection Sampling）}{62}{subsubsection.8.2.4}%
\contentsline {subsubsection}{\numberline {8.2.5}束搜索变体}{62}{subsubsection.8.2.5}%
\contentsline {subsection}{\numberline {8.3}训练稳定性}{62}{subsection.8.3}%
\contentsline {subsubsection}{\numberline {8.3.1}梯度管理}{62}{subsubsection.8.3.1}%
\contentsline {subsubsection}{\numberline {8.3.2}学习率调度}{63}{subsubsection.8.3.2}%
\contentsline {subsubsection}{\numberline {8.3.3}奖励归一化}{63}{subsubsection.8.3.3}%
\contentsline {subsubsection}{\numberline {8.3.4}KL散度控制}{63}{subsubsection.8.3.4}%
\contentsline {subsection}{\numberline {8.4}分布式训练}{64}{subsection.8.4}%
\contentsline {subsubsection}{\numberline {8.4.1}数据并行}{64}{subsubsection.8.4.1}%
\contentsline {subsubsection}{\numberline {8.4.2}张量并行（Tensor Parallelism）}{64}{subsubsection.8.4.2}%
\contentsline {subsubsection}{\numberline {8.4.3}流水线并行（Pipeline Parallelism）}{65}{subsubsection.8.4.3}%
\contentsline {subsubsection}{\numberline {8.4.4}采样与训练分离}{65}{subsubsection.8.4.4}%
\contentsline {subsection}{\numberline {8.5}内存优化}{65}{subsection.8.5}%
\contentsline {subsubsection}{\numberline {8.5.1}梯度检查点（Gradient Checkpointing）}{65}{subsubsection.8.5.1}%
\contentsline {subsubsection}{\numberline {8.5.2}混合精度训练}{65}{subsubsection.8.5.2}%
\contentsline {subsubsection}{\numberline {8.5.3}ZeRO优化}{66}{subsubsection.8.5.3}%
\contentsline {subsubsection}{\numberline {8.5.4}LoRA/QLoRA}{66}{subsubsection.8.5.4}%
\contentsline {subsection}{\numberline {8.6}超参数配置指南}{66}{subsection.8.6}%
\contentsline {subsubsection}{\numberline {8.6.1}GRPO推荐配置}{66}{subsubsection.8.6.1}%
\contentsline {subsubsection}{\numberline {8.6.2}PPO推荐配置}{66}{subsubsection.8.6.2}%
\contentsline {subsection}{\numberline {8.7}监控与调试}{67}{subsection.8.7}%
\contentsline {subsubsection}{\numberline {8.7.1}关键指标监控}{67}{subsubsection.8.7.1}%
\contentsline {subsubsection}{\numberline {8.7.2}异常检测}{67}{subsubsection.8.7.2}%
\contentsline {subsubsection}{\numberline {8.7.3}调试技巧}{67}{subsubsection.8.7.3}%
\contentsline {section}{\numberline {9}失败尝试与经验教训}{68}{section.9}%
\contentsline {subsection}{\numberline {9.1}DeepSeek报告的失败尝试}{68}{subsection.9.1}%
\contentsline {subsubsection}{\numberline {9.1.1}过程奖励模型（PRM）}{68}{subsubsection.9.1.1}%
\contentsline {subsubsection}{\numberline {9.1.2}蒙特卡洛树搜索（MCTS）}{68}{subsubsection.9.1.2}%
\contentsline {subsubsection}{\numberline {9.1.3}直接从Base模型RL}{69}{subsubsection.9.1.3}%
\contentsline {subsubsection}{\numberline {9.1.4}复杂奖励工程}{69}{subsubsection.9.1.4}%
\contentsline {subsection}{\numberline {9.2}常见问题与解决}{70}{subsection.9.2}%
\contentsline {subsubsection}{\numberline {9.2.1}奖励黑客（Reward Hacking）}{70}{subsubsection.9.2.1}%
\contentsline {subsubsection}{\numberline {9.2.2}模式崩塌（Mode Collapse）}{71}{subsubsection.9.2.2}%
\contentsline {subsubsection}{\numberline {9.2.3}遗忘问题（Catastrophic Forgetting）}{71}{subsubsection.9.2.3}%
\contentsline {subsubsection}{\numberline {9.2.4}训练不稳定}{72}{subsubsection.9.2.4}%
\contentsline {subsubsection}{\numberline {9.2.5}长度退化}{73}{subsubsection.9.2.5}%
\contentsline {subsection}{\numberline {9.3}规模化中的挑战}{73}{subsection.9.3}%
\contentsline {subsubsection}{\numberline {9.3.1}计算资源管理}{73}{subsubsection.9.3.1}%
\contentsline {subsubsection}{\numberline {9.3.2}超参数敏感性}{74}{subsubsection.9.3.2}%
\contentsline {subsubsection}{\numberline {9.3.3}模式崩塌（Mode Collapse）}{74}{subsubsection.9.3.3}%
\contentsline {subsubsection}{\numberline {9.3.4}遗忘问题}{75}{subsubsection.9.3.4}%
\contentsline {subsubsection}{\numberline {9.3.5}训练不稳定}{75}{subsubsection.9.3.5}%
\contentsline {section}{\numberline {10}未来研究方向}{75}{section.10}%
\contentsline {subsection}{\numberline {10.1}算法改进}{75}{subsection.10.1}%
\contentsline {subsubsection}{\numberline {10.1.1}更高效的RL算法}{75}{subsubsection.10.1.1}%
\contentsline {subsubsection}{\numberline {10.1.2}更好的奖励设计}{76}{subsubsection.10.1.2}%
\contentsline {subsubsection}{\numberline {10.1.3}混合方法}{76}{subsubsection.10.1.3}%
\contentsline {subsection}{\numberline {10.2}推理时Scaling}{76}{subsection.10.2}%
\contentsline {subsubsection}{\numberline {10.2.1}高效搜索算法}{76}{subsubsection.10.2.1}%
\contentsline {subsubsection}{\numberline {10.2.2}动态计算分配}{77}{subsubsection.10.2.2}%
\contentsline {subsubsection}{\numberline {10.2.3}推理优化方法}{77}{subsubsection.10.2.3}%
\contentsline {subsection}{\numberline {10.3}多模态推理}{77}{subsection.10.3}%
\contentsline {subsubsection}{\numberline {10.3.1}视觉推理}{77}{subsubsection.10.3.1}%
\contentsline {subsubsection}{\numberline {10.3.2}跨模态推理}{78}{subsubsection.10.3.2}%
\contentsline {subsection}{\numberline {10.4}Agent与工具使用}{78}{subsection.10.4}%
\contentsline {subsubsection}{\numberline {10.4.1}工具增强推理}{78}{subsubsection.10.4.1}%
\contentsline {subsubsection}{\numberline {10.4.2}多Agent推理}{78}{subsubsection.10.4.2}%
\contentsline {subsection}{\numberline {10.5}理论研究}{79}{subsection.10.5}%
\contentsline {subsubsection}{\numberline {10.5.1}推理能力的本质}{79}{subsubsection.10.5.1}%
\contentsline {subsubsection}{\numberline {10.5.2}Scaling Laws}{79}{subsubsection.10.5.2}%
\contentsline {subsubsection}{\numberline {10.5.3}泛化理论}{79}{subsubsection.10.5.3}%
\contentsline {subsection}{\numberline {10.6}安全与对齐}{79}{subsection.10.6}%
\contentsline {subsubsection}{\numberline {10.6.1}安全推理}{79}{subsubsection.10.6.1}%
\contentsline {subsubsection}{\numberline {10.6.2}对齐挑战}{79}{subsubsection.10.6.2}%
\contentsline {subsection}{\numberline {10.7}开放问题}{80}{subsection.10.7}%
\contentsline {section}{\numberline {11}BitNet：1比特大语言模型}{80}{section.11}%
\contentsline {subsection}{\numberline {11.1}BitNet技术概述}{80}{subsection.11.1}%
\contentsline {subsubsection}{\numberline {11.1.1}发展历程}{80}{subsubsection.11.1.1}%
\contentsline {subsubsection}{\numberline {11.1.2}核心创新：BitLinear层}{81}{subsubsection.11.1.2}%
\contentsline {paragraph}{原始BitNet（1-bit）}{81}{Item.130}%
\contentsline {paragraph}{BitNet b1.58（1.58-bit）}{81}{equation.137}%
\contentsline {paragraph}{激活量化}{82}{equation.138}%
\contentsline {subsubsection}{\numberline {11.1.3}BitLinear前向传播}{82}{subsubsection.11.1.3}%
\contentsline {subsection}{\numberline {11.2}训练与推理流程}{82}{subsection.11.2}%
\contentsline {subsubsection}{\numberline {11.2.1}训练流程}{82}{subsubsection.11.2.1}%
\contentsline {paragraph}{直通估计器（Straight-Through Estimator, STE）}{82}{subsubsection.11.2.1}%
\contentsline {paragraph}{训练时的计算}{83}{equation.140}%
\contentsline {paragraph}{训练硬件要求}{83}{figure.caption.16}%
\contentsline {subsubsection}{\numberline {11.2.2}推理流程}{83}{subsubsection.11.2.2}%
\contentsline {paragraph}{bitnet.cpp推理框架}{84}{Item.133}%
\contentsline {subsection}{\numberline {11.3}性能对比分析}{84}{subsection.11.3}%
\contentsline {subsubsection}{\numberline {11.3.1}模型大小与内存消耗}{84}{subsubsection.11.3.1}%
\contentsline {subsubsection}{\numberline {11.3.2}推理延迟}{84}{subsubsection.11.3.2}%
\contentsline {paragraph}{100B模型CPU运行}{85}{table.caption.18}%
\contentsline {subsubsection}{\numberline {11.3.3}能耗对比}{85}{subsubsection.11.3.3}%
\contentsline {paragraph}{能耗优势来源}{85}{table.caption.19}%
\contentsline {subsubsection}{\numberline {11.3.4}模型性能对比}{85}{subsubsection.11.3.4}%
\contentsline {subsubsection}{\numberline {11.3.5}零样本任务性能}{86}{subsubsection.11.3.5}%
\contentsline {subsection}{\numberline {11.4}与传统量化方法对比}{86}{subsection.11.4}%
\contentsline {subsubsection}{\numberline {11.4.1}后训练量化（PTQ）vs BitNet}{86}{subsubsection.11.4.1}%
\contentsline {subsubsection}{\numberline {11.4.2}为什么不能从FP16转换？}{86}{subsubsection.11.4.2}%
\contentsline {subsection}{\numberline {11.5}硬件优化与未来展望}{87}{subsection.11.5}%
\contentsline {subsubsection}{\numberline {11.5.1}当前硬件支持}{87}{subsubsection.11.5.1}%
\contentsline {subsubsection}{\numberline {11.5.2}专用硬件前景}{87}{subsubsection.11.5.2}%
\contentsline {subsubsection}{\numberline {11.5.3}BitNet与RL训练的结合}{87}{subsubsection.11.5.3}%
\contentsline {subsection}{\numberline {11.6}官方模型资源}{88}{subsection.11.6}%
\contentsline {subsection}{\numberline {11.7}小结}{88}{subsection.11.7}%
\contentsline {section}{\numberline {12}总结}{88}{section.12}%
\contentsline {subsection}{\numberline {12.1}核心结论}{88}{subsection.12.1}%
\contentsline {subsection}{\numberline {12.2}本报告的创新贡献}{89}{subsection.12.2}%
\contentsline {subsection}{\numberline {12.3}实践建议}{90}{subsection.12.3}%
\contentsline {subsection}{\numberline {12.4}展望}{91}{subsection.12.4}%
