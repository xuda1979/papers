\documentclass[12pt,a4paper]{article}
\usepackage{ctex}
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,calc}
\usepackage{float}
\usepackage{subcaption}
\usepackage{fancyhdr}
\usepackage{titlesec}

\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm,headheight=15pt}

% 代码样式
\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{gray!10},
    keywordstyle=\color{blue},
    commentstyle=\color{green!60!black},
    stringstyle=\color{red},
    literate={é}{{\'e}}1 {ö}{{\"o}}1 {ü}{{\"u}}1
}

% 定理环境
\newtheorem{theorem}{定理}[section]
\newtheorem{lemma}[theorem]{引理}
\newtheorem{definition}[theorem]{定义}
\newtheorem{remark}[theorem]{注记}

% 页眉页脚
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{大语言模型强化学习训练技术报告}
\fancyhead[R]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}

% 颜色定义
\definecolor{deepblue}{RGB}{0,51,102}
\definecolor{deepgreen}{RGB}{0,102,51}
\definecolor{deepred}{RGB}{153,0,0}

\title{\textbf{大语言模型的强化学习训练}\\[0.5cm]
\Large 提升推理、数学与代码能力的自我学习方法\\[0.3cm]
\large 技术详细报告（扩展版）}
\author{许达\\未来院三室}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
本报告详细介绍了利用强化学习（Reinforcement Learning, RL）训练大语言模型（Large Language Models, LLMs）以提升其推理、数学和代码生成能力的最新技术进展。我们重点分析了\textbf{不依赖人类反馈}的自我学习方法，系统性地介绍了以下核心方法：
\begin{itemize}
    \item \textbf{策略梯度方法}：PPO、GRPO、REINFORCE、RLOO等
    \item \textbf{自我训练方法}：ReST、ReST-EM、STaR、Quiet-STaR、V-STaR、Expert Iteration等
    \item \textbf{偏好优化方法}：DPO、Online DPO、IPO、Self-Rewarding等
    \item \textbf{采样筛选方法}：Rejection Sampling Fine-tuning (RSF)、RAFT等
    \item \textbf{过程奖励方法}：PRM、ORM、Math-Shepherd、PRIME等
    \item \textbf{搜索增强方法}：MCTS增强推理、Best-of-N采样等
\end{itemize}
报告深入分析了DeepSeek-R1的纯RL训练范式、OpenAI o1的推理模型架构、Qwen系列、WizardMath/WizardCoder、Marco-o1、Kimi k1.5等重要模型的技术方案。报告涵盖了核心算法的数学推导、实现细节、训练技巧以及主要研究机构的技术路线对比。此外，本报告创新性地提出了多种新型算法框架，包括：\textbf{自适应分层奖励优化（AHRO）}、\textbf{认知架构引导策略学习（CAPL）}、\textbf{动态思维链自演化（DCoT-SE）}、\textbf{多粒度推理一致性强化（MGRCR）}、\textbf{元认知反馈回路优化（MFLO）}等。本报告还深入介绍了\textbf{BitNet}系列1比特大语言模型技术，包括其训练推理机制、与传统LLM在内存消耗、能耗和性能方面的全面对比，旨在为研究人员和工程师提供全面的技术参考和未来研究方向。
\end{abstract}

\tableofcontents
\newpage

%===========================================
\section{引言}
%===========================================

\subsection{研究背景与动机}

大语言模型在自然语言处理领域取得了突破性进展，但在复杂推理任务上仍存在明显局限。传统的监督微调（Supervised Fine-Tuning, SFT）方法面临以下核心挑战：

\begin{enumerate}[label=(\arabic*)]
    \item \textbf{标注成本高昂}：高质量推理数据需要领域专家标注，成本极高且难以规模化。
    \item \textbf{能力上限受限}：模型只能学习到人类标注者已知的解法，无法突破人类认知边界。
    \item \textbf{泛化能力不足}：基于模仿学习的方法在分布外（Out-of-Distribution）问题上表现较差。
    \item \textbf{标注质量不一}：复杂推理任务的标注质量难以保证，错误标注会误导模型。
\end{enumerate}

强化学习提供了一种全新的训练范式，其核心优势在于：

\begin{itemize}
    \item \textbf{自主探索}：模型通过与环境交互自主发现解题策略
    \item \textbf{可扩展性}：无需人工标注即可持续训练
    \item \textbf{突破上限}：有可能发现超越人类已知的新方法
    \item \textbf{自我改进}：通过迭代优化持续提升性能
\end{itemize}

\subsection{研究进展时间线}

强化学习在LLM训练中的应用经历了以下关键阶段：

\begin{table}[H]
\centering
\caption{LLM强化学习训练里程碑}
\begin{tabular}{llp{8cm}}
\toprule
\textbf{时间} & \textbf{工作} & \textbf{主要贡献} \\
\midrule
2022.03 & InstructGPT & 首次大规模应用RLHF，开创指令跟随范式 \\
2022.05 & STaR & 提出自我迭代推理训练范式 \\
2022.09 & SPIN & 自博弈微调方法 \\
2022.12 & Constitutional AI & 提出RLAIF，用AI反馈替代人类反馈 \\
2023.02 & AlphaCode & 大规模采样+聚类用于竞赛编程 \\
2023.05 & Let's Verify & 系统研究过程奖励模型(PRM)在数学推理中的应用 \\
2023.06 & WizardMath & Evol-Instruct + RLEIF方法 \\
2023.08 & ReST & Google提出强化自我训练，迭代改进模型 \\
2023.09 & WizardCoder & 代码生成的强化学习优化 \\
2023.10 & ReST-EM & 结合期望最大化的自我训练方法 \\
2023.12 & Self-Rewarding LM & Meta提出自奖励语言模型，无需外部奖励 \\
2024.01 & Quiet-STaR & 隐式思考链，每个token位置进行内部推理 \\
2024.01 & KTO & 基于前景理论的偏好优化 \\
2024.02 & DeepSeekMath & 提出GRPO算法，简化RL训练流程 \\
2024.03 & RAFT/RSF & 奖励排序微调和拒绝采样方法 \\
2024.03 & ORPO & 统一SFT和偏好优化 \\
2024.04 & SimPO & 简化DPO，移除参考模型 \\
2024.05 & V-STaR & 结合验证器的自我训练 \\
2024.06 & Math-Shepherd & 过程奖励模型用于数学推理 \\
2024.07 & RLOO & REINFORCE Leave-One-Out基线优化 \\
2024.08 & PRIME & 隐式过程奖励学习 \\
2024.08 & OpenR & 开源推理模型训练框架 \\
2024.09 & OpenAI o1 & 推理模型范式，展示推理时scaling \\
2024.10 & Qwen2.5-Math & 结合多种RL方法的数学推理模型 \\
2024.11 & Marco-o1 & 阿里开源推理模型，MCTS增强 \\
2024.12 & rStar & 自博弈互相推理方法 \\
2025.01 & DeepSeek-R1 & 首个开源纯RL推理模型，涌现长链推理能力 \\
2025.01 & Kimi k1.5 & Moonshot AI推理模型，长上下文RL \\
\bottomrule
\end{tabular}
\end{table}

\subsection{本报告结构}

本报告组织如下：
\begin{itemize}
    \item \textbf{第2节}：强化学习基础理论（MDP、策略梯度、GAE等）
    \item \textbf{第3节}：核心算法详解（PPO、GRPO、DPO、RLAIF、奖励设计）
    \item \textbf{第4节}：DeepSeek-R1技术详解（训练流程、Zero实验、蒸馏）
    \item \textbf{第5节}：OpenAI o1技术分析（官方信息、技术推测、推理时Scaling）
    \item \textbf{第6节}：其他主要研究方法（涵盖ReST、STaR、Expert Iteration、Self-Rewarding、REINFORCE/RLOO、RAFT/RSF、V-STaR、PRM/ORM、PRIME、Online DPO、WizardMath、Qwen系列、Marco-o1、Kimi k1.5等重要方法和模型）
    \item \textbf{第7节}：创新算法框架（AHRO、CAPL、DCoT-SE、MGRCR、MFLO等）
    \item \textbf{第8节}：实现细节与训练技巧
    \item \textbf{第9节}：失败尝试与经验教训
    \item \textbf{第10节}：未来研究方向
    \item \textbf{第11节}：BitNet系列1比特大语言模型
    \item \textbf{第12节}：总结
\end{itemize}

%===========================================
\section{强化学习基础理论}
%===========================================

\subsection{马尔可夫决策过程}

强化学习问题通常建模为马尔可夫决策过程（Markov Decision Process, MDP），形式化定义为五元组$(\mathcal{S}, \mathcal{A}, P, R, \gamma)$：

\begin{definition}[马尔可夫决策过程]
\begin{itemize}
    \item $\mathcal{S}$：状态空间（State Space）
    \item $\mathcal{A}$：动作空间（Action Space）
    \item $P: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow [0,1]$：状态转移概率
    \item $R: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$：奖励函数
    \item $\gamma \in [0,1]$：折扣因子
\end{itemize}
\end{definition}

在LLM场景下，MDP的具体定义为：
\begin{itemize}
    \item \textbf{状态}：当前已生成的token序列（包括prompt），形式化为$s_t = (x_1, x_2, \ldots, x_{n+t})$，其中$x_1, \ldots, x_n$为prompt，$x_{n+1}, \ldots, x_{n+t}$为已生成tokens
    \item \textbf{动作}：下一个要生成的token，$a_t \in \mathcal{V}$，$\mathcal{V}$为词汇表
    \item \textbf{转移}：确定性转移，$s_{t+1} = s_t \oplus a_t$（序列拼接）
    \item \textbf{奖励}：通常在序列结束时给予（稀疏奖励），$r_t = 0$ for $t < T$, $r_T = R(s_T)$
    \item \textbf{终止条件}：生成EOS token或达到最大长度
\end{itemize}

\subsection{部分可观测马尔可夫决策过程（POMDP）}

在实际LLM应用中，完全MDP假设可能过于理想化。更精确的建模应考虑部分可观测性：

\begin{definition}[POMDP扩展]
POMDP定义为七元组$(\mathcal{S}, \mathcal{A}, P, R, \Omega, O, \gamma)$，额外包含：
\begin{itemize}
    \item $\Omega$：观测空间
    \item $O: \mathcal{S} \times \mathcal{A} \rightarrow \Delta(\Omega)$：观测函数
\end{itemize}
\end{definition}

对于LLM，POMDP视角捕捉了以下现实约束：
\begin{itemize}
    \item 模型无法直接观测用户真实意图
    \item 问题的完整上下文可能超出context window
    \item 外部世界状态（如数据库、API）不完全可知
\end{itemize}

\subsection{策略梯度方法}

策略梯度方法直接优化参数化策略$\pi_\theta$，目标是最大化期望累积奖励：

\begin{equation}
J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}\left[\sum_{t=0}^{T} \gamma^t r_t\right]
\end{equation}

其中$\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \ldots)$表示一条轨迹。

\begin{theorem}[策略梯度定理]
策略梯度可表示为：
\begin{equation}
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}\left[\sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot G_t\right]
\end{equation}
其中$G_t = \sum_{k=t}^{T} \gamma^{k-t} r_k$是从时刻$t$开始的累积回报。
\end{theorem}

为降低方差，通常引入基线函数$b(s_t)$，使用优势函数（Advantage Function）：
\begin{equation}
A^{\pi}(s_t, a_t) = Q^{\pi}(s_t, a_t) - V^{\pi}(s_t)
\end{equation}

其中$Q^{\pi}(s,a)$是动作价值函数，$V^{\pi}(s)$是状态价值函数。

\subsection{值函数估计}

状态价值函数定义为从状态$s$出发，遵循策略$\pi$的期望回报：
\begin{equation}
V^{\pi}(s) = \mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty} \gamma^t r_t \mid s_0 = s\right]
\end{equation}

动作价值函数定义为：
\begin{equation}
Q^{\pi}(s, a) = \mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty} \gamma^t r_t \mid s_0 = s, a_0 = a\right]
\end{equation}

两者满足贝尔曼方程：
\begin{align}
V^{\pi}(s) &= \sum_a \pi(a|s) Q^{\pi}(s, a) \\
Q^{\pi}(s, a) &= R(s, a) + \gamma \sum_{s'} P(s'|s, a) V^{\pi}(s')
\end{align}

\subsection{广义优势估计（GAE）}

广义优势估计（Generalized Advantage Estimation）是一种高效的优势函数估计方法，平衡偏差和方差：

\begin{equation}
\hat{A}_t^{GAE(\gamma,\lambda)} = \sum_{l=0}^{\infty} (\gamma\lambda)^l \delta_{t+l}
\end{equation}

其中$\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$是时序差分（TD）误差。

$\lambda$参数控制偏差-方差权衡：
\begin{itemize}
    \item $\lambda = 0$：单步TD估计，低方差高偏差
    \item $\lambda = 1$：蒙特卡洛估计，高方差低偏差
    \item $\lambda \in (0,1)$：折中方案
\end{itemize}

\subsection{信任域方法}

信任域方法是一类重要的策略优化算法，核心思想是在每次更新时限制策略变化幅度。

\begin{definition}[信任域策略优化]
信任域方法的目标是：
\begin{equation}
\max_\theta \mathbb{E}_{s \sim \rho_{\theta_{old}}, a \sim \pi_{\theta_{old}}}\left[\frac{\pi_\theta(a|s)}{\pi_{\theta_{old}}(a|s)} A^{\pi_{\theta_{old}}}(s,a)\right]
\end{equation}
约束条件：
\begin{equation}
\mathbb{E}_{s \sim \rho_{\theta_{old}}}\left[\mathbb{D}_{KL}[\pi_{\theta_{old}}(\cdot|s) \| \pi_\theta(\cdot|s)]\right] \leq \delta
\end{equation}
\end{definition}

\subsection{重要性采样与Off-Policy学习}

重要性采样允许使用旧策略采集的数据更新新策略：

\begin{equation}
\mathbb{E}_{a \sim \pi_{\theta_{old}}}[f(a)] = \mathbb{E}_{a \sim \pi_\theta}\left[\frac{\pi_{\theta_{old}}(a)}{\pi_\theta(a)} f(a)\right]
\end{equation}

\textbf{方差分析}：重要性采样的方差为：
\begin{equation}
\text{Var}\left[\frac{\pi_{\theta_{old}}(a)}{\pi_\theta(a)} f(a)\right] = \mathbb{E}\left[\left(\frac{\pi_{\theta_{old}}(a)}{\pi_\theta(a)}\right)^2 f(a)^2\right] - \left(\mathbb{E}[f(a)]\right)^2
\end{equation}

当$\pi_{\theta_{old}}$与$\pi_\theta$差异过大时，方差会急剧增加，这是限制策略变化的核心原因。

\subsection{自然梯度与Fisher信息矩阵}

自然梯度考虑参数空间的几何结构：

\begin{definition}[Fisher信息矩阵]
\begin{equation}
F_\theta = \mathbb{E}_{s,a}\left[\nabla_\theta \log \pi_\theta(a|s) \nabla_\theta \log \pi_\theta(a|s)^T\right]
\end{equation}
\end{definition}

自然梯度为：
\begin{equation}
\tilde{\nabla}_\theta J(\theta) = F_\theta^{-1} \nabla_\theta J(\theta)
\end{equation}

自然梯度的优势：
\begin{itemize}
    \item 对参数化不变
    \item 更好地反映策略空间的几何
    \item 训练更稳定
\end{itemize}

%===========================================
\section{核心算法详解}
%===========================================

\subsection{PPO算法}

\subsubsection{算法动机}

Proximal Policy Optimization（PPO）是目前最广泛使用的策略梯度算法之一。其核心动机是解决策略梯度方法的两个关键问题：

\begin{enumerate}
    \item \textbf{样本效率低}：传统策略梯度是on-policy方法，每次更新后数据即失效
    \item \textbf{训练不稳定}：策略更新步长难以控制，容易导致性能崩溃
\end{enumerate}

PPO通过引入重要性采样和裁剪机制，允许多次使用同一批数据进行更新，同时限制策略变化幅度。

\subsubsection{数学推导}

PPO的核心是裁剪目标函数。首先定义策略比率：
\begin{equation}
r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}
\end{equation}

原始的重要性采样目标为：
\begin{equation}
L^{IS}(\theta) = \mathbb{E}_t\left[r_t(\theta) \hat{A}_t\right]
\end{equation}

但直接优化该目标可能导致策略变化过大。PPO引入裁剪机制：

\begin{equation}
L^{CLIP}(\theta) = \mathbb{E}_t\left[\min\left(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t\right)\right]
\end{equation}

其中$\epsilon$是裁剪参数，通常取$0.1$到$0.2$。

\begin{remark}
裁剪机制的直觉解释：
\begin{itemize}
    \item 当$\hat{A}_t > 0$（好动作）：限制$r_t(\theta) \leq 1+\epsilon$，防止过度增加该动作概率
    \item 当$\hat{A}_t < 0$（坏动作）：限制$r_t(\theta) \geq 1-\epsilon$，防止过度减少该动作概率
\end{itemize}
\end{remark}

\subsubsection{完整目标函数}

PPO的完整目标函数包含三部分：

\begin{equation}
L(\theta) = L^{CLIP}(\theta) - c_1 L^{VF}(\theta) + c_2 S[\pi_\theta]
\end{equation}

其中：
\begin{itemize}
    \item $L^{VF}(\theta) = (V_\theta(s_t) - V_t^{target})^2$：值函数损失
    \item $S[\pi_\theta] = -\sum_a \pi_\theta(a|s) \log \pi_\theta(a|s)$：熵正则化项
    \item $c_1, c_2$：平衡系数
\end{itemize}

\subsubsection{算法流程}

\begin{algorithm}[H]
\caption{PPO算法}
\begin{algorithmic}[1]
\STATE 初始化策略网络$\pi_\theta$和值网络$V_\phi$
\FOR{iteration $= 1, 2, \ldots$}
    \FOR{actor $= 1, 2, \ldots, N$}
        \STATE 使用当前策略$\pi_{\theta_{old}}$采集轨迹
        \STATE 计算奖励和优势估计$\hat{A}_t$
    \ENDFOR
    \FOR{epoch $= 1, 2, \ldots, K$}
        \FOR{minibatch in collected data}
            \STATE 计算策略比率$r_t(\theta)$
            \STATE 计算裁剪目标$L^{CLIP}$
            \STATE 更新策略：$\theta \leftarrow \theta + \alpha \nabla_\theta L^{CLIP}$
            \STATE 更新值函数：$\phi \leftarrow \phi - \beta \nabla_\phi L^{VF}$
        \ENDFOR
    \ENDFOR
    \STATE $\theta_{old} \leftarrow \theta$
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsubsection{PPO的变体与改进}

\textbf{1. PPO-Penalty}：使用KL惩罚代替裁剪
\begin{equation}
L^{KLPEN}(\theta) = \mathbb{E}_t\left[r_t(\theta)\hat{A}_t - \beta \mathbb{D}_{KL}[\pi_{\theta_{old}} \| \pi_\theta]\right]
\end{equation}

自适应调整$\beta$：
\begin{itemize}
    \item 如果$\mathbb{D}_{KL} > d_{target} \times 1.5$：$\beta \leftarrow 2\beta$
    \item 如果$\mathbb{D}_{KL} < d_{target} / 1.5$：$\beta \leftarrow \beta/2$
\end{itemize}

\textbf{2. Dual-Clip PPO}：双边裁剪处理负优势
\begin{equation}
L^{DCLIP}(\theta) = \mathbb{E}_t\left[\max\left(\min\left(r_t(\theta)\hat{A}_t, \text{clip}(r_t, 1-\epsilon, 1+\epsilon)\hat{A}_t\right), c\hat{A}_t\right)\right]
\end{equation}
其中$c > 1$是下界系数，防止负优势时策略变化过快。

\textbf{3. PPO with Rollback}：检测异常并回滚
\begin{algorithmic}[1]
\IF{$\mathbb{D}_{KL} > d_{max}$ or reward\_drop $>$ threshold}
    \STATE $\theta \leftarrow \theta_{checkpoint}$
    \STATE 减小学习率
\ENDIF
\end{algorithmic}

\subsubsection{PPO在LLM中的特殊考虑}

\textbf{Token级vs序列级}：LLM中的PPO可以在两个层面操作：
\begin{itemize}
    \item \textbf{Token级}：每个token作为一个动作，计算token级优势
    \begin{equation}
    \hat{A}_t^{token} = r_t + \gamma V(s_{t+1}) - V(s_t)
    \end{equation}
    
    \item \textbf{序列级}：整个回答作为一个动作，简化训练
    \begin{equation}
    \hat{A}^{seq} = R(q, o) - V(q)
    \end{equation}
\end{itemize}

\textbf{Value Head设计}：
\begin{itemize}
    \item 共享backbone + 独立value head
    \item 完全独立的value网络（更稳定但成本高）
    \item Frozen backbone + trainable value head
\end{itemize}

\subsection{GRPO算法}

\subsubsection{算法动机}

Group Relative Policy Optimization（GRPO）是DeepSeek团队提出的简化RL算法，主要解决PPO在LLM训练中的以下问题：

\begin{enumerate}
    \item \textbf{Critic网络开销大}：需要训练额外的值函数网络，内存和计算成本高
    \item \textbf{值函数估计不准}：LLM生成任务中，状态空间巨大，值函数难以准确估计
    \item \textbf{训练复杂度高}：需要同时优化Actor和Critic，超参数调节困难
\end{enumerate}

GRPO的核心思想是：\textbf{用组内相对排名代替绝对价值估计}。

\subsubsection{数学推导}

对于每个问题$q$，GRPO采样一组回答$\{o_1, o_2, \ldots, o_G\}$，计算每个回答的奖励$\{r_1, r_2, \ldots, r_G\}$。

优势估计通过组内归一化计算：
\begin{equation}
\hat{A}_i = \frac{r_i - \text{mean}(\mathbf{r})}{\text{std}(\mathbf{r}) + \epsilon}
\end{equation}

其中$\mathbf{r} = (r_1, r_2, \ldots, r_G)$，$\epsilon$是数值稳定性常数。

GRPO的目标函数为：
\begin{equation}
\mathcal{J}_{GRPO}(\theta) = \mathbb{E}_{q \sim \mathcal{D}, \{o_i\} \sim \pi_{\theta_{old}}}\left[\frac{1}{G}\sum_{i=1}^{G} \mathcal{L}_i(\theta)\right]
\end{equation}

其中每个样本的损失为：
\begin{equation}
\mathcal{L}_i(\theta) = \min\left(\frac{\pi_\theta(o_i|q)}{\pi_{\theta_{old}}(o_i|q)}\hat{A}_i, \text{clip}\left(\frac{\pi_\theta(o_i|q)}{\pi_{\theta_{old}}(o_i|q)}, 1-\epsilon, 1+\epsilon\right)\hat{A}_i\right)
\end{equation}

同时加入KL散度约束防止策略偏离过远：
\begin{equation}
\mathcal{L}_{total}(\theta) = \mathcal{J}_{GRPO}(\theta) - \beta \cdot \mathbb{D}_{KL}[\pi_\theta \| \pi_{ref}]
\end{equation}

\subsubsection{与PPO的对比}

\begin{table}[H]
\centering
\caption{GRPO与PPO对比}
\begin{tabular}{lcc}
\toprule
\textbf{特性} & \textbf{PPO} & \textbf{GRPO} \\
\midrule
Critic网络 & 需要 & 不需要 \\
内存占用 & 高（2倍模型） & 低（1倍模型） \\
优势估计 & GAE & 组内归一化 \\
采样方式 & 单样本/少量 & 组采样（多个） \\
值函数损失 & 需要优化 & 无 \\
实现复杂度 & 高 & 低 \\
训练稳定性 & 依赖Critic质量 & 组内比较更稳定 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{算法流程}

\begin{algorithm}[H]
\caption{GRPO算法}
\begin{algorithmic}[1]
\STATE 初始化策略网络$\pi_\theta$，设置参考策略$\pi_{ref} = \pi_\theta$
\FOR{iteration $= 1, 2, \ldots$}
    \FOR{每个问题 $q$ in batch}
        \STATE 采样$G$个回答：$\{o_1, \ldots, o_G\} \sim \pi_{\theta_{old}}(\cdot|q)$
        \STATE 计算奖励：$r_i = R(q, o_i)$，$i = 1, \ldots, G$
        \STATE 归一化优势：$\hat{A}_i = (r_i - \bar{r}) / (\sigma_r + \epsilon)$
    \ENDFOR
    \FOR{epoch $= 1, \ldots, K$}
        \STATE 计算策略比率和裁剪损失
        \STATE 计算KL惩罚
        \STATE 更新策略：$\theta \leftarrow \theta + \alpha \nabla_\theta \mathcal{L}_{total}$
    \ENDFOR
    \STATE $\theta_{old} \leftarrow \theta$
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsubsection{GRPO的理论分析}

\begin{theorem}[GRPO的无偏性]
在组大小$G \rightarrow \infty$时，GRPO的梯度估计是无偏的：
\begin{equation}
\lim_{G \rightarrow \infty} \mathbb{E}\left[\nabla_\theta \mathcal{J}_{GRPO}(\theta)\right] = \nabla_\theta J(\theta)
\end{equation}
\end{theorem}

\textbf{证明sketch}：
\begin{enumerate}
    \item 当$G \rightarrow \infty$时，$\bar{r} \rightarrow \mathbb{E}_{\pi_\theta}[r]$
    \item 归一化后的优势$\hat{A}_i$近似于真实优势的标准化版本
    \item 由于裁剪是对称的，期望不变
\end{enumerate}

\textbf{方差分析}：GRPO的方差主要来源于：
\begin{equation}
\text{Var}[\hat{A}] = \frac{1}{G-1}\text{Var}[r] + O(1/G^2)
\end{equation}

增加组大小$G$可以有效降低方差，但会增加计算成本。

\subsubsection{GRPO变体}

\textbf{1. Weighted GRPO}：引入样本权重
\begin{equation}
\hat{A}_i^{weighted} = w_i \cdot \frac{r_i - \bar{r}_w}{\sigma_r + \epsilon}
\end{equation}
其中$w_i$可以基于问题难度、样本新颖性等因素确定。

\textbf{2. Hierarchical GRPO}：分层组采样
\begin{enumerate}
    \item 按问题难度分层
    \item 每层独立进行组内归一化
    \item 加权聚合不同层的梯度
\end{enumerate}

\textbf{3. Adaptive GRPO}：自适应组大小
\begin{equation}
G_q = G_{base} \cdot \exp\left(\alpha \cdot \text{difficulty}(q)\right)
\end{equation}
难题使用更大的组以获得更稳定的梯度估计。

\subsection{Direct Preference Optimization (DPO)}

DPO是另一种无需显式奖励模型的RL方法。

\subsubsection{核心思想}

DPO直接从偏好数据学习，避免了奖励建模和采样的复杂性。

\begin{definition}[Bradley-Terry模型]
给定偏好对$(y_w, y_l)$，偏好概率为：
\begin{equation}
P(y_w \succ y_l | x) = \sigma(r(x, y_w) - r(x, y_l))
\end{equation}
其中$\sigma$是sigmoid函数。
\end{definition}

\subsubsection{DPO目标函数}

通过重参数化，DPO导出直接优化策略的目标：
\begin{equation}
\mathcal{L}_{DPO}(\theta) = -\mathbb{E}_{(x,y_w,y_l)}\left[\log \sigma\left(\beta \log\frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \beta \log\frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)}\right)\right]
\end{equation}

\subsubsection{DPO vs RLHF}

\begin{table}[H]
\centering
\caption{DPO与RLHF对比}
\begin{tabular}{lcc}
\toprule
\textbf{方面} & \textbf{DPO} & \textbf{RLHF (PPO)} \\
\midrule
奖励模型 & 隐式 & 显式 \\
采样需求 & 无 & 大量在线采样 \\
内存消耗 & 低 & 高 \\
训练稳定性 & 高 & 需要调参 \\
数据效率 & 高 & 中 \\
探索能力 & 弱 & 强 \\
适用场景 & 偏好对齐 & 复杂任务优化 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Reinforcement Learning from AI Feedback (RLAIF)}

\subsubsection{核心框架}

RLAIF使用AI模型替代人类提供反馈：

\begin{enumerate}
    \item \textbf{生成阶段}：策略模型生成多个候选回答
    \item \textbf{评估阶段}：评判模型对候选进行打分/排序
    \item \textbf{优化阶段}：使用AI反馈进行RL训练
\end{enumerate}

\subsubsection{评判模型设计}

\textbf{评分Prompt模板}：
\begin{verbatim}
请评估以下回答的质量（1-10分）：
问题：{question}
回答：{answer}

评分标准：
- 准确性（40%）
- 完整性（30%）
- 清晰度（20%）
- 简洁性（10%）

请给出分数和理由：
\end{verbatim}

\textbf{对比Prompt模板}：
\begin{verbatim}
以下是同一问题的两个回答，请选择更好的一个：
问题：{question}
回答A：{answer_a}
回答B：{answer_b}

请选择A或B，并解释原因：
\end{verbatim}

\subsubsection{RLAIF的变体}

\textbf{1. Self-RLAIF}：模型自评
\begin{equation}
r(x, y) = \mathbb{E}_{prompt \sim \mathcal{P}}\left[\pi_{eval}(\text{"good"} | prompt, x, y)\right]
\end{equation}

\textbf{2. Ensemble RLAIF}：多评判者集成
\begin{equation}
r_{ensemble}(x, y) = \frac{1}{K}\sum_{k=1}^{K} r_k(x, y)
\end{equation}

\textbf{3. Debate RLAIF}：辩论式评估
让多个模型对回答质量进行辩论，综合辩论结果得出最终评分。

\subsection{奖励函数设计}

奖励函数是RL训练的核心，设计原则包括：

\subsubsection{可验证奖励}

对于数学和代码任务，可以设计自动验证的奖励：

\textbf{数学任务奖励}：
\begin{equation}
R_{math}(q, o) = \begin{cases}
+1 & \text{如果答案正确} \\
0 & \text{如果答案错误}
\end{cases}
\end{equation}

答案匹配可以采用多种方式：
\begin{itemize}
    \item 精确匹配：字符串完全相同
    \item 数值匹配：数值在容差范围内相等
    \item 符号匹配：使用SymPy等符号计算库验证等价性
\end{itemize}

\textbf{代码任务奖励}：
\begin{equation}
R_{code}(q, o) = \frac{\text{通过的测试用例数}}{\text{总测试用例数}}
\end{equation}

还可以加入额外奖励：
\begin{itemize}
    \item 编译成功奖励
    \item 运行效率奖励
    \item 代码风格奖励
\end{itemize}

\subsubsection{格式奖励}

为确保输出格式正确，可以加入格式奖励：
\begin{equation}
R_{format}(o) = \begin{cases}
r_+ & \text{如果格式正确（包含指定标签）} \\
r_- & \text{如果格式错误}
\end{cases}
\end{equation}

例如，要求输出包含\texttt{<think>...</think>}推理过程和\texttt{<answer>...</answer>}最终答案。

\subsubsection{长度惩罚}

防止输出过长或过短：
\begin{equation}
R_{length}(o) = -\lambda \cdot \max(0, |o| - L_{max})
\end{equation}

\subsubsection{复合奖励}

综合多种奖励：
\begin{equation}
R_{total} = \alpha R_{accuracy} + \beta R_{format} + \gamma R_{length}
\end{equation}

\subsubsection{过程奖励与结果奖励的数学分析}

\textbf{结果奖励模型（ORM）}：
\begin{equation}
R_{ORM}(q, o) = f(\text{final\_answer}(o), \text{ground\_truth}(q))
\end{equation}

\textbf{过程奖励模型（PRM）}：
\begin{equation}
R_{PRM}(q, o) = \sum_{t=1}^{T} \gamma^{T-t} r_t(s_t)
\end{equation}
其中$r_t$是第$t$步的奖励。

\textbf{混合奖励}：
\begin{equation}
R_{hybrid} = \alpha R_{ORM} + (1-\alpha) R_{PRM}
\end{equation}

\textbf{理论分析}：PRM提供更密集的信号，但存在以下权衡：
\begin{itemize}
    \item \textbf{信用分配}：PRM更精确地分配奖励到正确的步骤
    \item \textbf{标注成本}：PRM需要步骤级标注，成本远高于ORM
    \item \textbf{泛化性}：PRM对步骤格式敏感，泛化能力可能较差
    \item \textbf{Reward Hacking}：PRM更容易被针对特定步骤的黑客攻击
\end{itemize}

\subsubsection{奖励塑形（Reward Shaping）}

\begin{theorem}[势能奖励塑形]
设$\Phi: \mathcal{S} \rightarrow \mathbb{R}$为势函数，定义塑形奖励：
\begin{equation}
R'(s, a, s') = R(s, a, s') + \gamma \Phi(s') - \Phi(s)
\end{equation}
则最优策略不变。
\end{theorem}

\textbf{LLM中的应用}：
\begin{itemize}
    \item 中间状态奖励：对包含关键词/结构的状态给予正势能
    \item 进度奖励：$\Phi(s) = \alpha \cdot \text{progress}(s)$
    \item 多样性奖励：$\Phi(s) = -\beta \cdot \text{repetition}(s)$
\end{itemize}

%===========================================
\section{DeepSeek-R1技术详解}
%===========================================

\subsection{项目概述}

DeepSeek-R1是DeepSeek团队于2025年1月发布的推理模型，是首个公开技术细节的纯RL训练推理模型。其核心贡献包括：

\begin{enumerate}
    \item 证明了\textbf{纯RL训练可以涌现复杂推理能力}
    \item 展示了\textbf{无需人类标注的自我学习}范式
    \item 提出了\textbf{高效的GRPO算法}
    \item 开源了\textbf{完整的模型和蒸馏版本}
\end{enumerate}

\subsection{训练流程}

DeepSeek-R1采用四阶段训练流程：

\subsubsection{Stage 1: 冷启动SFT}

\textbf{目的}：为RL训练提供良好的初始化

\textbf{数据}：数千条长链推理（Chain-of-Thought）数据，包括：
\begin{itemize}
    \item 人工标注的详细推理过程
    \item 已有模型生成的高质量推理链
    \item 格式化的推理模板
\end{itemize}

\textbf{训练细节}：
\begin{itemize}
    \item 学习率：$1 \times 10^{-5}$
    \item 训练轮数：2-3 epochs
    \item 数据量：约2000-5000条
\end{itemize}

\subsubsection{Stage 2: 推理RL}

\textbf{目的}：通过RL训练提升推理能力

\textbf{算法}：GRPO

\textbf{奖励设计}：
\begin{equation}
R = R_{accuracy} + R_{format}
\end{equation}

\begin{itemize}
    \item $R_{accuracy}$：答案正确性，通过规则验证
    \item $R_{format}$：格式正确性，检查是否包含必要标签
\end{itemize}

\textbf{关键超参数}：
\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
\textbf{参数} & \textbf{值} \\
\midrule
组大小 $G$ & 64 \\
裁剪参数 $\epsilon$ & 0.2 \\
KL系数 $\beta$ & 0.01 \\
学习率 & $1 \times 10^{-6}$ \\
批大小 & 512 \\
最大长度 & 32768 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Stage 3: 拒绝采样SFT}

\textbf{目的}：利用RL模型生成高质量数据，进行SFT以提升可读性

\textbf{流程}：
\begin{enumerate}
    \item 使用Stage 2的RL检查点生成大量回答
    \item 根据正确性和质量筛选
    \item 对筛选后的数据进行SFT
\end{enumerate}

\textbf{筛选标准}：
\begin{itemize}
    \item 答案正确
    \item 推理过程清晰
    \item 语言流畅，无混杂
    \item 长度适中
\end{itemize}

\subsubsection{Stage 4: 全场景RL}

\textbf{目的}：扩展到更多任务类型，保持通用能力

\textbf{任务类型}：
\begin{itemize}
    \item 数学推理
    \item 代码生成
    \item 逻辑推理
    \item 常识问答
    \item 写作任务
\end{itemize}

\textbf{奖励设计}：针对不同任务使用不同的奖励函数

\subsection{DeepSeek-R1-Zero实验}

R1-Zero是一个重要的消融实验，\textbf{完全不使用SFT}，直接从Base模型开始RL训练。

\subsubsection{实验设置}

\begin{itemize}
    \item 基础模型：DeepSeek-V3 Base
    \item 训练算法：GRPO
    \item 奖励：纯规则奖励（准确性 + 格式）
    \item 无任何人类标注数据
\end{itemize}

\subsubsection{涌现现象}

训练过程中观察到多种涌现行为：

\textbf{1. 自我验证}：模型学会检查自己的答案
\begin{quote}
\textit{"Let me verify this answer by substituting back..."}
\end{quote}

\textbf{2. 反思与纠错}：发现错误后重新思考
\begin{quote}
\textit{"Wait, I made an error in step 3. Let me recalculate..."}
\end{quote}

\textbf{3. 多路径探索}：尝试不同的解题方法
\begin{quote}
\textit{"There are two approaches to this problem. Let me try the algebraic method first..."}
\end{quote}

\textbf{4. Aha Moment}：顿悟式的突破
\begin{quote}
\textit{"Hmm, I see! The key insight is that..."}
\end{quote}

\subsubsection{局限性}

R1-Zero存在以下问题：
\begin{itemize}
    \item 可读性差：输出冗长、结构混乱
    \item 语言混杂：英文、中文、代码混合
    \item 格式不稳定：有时不遵循指定格式
\end{itemize}

这些问题通过后续的SFT阶段得到解决。

\subsection{性能表现}

\begin{table}[H]
\centering
\caption{DeepSeek-R1性能对比}
\begin{tabular}{lccc}
\toprule
\textbf{基准测试} & \textbf{DeepSeek-R1} & \textbf{OpenAI o1} & \textbf{Claude 3.5} \\
\midrule
AIME 2024 & 79.8\% & 83.3\% & 16.0\% \\
MATH-500 & 97.3\% & 96.4\% & 78.3\% \\
Codeforces Rating & 2029 & 1891 & - \\
GPQA Diamond & 71.5\% & 78.0\% & 65.0\% \\
\bottomrule
\end{tabular}
\end{table}

\subsection{蒸馏到小模型}

DeepSeek-R1的推理能力可以通过知识蒸馏迁移到小模型：

\subsubsection{蒸馏方法}

\begin{enumerate}
    \item 使用R1生成大量高质量推理数据
    \item 对小模型进行SFT
    \item 可选：对小模型进行额外的RL训练
\end{enumerate}

\subsubsection{蒸馏效果}

\begin{table}[H]
\centering
\caption{蒸馏模型性能}
\begin{tabular}{lccc}
\toprule
\textbf{模型} & \textbf{参数量} & \textbf{AIME 2024} & \textbf{MATH-500} \\
\midrule
DeepSeek-R1 & 671B & 79.8\% & 97.3\% \\
R1-Distill-Qwen-32B & 32B & 72.6\% & 94.3\% \\
R1-Distill-Qwen-14B & 14B & 69.7\% & 93.9\% \\
R1-Distill-Qwen-7B & 7B & 55.5\% & 92.8\% \\
R1-Distill-Qwen-1.5B & 1.5B & 28.9\% & 83.9\% \\
\midrule
OpenAI o1-mini & - & 63.6\% & 90.0\% \\
\bottomrule
\end{tabular}
\end{table}

%===========================================
\section{OpenAI o1技术分析}
%===========================================

\subsection{官方披露信息}

OpenAI o1于2024年9月发布，官方披露的信息有限：

\begin{enumerate}
    \item 使用大规模强化学习训练
    \item 产生长链"思考"过程（对用户隐藏）
    \item 性能随推理时间/token数量提升
    \item 在数学、代码、科学推理上达到专家水平
\end{enumerate}

\subsection{技术推测}

基于公开信息和研究社区分析，o1可能采用以下技术：

\subsubsection{过程奖励模型（PRM）}

根据OpenAI 2023年的论文"Let's Verify Step by Step"，PRM可能是o1的核心组件：

\begin{definition}[过程奖励模型]
PRM对推理过程中的每一步进行评分：
\begin{equation}
r_t = P(\text{step } t \text{ is correct} | s_1, s_2, \ldots, s_t)
\end{equation}
最终奖励为各步骤奖励的聚合。
\end{definition}

PRM相比结果奖励模型（ORM）的优势：
\begin{itemize}
    \item 提供密集监督信号
    \item 精确定位错误步骤
    \item 更好的credit assignment
\end{itemize}

PRM的训练需要大量步骤级人工标注，这是o1的核心技术壁垒之一。

\subsubsection{大规模RL训练}

推测o1使用了：
\begin{itemize}
    \item PPO或类似算法
    \item 大规模分布式训练
    \item 精心设计的奖励函数（PRM + ORM混合）
    \item 大量计算资源
\end{itemize}

\subsubsection{推理时搜索}

o1可能在推理时使用某种形式的搜索：
\begin{itemize}
    \item Best-of-N采样
    \item 树搜索（MCTS变体）
    \item 自适应计算量分配
\end{itemize}

\subsection{推理时Scaling}

o1展示了一种新的scaling维度——推理时计算：

\begin{equation}
\text{Performance} = f(\text{Model Size}, \text{Training Compute}, \textcolor{red}{\text{Inference Compute}})
\end{equation}

关键发现：
\begin{itemize}
    \item 更多"思考"时间 → 更高准确率
    \item 难题需要更长的推理链
    \item 存在收益递减，但上限较高
\end{itemize}

这开辟了提升LLM能力的新途径：不仅可以训练更大的模型，还可以让模型"思考更久"。

\subsection{与DeepSeek-R1的对比}

\begin{table}[H]
\centering
\caption{o1与R1技术路线对比}
\begin{tabular}{lcc}
\toprule
\textbf{方面} & \textbf{OpenAI o1} & \textbf{DeepSeek-R1} \\
\midrule
奖励模型 & PRM（推测） & 规则奖励 \\
人工标注 & 大量步骤级标注 & 极少量 \\
RL算法 & PPO（推测） & GRPO \\
思考过程 & 隐藏 & 公开 \\
是否开源 & 否 & 是 \\
训练成本 & 极高 & 相对较低 \\
\bottomrule
\end{tabular}
\end{table}

%===========================================
\section{其他主要研究方法}
%===========================================

\subsection{Constitutional AI（Anthropic）}

\subsubsection{核心思想}

Constitutional AI提出了RLAIF（RL from AI Feedback），用AI反馈替代人类反馈：

\begin{enumerate}
    \item \textbf{Self-Critique}：让模型评估自己的回答
    \item \textbf{Revision}：根据"宪法"原则修改回答
    \item \textbf{RL训练}：用AI评分作为奖励
\end{enumerate}

\subsubsection{宪法原则示例}

\begin{itemize}
    \item "选择最有帮助、最诚实、最无害的回答"
    \item "选择最不具有欺骗性的回答"
    \item "选择最尊重用户自主权的回答"
\end{itemize}

\subsubsection{优势与局限}

\textbf{优势}：
\begin{itemize}
    \item 大幅减少人类标注需求
    \item 原则可编程、可调整
    \item 易于扩展到新场景
\end{itemize}

\textbf{局限}：
\begin{itemize}
    \item AI评估可能存在偏差
    \item 难以处理需要专业知识的任务
    \item 对基础模型能力有要求
\end{itemize}

\subsection{Self-Play与迭代改进}

自博弈（Self-Play）方法让模型与自己对弈，不断迭代提升：

\subsubsection{基本流程}

\begin{enumerate}
    \item 生成候选回答
    \item 模型自评或交叉评估
    \item 选择最佳回答进行训练
    \item 重复迭代
\end{enumerate}

\subsubsection{代表工作}

\begin{itemize}
    \item \textbf{Self-Instruct}：自动生成指令数据
    \item \textbf{Self-Rewarding}：模型自己评分
    \item \textbf{SPIN}：自博弈迭代训练
\end{itemize}

\subsection{Google的推理研究}

\subsubsection{Chain-of-Thought Prompting}

Google在CoT提示方面做出了开创性工作：

\textbf{Few-shot CoT}：在prompt中提供推理示例
\begin{verbatim}
Q: Roger has 5 tennis balls...
A: Roger started with 5 balls. 
   2 cans of 3 balls each is 6 balls.
   5 + 6 = 11. The answer is 11.
\end{verbatim}

\textbf{Zero-shot CoT}：简单添加"Let's think step by step"
\begin{verbatim}
Q: Roger has 5 tennis balls...
A: Let's think step by step.
   [模型自动生成推理过程]
\end{verbatim}

\subsubsection{Self-Consistency}

自一致性方法通过多次采样提升准确率：

\begin{enumerate}
    \item 对同一问题采样$N$个推理路径
    \item 提取每个路径的最终答案
    \item 投票选择出现最多的答案
\end{enumerate}

\begin{equation}
\hat{a} = \arg\max_a \sum_{i=1}^{N} \mathbf{1}[a_i = a]
\end{equation}

\subsection{STaR: Self-Taught Reasoner}

\subsubsection{核心方法}

STaR通过迭代的自举学习提升推理能力：

\begin{algorithm}[H]
\caption{STaR算法}
\begin{algorithmic}[1]
\STATE 初始化模型$\pi_0$
\FOR{iteration $k = 0, 1, \ldots$}
    \FOR{每个问题 $q$ with 答案 $a^*$}
        \STATE 采样推理链：$c \sim \pi_k(\cdot|q)$
        \STATE 提取答案：$a = \text{extract}(c)$
        \IF{$a = a^*$}
            \STATE 加入训练集：$\mathcal{D}_k \leftarrow \mathcal{D}_k \cup \{(q, c)\}$
        \ELSE
            \STATE 生成rationalization：$c' \sim \pi_k(\cdot|q, a^*)$（给定答案提示）
            \STATE 加入训练集：$\mathcal{D}_k \leftarrow \mathcal{D}_k \cup \{(q, c')\}$
        \ENDIF
    \ENDFOR
    \STATE 在$\mathcal{D}_k$上微调：$\pi_{k+1} \leftarrow \text{SFT}(\pi_k, \mathcal{D}_k)$
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsubsection{Rationalization的作用}

当模型无法独立解出问题时，通过给定正确答案让模型"倒推"推理过程（rationalization），这种方法可以：
\begin{itemize}
    \item 利用更多训练样本
    \item 学习到新的推理模式
    \item 逐步提升解题能力
\end{itemize}

\subsection{Quiet-STaR: 静默思考}

\subsubsection{核心创新}

Quiet-STaR让模型在每个token位置进行隐式推理：

\begin{equation}
h_t = f(x_{\leq t}) + g(\text{thought}_t)
\end{equation}

其中$\text{thought}_t$是在位置$t$生成的内部"思考"。

\subsubsection{训练目标}

\begin{equation}
\mathcal{L} = -\sum_t \log P(x_{t+1} | x_{\leq t}, \text{thought}_t)
\end{equation}

使用REINFORCE优化思考生成策略。

\subsection{ReST：强化自我训练}

\subsubsection{核心方法}

ReST（Reinforced Self-Training）是Google DeepMind提出的自我训练方法，通过迭代生成-筛选-训练循环提升模型能力：

\begin{algorithm}[H]
\caption{ReST算法}
\begin{algorithmic}[1]
\STATE 初始化策略模型$\pi_\theta$
\FOR{iteration $t = 1, 2, \ldots, T$}
    \STATE \textbf{Generate}：对每个问题$x$，采样$K$个回答$\{y_1, \ldots, y_K\} \sim \pi_\theta(\cdot|x)$
    \STATE \textbf{Improve}：使用奖励模型$R$评分，筛选高质量样本
    \begin{equation}
    \mathcal{D}_{high} = \{(x, y) : R(x, y) \geq \tau\}
    \end{equation}
    \STATE \textbf{Train}：在筛选后数据上进行SFT
    \begin{equation}
    \theta \leftarrow \arg\min_\theta \mathbb{E}_{(x,y) \sim \mathcal{D}_{high}}[-\log \pi_\theta(y|x)]
    \end{equation}
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsubsection{与其他方法的关系}

ReST可以看作是以下方法的统一框架：
\begin{itemize}
    \item \textbf{Rejection Sampling}：单轮ReST
    \item \textbf{Expert Iteration}：使用搜索算法作为"专家"
    \item \textbf{STaR}：特定的rationalization机制
\end{itemize}

\subsubsection{理论分析}

\begin{theorem}[ReST的策略改进]
设$\pi^*$为最优策略，$\tau$为奖励阈值。如果
\begin{equation}
\mathbb{P}_{y \sim \pi_\theta}[R(x,y) \geq \tau] > 0
\end{equation}
则ReST迭代后的策略$\pi_{\theta'}$满足：
\begin{equation}
\mathbb{E}_{y \sim \pi_{\theta'}}[R(x,y)] \geq \mathbb{E}_{y \sim \pi_\theta}[R(x,y)]
\end{equation}
\end{theorem}

\subsection{ReST-EM：期望最大化自我训练}

\subsubsection{EM框架}

ReST-EM将自我训练形式化为期望最大化（EM）问题：

\textbf{E步（Expectation）}：
\begin{equation}
q(y|x) = \frac{\pi_\theta(y|x) \cdot \mathbf{1}[R(x,y) \geq \tau]}{Z(x)}
\end{equation}
其中$Z(x)$是归一化常数。

\textbf{M步（Maximization）}：
\begin{equation}
\theta^{new} = \arg\max_\theta \mathbb{E}_{x \sim \mathcal{D}, y \sim q}[\log \pi_\theta(y|x)]
\end{equation}

\subsubsection{与RLHF的联系}

ReST-EM可以看作是KL约束RL的特殊情况：
\begin{equation}
\max_\pi \mathbb{E}_{y \sim \pi}[R(x,y)] - \beta \mathbb{D}_{KL}[\pi \| \pi_{ref}]
\end{equation}
当$\beta \rightarrow 0$且使用硬阈值时，退化为ReST-EM。

\subsection{Expert Iteration (ExIt)}

\subsubsection{算法思想}

Expert Iteration来源于AlphaGo的成功经验，将搜索算法作为"专家"来指导策略学习：

\begin{enumerate}
    \item \textbf{Expert Policy}：使用搜索算法（如MCTS、Beam Search）生成高质量轨迹
    \item \textbf{Apprentice Policy}：神经网络学习模仿Expert
    \item \textbf{Iteration}：用改进的Apprentice引导下一轮Expert搜索
\end{enumerate}

\subsubsection{在LLM中的应用}

\begin{algorithm}[H]
\caption{Expert Iteration for LLM}
\begin{algorithmic}[1]
\STATE 初始化策略$\pi_\theta$
\FOR{iteration $= 1, 2, \ldots$}
    \FOR{每个问题 $x$}
        \STATE Expert：$y^* = \text{BeamSearch}(\pi_\theta, x, k)$ 或 $y^* = \text{MCTS}(\pi_\theta, x)$
        \STATE 验证：如果$R(x, y^*) \geq \tau$，加入$\mathcal{D}$
    \ENDFOR
    \STATE Apprentice：$\theta \leftarrow \text{SFT}(\theta, \mathcal{D})$
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsubsection{搜索策略}

\textbf{Best-of-N}：
\begin{equation}
y^* = \arg\max_{y \in \{y_1, \ldots, y_N\}} R(x, y), \quad y_i \sim \pi_\theta(\cdot|x)
\end{equation}

\textbf{Beam Search with Verifier}：
\begin{equation}
y^* = \arg\max_{y \in \text{Beam}_k} \sum_{t} \log \pi_\theta(y_t|y_{<t}, x) + \alpha \cdot V(x, y)
\end{equation}

\subsection{Self-Rewarding Language Models}

\subsubsection{核心创新}

Meta提出的Self-Rewarding方法让模型同时充当生成器和评判者：

\begin{enumerate}
    \item \textbf{生成}：模型生成候选回答
    \item \textbf{自评}：同一模型使用特殊prompt评估回答质量
    \item \textbf{训练}：基于自评结果进行RL或偏好学习
\end{enumerate}

\subsubsection{自评Prompt设计}

\begin{verbatim}
Review the user's question and the corresponding response using 
the additive 5-point scoring system described below:
- Add 1 point if the response is relevant and provides some 
  information related to the user's inquiry.
- Add 1 point if the response addresses a substantial portion 
  of the user's question.
- Add 1 point if the response answers the basic elements of 
  the user's question in a useful way.
- Add 1 point if the response is clearly written from an AI 
  assistant's perspective.
- Add 1 point if the response is impeccably tailored to the 
  user's question, demonstrating expert knowledge.

Question: {question}
Response: {response}

Score: 
\end{verbatim}

\subsubsection{迭代训练}

\begin{equation}
\pi^{(t+1)} = \text{DPO}(\pi^{(t)}, \mathcal{D}_{pref}^{(t)})
\end{equation}
其中偏好数据由$\pi^{(t)}$生成并自评：
\begin{equation}
\mathcal{D}_{pref}^{(t)} = \{(x, y_w, y_l) : R_{\pi^{(t)}}(x, y_w) > R_{\pi^{(t)}}(x, y_l)\}
\end{equation}

\subsection{REINFORCE及其变体}

\subsubsection{基础REINFORCE}

REINFORCE是最基本的策略梯度算法：
\begin{equation}
\nabla_\theta J(\theta) = \mathbb{E}_{y \sim \pi_\theta}[(R(x,y) - b) \nabla_\theta \log \pi_\theta(y|x)]
\end{equation}

\textbf{基线选择}：
\begin{itemize}
    \item 常数基线：$b = \bar{R}$（平均奖励）
    \item 状态相关基线：$b(s) = V(s)$
    \item 自身平均：$b = \frac{1}{K}\sum_{i=1}^K R(x, y_i)$
\end{itemize}

\subsubsection{RLOO (REINFORCE Leave-One-Out)}

RLOO使用Leave-One-Out估计作为基线，显著降低方差：

\begin{equation}
\nabla_\theta J_{RLOO} = \frac{1}{K}\sum_{i=1}^{K}\left(R(x, y_i) - \frac{1}{K-1}\sum_{j \neq i}R(x, y_j)\right)\nabla_\theta \log \pi_\theta(y_i|x)
\end{equation}

\textbf{优势}：
\begin{itemize}
    \item 无需额外的Value网络
    \item 基线与样本相关，降低方差
    \item 实现简单，计算高效
\end{itemize}

\subsubsection{与GRPO的关系}

GRPO可以看作是RLOO的一种形式，但使用标准化而非减法：
\begin{equation}
\hat{A}_i^{GRPO} = \frac{R_i - \bar{R}}{\sigma_R}, \quad \hat{A}_i^{RLOO} = R_i - \bar{R}_{-i}
\end{equation}

\subsection{RAFT：奖励排序微调}

\subsubsection{算法流程}

RAFT（Reward rAnked FineTuning）是一种简化的RL方法：

\begin{algorithm}[H]
\caption{RAFT算法}
\begin{algorithmic}[1]
\FOR{每个问题 $x$}
    \STATE 采样$K$个回答：$\{y_1, \ldots, y_K\} \sim \pi_\theta(\cdot|x)$
    \STATE 计算奖励：$r_i = R(x, y_i)$
    \STATE 排序并选择Top-$k$：$\mathcal{T} = \text{TopK}(\{(y_i, r_i)\}, k)$
    \STATE 加入训练集：$\mathcal{D} \leftarrow \mathcal{D} \cup \{(x, y) : y \in \mathcal{T}\}$
\ENDFOR
\STATE 在$\mathcal{D}$上进行SFT
\end{algorithmic}
\end{algorithm}

\subsubsection{与拒绝采样的区别}

\begin{itemize}
    \item \textbf{拒绝采样}：使用绝对阈值$\tau$筛选
    \item \textbf{RAFT}：使用相对排名筛选，保证每个问题都有训练样本
\end{itemize}

\subsection{Rejection Sampling Fine-tuning (RSF)}

\subsubsection{方法细节}

RSF是一种简单有效的方法，广泛用于数学和代码任务：

\begin{equation}
\mathcal{D}_{RSF} = \{(x, y) : y \sim \pi_\theta(\cdot|x), R(x, y) = 1\}
\end{equation}

\textbf{关键设计}：
\begin{itemize}
    \item 采样温度：通常使用较高温度（0.7-1.0）增加多样性
    \item 采样数量：每个问题采样10-100个
    \item 去重策略：移除重复的正确答案
\end{itemize}

\subsubsection{在数学推理中的应用}

DeepSeekMath、Qwen-Math等模型大量使用RSF：
\begin{enumerate}
    \item 收集大量数学问题
    \item 对每个问题采样多个解答
    \item 通过答案验证筛选正确解答
    \item 在正确解答上进行SFT
\end{enumerate}

\subsection{V-STaR：验证器增强的自我训练}

\subsubsection{方法改进}

V-STaR在STaR基础上引入显式验证器：

\begin{enumerate}
    \item 训练验证器$V(x, y) \rightarrow \{0, 1\}$判断解答正确性
    \item 使用验证器（而非答案匹配）筛选训练数据
    \item 迭代提升生成器和验证器
\end{enumerate}

\subsubsection{验证器训练}

\begin{equation}
\mathcal{L}_V = -\mathbb{E}_{(x,y,l)}[l \log V(x,y) + (1-l)\log(1-V(x,y))]
\end{equation}
其中$l \in \{0, 1\}$表示解答是否正确。

\subsubsection{联合训练}

\begin{algorithm}[H]
\caption{V-STaR联合训练}
\begin{algorithmic}[1]
\FOR{iteration $t$}
    \STATE 生成：$\{y_i\} \sim \pi_\theta(\cdot|x)$
    \STATE 标注：使用真实答案或$V$判断正确性
    \STATE 训练验证器：更新$V$
    \STATE 筛选：$\mathcal{D}^+ = \{(x, y) : V(x, y) > 0.5\}$
    \STATE 训练生成器：在$\mathcal{D}^+$上SFT
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{过程奖励模型（PRM）与结果奖励模型（ORM）}

\subsubsection{Let's Verify Step by Step}

OpenAI的工作系统研究了PRM在数学推理中的应用：

\textbf{PRM定义}：
\begin{equation}
R_{PRM}(x, y) = \prod_{t=1}^{T} P(\text{step } t \text{ is correct} | x, y_{\leq t})
\end{equation}

\textbf{数据标注}：
\begin{itemize}
    \item 人工标注每个推理步骤的正确性
    \item 标签：正确/错误/中立
    \item 大规模标注（800K步骤级标注）
\end{itemize}

\subsubsection{Math-Shepherd}

Math-Shepherd提出自动标注过程奖励的方法：

\begin{enumerate}
    \item 对每个中间步骤，从该步骤继续采样多个完成
    \item 如果存在正确完成，该步骤标为正确
    \item 如果所有完成都错误，该步骤标为错误
\end{enumerate}

\begin{equation}
l_t = \mathbf{1}\left[\exists \text{ completion from step } t \text{ that reaches correct answer}\right]
\end{equation}

\subsubsection{PRM vs ORM对比}

\begin{table}[H]
\centering
\caption{PRM与ORM对比}
\begin{tabular}{lcc}
\toprule
\textbf{特性} & \textbf{PRM} & \textbf{ORM} \\
\midrule
奖励粒度 & 步骤级 & 序列级 \\
标注成本 & 高 & 低 \\
信用分配 & 精确 & 粗糙 \\
Reward Hacking & 较难 & 较易 \\
泛化能力 & 需要验证 & 相对稳定 \\
推理时开销 & 高（每步评估） & 低（仅结尾评估） \\
\bottomrule
\end{tabular}
\end{table}

\subsection{PRIME：隐式过程奖励}

\subsubsection{核心思想}

PRIME（Process Reward Implicit）提出无需显式步骤标注的过程奖励学习：

\begin{equation}
R_{PRIME}(x, y_{\leq t}) = \mathbb{E}_{y_{>t} \sim \pi}[R_{ORM}(x, y)]
\end{equation}

\subsubsection{实现方法}

通过蒙特卡洛估计隐式过程奖励：
\begin{equation}
\hat{R}_{PRIME}(x, y_{\leq t}) = \frac{1}{M}\sum_{m=1}^{M} R_{ORM}(x, y_{\leq t} \oplus y_{>t}^{(m)})
\end{equation}
其中$y_{>t}^{(m)} \sim \pi(\cdot|x, y_{\leq t})$是从当前状态采样的完成。

\subsection{Online DPO与迭代偏好优化}

\subsubsection{Online DPO}

标准DPO使用离线偏好数据，Online DPO在训练过程中动态生成偏好对：

\begin{algorithm}[H]
\caption{Online DPO}
\begin{algorithmic}[1]
\FOR{iteration $t$}
    \STATE 采样：$y_1, y_2 \sim \pi_{\theta_t}(\cdot|x)$
    \STATE 评估：$r_1 = R(x, y_1), r_2 = R(x, y_2)$
    \STATE 构建偏好对：$(y_w, y_l) = (y_1, y_2)$ if $r_1 > r_2$ else $(y_2, y_1)$
    \STATE DPO更新：
    \begin{equation}
    \theta_{t+1} = \theta_t - \eta \nabla_\theta \mathcal{L}_{DPO}(x, y_w, y_l; \theta_t)
    \end{equation}
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsubsection{IPO (Identity Preference Optimization)}

IPO修改DPO损失函数，避免过拟合：
\begin{equation}
\mathcal{L}_{IPO} = \left(\log\frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \log\frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)} - \frac{1}{2\beta}\right)^2
\end{equation}

\subsection{WizardMath与WizardCoder系列}

\subsubsection{Evol-Instruct方法}

WizardLM系列使用进化指令方法自动生成训练数据：

\textbf{指令进化}：
\begin{itemize}
    \item \textbf{增加约束}：添加更多限制条件
    \item \textbf{深化}：要求更深入的推理
    \item \textbf{具体化}：添加具体细节
    \item \textbf{增加推理步骤}：要求更多中间步骤
    \item \textbf{复杂化输入}：使问题更复杂
\end{itemize}

\subsubsection{RLEIF (RL from Evol-Instruct Feedback)}

WizardMath结合进化指令和RL：
\begin{enumerate}
    \item 使用Evol-Instruct生成难题
    \item 采样多个解答
    \item 筛选正确解答进行训练
    \item 迭代上述过程
\end{enumerate}

\subsection{Qwen系列数学与代码模型}

\subsubsection{Qwen2.5-Math}

Qwen2.5-Math结合多种方法：
\begin{itemize}
    \item \textbf{大规模预训练}：数学相关语料
    \item \textbf{SFT}：高质量数学解题数据
    \item \textbf{Rejection Sampling}：迭代筛选正确解答
    \item \textbf{GRPO}：进一步RL优化
\end{itemize}

\subsubsection{训练流程}

\begin{enumerate}
    \item 预训练：在数学语料上继续预训练
    \item SFT：监督微调建立基础能力
    \item RS迭代：多轮拒绝采样提升
    \item RL微调：GRPO进一步优化
\end{enumerate}

\subsection{Marco-o1与MCTS增强推理}

\subsubsection{MCTS在LLM中的应用}

Marco-o1使用蒙特卡洛树搜索增强推理：

\begin{algorithm}[H]
\caption{MCTS for LLM Reasoning}
\begin{algorithmic}[1]
\STATE 初始化根节点$s_0 = x$（问题）
\FOR{iteration $= 1, \ldots, N$}
    \STATE \textbf{Selection}：使用UCB选择节点
    \begin{equation}
    a^* = \arg\max_a Q(s, a) + c\sqrt{\frac{\ln N(s)}{N(s,a)}}
    \end{equation}
    \STATE \textbf{Expansion}：使用LLM生成下一步
    \STATE \textbf{Simulation}：完成推理直到答案
    \STATE \textbf{Backpropagation}：更新节点统计
\ENDFOR
\STATE 返回访问次数最多的路径
\end{algorithmic}
\end{algorithm}

\subsubsection{与其他方法结合}

\begin{itemize}
    \item \textbf{MCTS + PRM}：使用PRM评估节点价值
    \item \textbf{MCTS + Self-Consistency}：多路径投票
    \item \textbf{MCTS + Expert Iteration}：搜索结果用于训练
\end{itemize}

\subsection{Kimi k1.5与长上下文RL}

\subsubsection{长上下文强化学习}

Kimi k1.5针对长上下文场景优化RL训练：
\begin{itemize}
    \item 支持超长推理链（128K+ tokens）
    \item 高效的长序列RL算法
    \item 分段奖励设计
\end{itemize}

\subsubsection{技术特点}

\begin{itemize}
    \item \textbf{分段GRPO}：将长序列分段计算
    \item \textbf{渐进式长度}：从短到长逐步训练
    \item \textbf{多模态支持}：文本+视觉推理
\end{itemize}

\subsection{方法对比总结}

\begin{table}[H]
\centering
\caption{主要无人类反馈RL方法对比}
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccc}
\toprule
\textbf{方法} & \textbf{奖励来源} & \textbf{是否需要Critic} & \textbf{数据效率} & \textbf{实现复杂度} & \textbf{适用场景} \\
\midrule
ReST & 规则/模型 & 否 & 中 & 低 & 通用 \\
STaR & 规则 & 否 & 高 & 低 & 推理任务 \\
Expert Iteration & 搜索 & 否 & 高 & 中 & 可验证任务 \\
Self-Rewarding & 自评 & 否 & 中 & 中 & 通用 \\
REINFORCE & 规则/模型 & 否 & 低 & 低 & 通用 \\
RLOO & 规则/模型 & 否 & 中 & 低 & 通用 \\
GRPO & 规则/模型 & 否 & 中 & 低 & 推理任务 \\
PPO & 规则/模型 & 是 & 中 & 高 & 通用 \\
DPO & 偏好数据 & 否 & 高 & 低 & 偏好对齐 \\
Online DPO & 规则/模型 & 否 & 中 & 中 & 迭代优化 \\
RSF & 规则 & 否 & 中 & 低 & 可验证任务 \\
RAFT & 规则/模型 & 否 & 中 & 低 & 通用 \\
\bottomrule
\end{tabular}
}
\end{table}

\subsection{其他重要方法与研究}

\subsubsection{SPIN: Self-Play Fine-Tuning}

SPIN通过自博弈迭代提升模型：

\textbf{核心思想}：将SFT数据中的真实回答作为"获胜者"，模型生成的回答作为"失败者"进行对比学习。

\begin{equation}
\mathcal{L}_{SPIN} = -\mathbb{E}_{(x,y^*) \sim \mathcal{D}}[\log\sigma(\beta(\log\frac{\pi_\theta(y^*|x)}{\pi_{ref}(y^*|x)} - \log\frac{\pi_\theta(\hat{y}|x)}{\pi_{ref}(\hat{y}|x)}))]
\end{equation}
其中$y^*$是真实回答，$\hat{y} \sim \pi_{\theta_{old}}$是上一轮模型生成的回答。

\subsubsection{Iterative DPO}

迭代DPO在多轮中不断改进：
\begin{enumerate}
    \item 使用当前模型生成新的回答对
    \item 使用奖励模型或规则判断偏好
    \item 进行DPO训练
    \item 重复迭代
\end{enumerate}

\subsubsection{KTO: Kahneman-Tversky Optimization}

KTO基于前景理论设计损失函数，不需要配对的偏好数据：

\begin{equation}
\mathcal{L}_{KTO} = \mathbb{E}_{(x,y)}[\lambda_y \cdot \sigma(\beta(r_{ref} - r_\theta(x,y)))]
\end{equation}
其中$\lambda_y$根据$y$是正例还是负例有不同取值。

\subsubsection{ORPO: Odds Ratio Preference Optimization}

ORPO统一了SFT和偏好优化：
\begin{equation}
\mathcal{L}_{ORPO} = \mathcal{L}_{SFT}(y_w) + \lambda \cdot \log\sigma(\log\frac{\text{odds}_\theta(y_w|x)}{\text{odds}_\theta(y_l|x)})
\end{equation}
其中$\text{odds}_\theta(y|x) = \frac{P_\theta(y|x)}{1-P_\theta(y|x)}$。

\subsubsection{SimPO: Simple Preference Optimization}

SimPO简化DPO，移除参考模型：
\begin{equation}
\mathcal{L}_{SimPO} = -\log\sigma(\frac{\beta}{|y_w|}\log\pi_\theta(y_w|x) - \frac{\beta}{|y_l|}\log\pi_\theta(y_l|x) - \gamma)
\end{equation}
使用长度归一化的对数概率作为隐式奖励。

\subsubsection{Code Generation: AlphaCode与竞赛编程}

\textbf{AlphaCode方法}：
\begin{enumerate}
    \item 大规模采样（百万级）
    \item 聚类去重
    \item 测试用例筛选
    \item 提交最佳候选
\end{enumerate}

\textbf{CodeRL}：
\begin{equation}
R_{code}(x, y) = \alpha \cdot \text{compile\_success} + \beta \cdot \text{test\_pass\_rate} + \gamma \cdot \text{efficiency}
\end{equation}

\subsubsection{Tool Use与Agent RL}

\textbf{ReAct框架}：交替进行推理（Reasoning）和行动（Acting）

\textbf{RL for Tool Learning}：
\begin{itemize}
    \item 状态：当前对话历史
    \item 动作：调用工具或生成回答
    \item 奖励：任务完成度 + 工具使用效率
\end{itemize}

\subsubsection{OpenR: 开源推理框架}

OpenR项目提供了开源的推理模型训练框架：
\begin{itemize}
    \item 支持多种RL算法（PPO、GRPO、DPO等）
    \item 集成过程奖励模型
    \item 支持MCTS搜索
    \item 提供完整的训练流程
\end{itemize}

\subsubsection{rStar: 自博弈推理}

rStar（Self-play mutual reasoning）通过自博弈提升推理：
\begin{enumerate}
    \item 生成多个推理路径
    \item 使用判别器评估
    \item 互相验证和改进
    \item 迭代提升
\end{enumerate}

\subsubsection{Inference-Time Compute Scaling}

推理时计算扩展的方法：
\begin{table}[H]
\centering
\caption{推理时计算扩展方法}
\begin{tabular}{lp{8cm}}
\toprule
\textbf{方法} & \textbf{描述} \\
\midrule
Best-of-N & 采样N个回答，选择最佳 \\
Self-Consistency & 多次采样后投票 \\
Beam Search & 保持K个最优候选 \\
MCTS & 树搜索探索解空间 \\
Iterative Refinement & 生成-评估-修改循环 \\
Verifier-Guided & 使用验证器引导生成 \\
\bottomrule
\end{tabular}
\end{table}

%===========================================
\section{创新算法框架}
%===========================================

本节提出多种创新性的LLM强化学习算法框架，旨在解决现有方法的局限性并探索新的技术方向。

\subsection{自适应分层奖励优化（AHRO）}

\subsubsection{动机与背景}

现有的奖励设计通常是静态的，无法适应不同难度和类型的问题。AHRO（Adaptive Hierarchical Reward Optimization）提出动态分层的奖励结构，根据问题特性自适应调整奖励分配。

\subsubsection{算法框架}

\begin{definition}[AHRO奖励结构]
定义多层次奖励函数：
\begin{equation}
R_{AHRO}(q, o) = \sum_{l=1}^{L} \omega_l(q) \cdot R_l(q, o)
\end{equation}
其中：
\begin{itemize}
    \item $L$：奖励层数
    \item $R_l$：第$l$层奖励函数
    \item $\omega_l(q)$：问题$q$对应的第$l$层权重
\end{itemize}
\end{definition}

\textbf{奖励层次设计}：
\begin{enumerate}
    \item \textbf{语法层}$R_1$：输出格式、语法正确性
    \item \textbf{语义层}$R_2$：逻辑连贯性、语义一致性
    \item \textbf{推理层}$R_3$：推理步骤正确性、方法选择
    \item \textbf{结果层}$R_4$：最终答案准确性
    \item \textbf{效率层}$R_5$：推理效率、简洁性
\end{enumerate}

\textbf{自适应权重学习}：
\begin{equation}
\omega_l(q) = \text{softmax}\left(\mathbf{W}_l \cdot \text{embed}(q) + \mathbf{b}_l\right)
\end{equation}

权重网络与策略网络联合训练：
\begin{equation}
\mathcal{L}_{total} = \mathcal{L}_{RL} + \lambda \mathcal{L}_{weight}
\end{equation}

其中$\mathcal{L}_{weight}$鼓励权重分配的多样性和稳定性。

\begin{algorithm}[H]
\caption{AHRO算法}
\begin{algorithmic}[1]
\STATE 初始化策略网络$\pi_\theta$和权重网络$W_\phi$
\FOR{iteration $= 1, 2, \ldots$}
    \FOR{每个问题 $q$ in batch}
        \STATE 计算自适应权重：$\omega(q) = W_\phi(q)$
        \STATE 采样回答：$o \sim \pi_\theta(\cdot|q)$
        \STATE 计算各层奖励：$R_1, R_2, \ldots, R_L$
        \STATE 计算加权奖励：$R = \sum_l \omega_l \cdot R_l$
    \ENDFOR
    \STATE 使用GRPO更新策略
    \STATE 更新权重网络以最大化奖励预测准确性
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsubsection{理论分析}

\begin{theorem}[AHRO收敛性]
在以下条件下，AHRO保证收敛到局部最优：
\begin{enumerate}
    \item 各层奖励函数有界且Lipschitz连续
    \item 权重网络容量充足
    \item 学习率满足Robbins-Monro条件
\end{enumerate}
\end{theorem}

\subsection{认知架构引导策略学习（CAPL）}

\subsubsection{核心思想}

CAPL（Cognitive Architecture-guided Policy Learning）借鉴认知科学中的双过程理论，设计显式的认知架构来引导策略学习。

\begin{definition}[双过程认知架构]
\begin{itemize}
    \item \textbf{System 1}：快速、直觉、自动的处理
    \item \textbf{System 2}：慢速、分析、有意识的推理
\end{itemize}
\end{definition}

\subsubsection{架构设计}

\textbf{双系统策略}：
\begin{equation}
\pi_{CAPL}(a|s) = \alpha(s) \cdot \pi_{S1}(a|s) + (1-\alpha(s)) \cdot \pi_{S2}(a|s)
\end{equation}

其中$\alpha(s) \in [0,1]$是切换函数，根据状态决定使用哪个系统。

\textbf{System 1实现}：
\begin{itemize}
    \item 标准的自回归生成
    \item 快速但可能出错
    \item 适用于简单、熟悉的问题
\end{itemize}

\textbf{System 2实现}：
\begin{itemize}
    \item 显式的推理规划模块
    \item 步骤分解和验证
    \item 适用于复杂、新颖的问题
\end{itemize}

\textbf{切换函数设计}：
\begin{equation}
\alpha(s) = \sigma\left(\mathbf{w}^T \cdot \text{features}(s) + b\right)
\end{equation}

特征包括：
\begin{itemize}
    \item 问题复杂度估计
    \item 当前置信度
    \item 历史错误率
    \item 领域匹配度
\end{itemize}

\subsubsection{训练方法}

采用分阶段训练：
\begin{enumerate}
    \item \textbf{Phase 1}：分别训练System 1和System 2
    \item \textbf{Phase 2}：训练切换函数$\alpha$
    \item \textbf{Phase 3}：端到端联合微调
\end{enumerate}

\textbf{切换函数训练目标}：
\begin{equation}
\mathcal{L}_{\alpha} = -\mathbb{E}\left[R \cdot \log \alpha + (1-R) \cdot \log(1-\alpha)\right]
\end{equation}

直觉：当System 1成功时增加$\alpha$，失败时减少。

\subsection{动态思维链自演化（DCoT-SE）}

\subsubsection{动机}

现有的CoT方法使用固定格式的推理链，DCoT-SE（Dynamic Chain-of-Thought Self-Evolution）让模型自主演化推理格式和策略。

\subsubsection{推理模式库}

定义可演化的推理模式集合$\mathcal{M}$：

\begin{table}[H]
\centering
\caption{推理模式示例}
\begin{tabular}{lp{8cm}}
\toprule
\textbf{模式} & \textbf{描述} \\
\midrule
Sequential & 线性步骤推理 \\
Tree & 树状分支探索 \\
Graph & 图结构关联推理 \\
Recursive & 递归分解 \\
Analogical & 类比推理 \\
Counterfactual & 反事实推理 \\
Abductive & 溯因推理 \\
Meta-reasoning & 元推理（推理关于推理的推理） \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{模式选择策略}

\begin{equation}
P(m|q) = \frac{\exp(\text{score}(m, q)/\tau)}{\sum_{m' \in \mathcal{M}} \exp(\text{score}(m', q)/\tau)}
\end{equation}

评分函数基于：
\begin{itemize}
    \item 问题-模式历史匹配成功率
    \item 问题特征与模式特征的相似度
    \item 模式当前的整体效果
\end{itemize}

\subsubsection{自演化机制}

\begin{algorithm}[H]
\caption{DCoT-SE自演化算法}
\begin{algorithmic}[1]
\STATE 初始化模式库$\mathcal{M}$和模式统计$S$
\FOR{generation $g = 1, 2, \ldots$}
    \FOR{每个问题 $q$}
        \STATE 选择模式：$m \sim P(\cdot|q)$
        \STATE 使用模式$m$生成推理：$o \sim \pi(\cdot|q, m)$
        \STATE 评估结果，更新统计$S[m]$
    \ENDFOR
    \IF{generation $g \mod K = 0$}
        \STATE \textbf{模式进化}：
        \STATE 淘汰低效模式
        \STATE 通过组合/变异生成新模式
        \STATE 从成功案例中提取新模式
    \ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

\textbf{新模式生成方法}：
\begin{enumerate}
    \item \textbf{组合}：$m_{new} = \text{combine}(m_1, m_2)$
    \item \textbf{变异}：$m_{new} = \text{mutate}(m, \delta)$
    \item \textbf{提取}：从高奖励轨迹中自动提取推理模式
\end{enumerate}

\subsection{多粒度推理一致性强化（MGRCR）}

\subsubsection{核心思想}

MGRCR（Multi-Granularity Reasoning Consistency Reinforcement）在多个粒度上强化推理的一致性，提高推理的可靠性和鲁棒性。

\subsubsection{粒度定义}

\begin{enumerate}
    \item \textbf{Token粒度}：单个token级别的决策
    \item \textbf{Span粒度}：短语/表达式级别
    \item \textbf{Step粒度}：完整推理步骤
    \item \textbf{Chain粒度}：整个推理链
    \item \textbf{Solution粒度}：完整解答
\end{enumerate}

\subsubsection{一致性奖励设计}

\textbf{内部一致性}：同一粒度内的逻辑一致
\begin{equation}
R_{internal}^{(g)} = \text{consistency}(\{u_1^{(g)}, u_2^{(g)}, \ldots\})
\end{equation}

\textbf{跨粒度一致性}：不同粒度间的语义一致
\begin{equation}
R_{cross}^{(g_1, g_2)} = \text{alignment}(U^{(g_1)}, U^{(g_2)})
\end{equation}

\textbf{总一致性奖励}：
\begin{equation}
R_{consistency} = \sum_g \lambda_g R_{internal}^{(g)} + \sum_{g_1 < g_2} \mu_{g_1,g_2} R_{cross}^{(g_1, g_2)}
\end{equation}

\subsubsection{一致性检测方法}

\textbf{数学一致性}：
\begin{itemize}
    \item 方程等价性检验（使用符号计算）
    \item 数值一致性检验
    \item 单位一致性检验
\end{itemize}

\textbf{逻辑一致性}：
\begin{itemize}
    \item 命题逻辑一致性
    \item 时序一致性
    \item 因果一致性
\end{itemize}

\textbf{语义一致性}：
\begin{itemize}
    \item 嵌入空间距离
    \item NLI模型判断
    \item 问答一致性
\end{itemize}

\subsubsection{训练框架}

\begin{equation}
\mathcal{L}_{MGRCR} = \mathcal{L}_{base} + \alpha \mathcal{L}_{consistency} + \beta \mathcal{L}_{regularization}
\end{equation}

其中$\mathcal{L}_{regularization}$防止模型为追求一致性而牺牲多样性。

\subsection{元认知反馈回路优化（MFLO）}

\subsubsection{元认知框架}

MFLO（Meta-cognitive Feedback Loop Optimization）引入元认知层，让模型能够监控、评估和调节自己的认知过程。

\begin{definition}[元认知层次]
\begin{enumerate}
    \item \textbf{对象层}：执行具体推理任务
    \item \textbf{元层}：监控和评估对象层行为
    \item \textbf{元元层}：调节元层的策略（可选）
\end{enumerate}
\end{definition}

\subsubsection{元认知组件}

\textbf{1. 监控组件（Monitoring）}：
\begin{equation}
\text{confidence}(s, a) = f_{monitor}(s, a, \theta_{monitor})
\end{equation}
预测当前决策的置信度。

\textbf{2. 评估组件（Evaluation）}：
\begin{equation}
\text{quality}(s, a) = f_{eval}(s, a, \theta_{eval})
\end{equation}
评估当前推理状态的质量。

\textbf{3. 控制组件（Control）}：
\begin{equation}
\text{action}_{meta}(s) = f_{control}(s, \text{confidence}, \text{quality})
\end{equation}
决定元认知干预动作：
\begin{itemize}
    \item Continue：继续当前推理
    \item Pause：停顿进行验证
    \item Backtrack：回溯到之前的状态
    \item Switch：切换推理策略
    \item Terminate：提前终止
\end{itemize}

\subsubsection{反馈回路设计}

\begin{algorithm}[H]
\caption{MFLO推理过程}
\begin{algorithmic}[1]
\STATE 输入问题$q$，初始化状态$s_0$
\FOR{$t = 0, 1, \ldots$}
    \STATE 对象层提出动作：$a_t \sim \pi_{object}(\cdot|s_t)$
    \STATE 元层评估：$c_t = \text{confidence}(s_t, a_t)$，$q_t = \text{quality}(s_t, a_t)$
    \STATE 元层决策：$m_t = \text{action}_{meta}(s_t, c_t, q_t)$
    \IF{$m_t = $ Continue}
        \STATE 执行$a_t$，更新$s_{t+1}$
    \ELSIF{$m_t = $ Pause}
        \STATE 执行验证子程序
        \STATE 根据验证结果调整
    \ELSIF{$m_t = $ Backtrack}
        \STATE $s_t \leftarrow s_{t-k}$（回溯$k$步）
        \STATE 重新采样动作
    \ELSIF{$m_t = $ Switch}
        \STATE 切换推理策略/模式
    \ELSIF{$m_t = $ Terminate}
        \STATE 输出当前最佳答案并退出
    \ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsubsection{训练方法}

分层训练元认知组件：

\textbf{Phase 1：监控组件}
\begin{equation}
\mathcal{L}_{monitor} = -\mathbb{E}\left[y \log \hat{c} + (1-y) \log(1-\hat{c})\right]
\end{equation}
其中$y$是事后判断的正确性标签。

\textbf{Phase 2：评估组件}
\begin{equation}
\mathcal{L}_{eval} = \mathbb{E}\left[(R - \hat{q})^2\right]
\end{equation}
预测最终奖励。

\textbf{Phase 3：控制组件}
使用强化学习训练，奖励为最终任务表现。

\textbf{Phase 4：联合微调}
端到端优化整个系统。

\subsection{对抗推理鲁棒性增强（ARRE）}

\subsubsection{动机}

现有推理模型对输入扰动敏感，ARRE（Adversarial Reasoning Robustness Enhancement）通过对抗训练提高推理鲁棒性。

\subsubsection{对抗样本生成}

\textbf{语义保持扰动}：
\begin{equation}
q' = \arg\max_{q': \text{sem}(q') = \text{sem}(q)} \mathcal{L}(\pi_\theta, q', a^*)
\end{equation}

扰动类型：
\begin{itemize}
    \item \textbf{同义替换}：替换为同义词
    \item \textbf{语序变换}：改变句子结构
    \item \textbf{冗余添加}：添加无关信息
    \item \textbf{关键信息隐藏}：使关键信息更难提取
    \item \textbf{误导性添加}：添加误导性但无害的信息
\end{itemize}

\textbf{推理路径扰动}：
\begin{itemize}
    \item 步骤顺序打乱
    \item 中间结果修改
    \item 推理策略变换
\end{itemize}

\subsubsection{对抗训练目标}

\begin{equation}
\mathcal{L}_{ARRE} = \mathbb{E}_q\left[\max_{q' \in \mathcal{B}(q)} \mathcal{L}(\pi_\theta, q', a^*)\right] + \lambda \mathcal{L}_{consistency}(q, q')
\end{equation}

其中$\mathcal{B}(q)$是$q$的语义保持邻域，$\mathcal{L}_{consistency}$鼓励对$q$和$q'$的推理过程一致。

\subsubsection{渐进式对抗训练}

\begin{algorithm}[H]
\caption{ARRE渐进训练}
\begin{algorithmic}[1]
\STATE 初始化扰动强度$\epsilon_0$
\FOR{stage $s = 1, 2, \ldots, S$}
    \STATE 设置当前扰动强度$\epsilon_s = \epsilon_0 \cdot (1 + \alpha s)$
    \FOR{iteration in stage $s$}
        \STATE 生成对抗样本$q' \in \mathcal{B}_{\epsilon_s}(q)$
        \STATE 计算对抗损失$\mathcal{L}_{adv}$
        \STATE 计算一致性损失$\mathcal{L}_{cons}$
        \STATE 更新模型
    \ENDFOR
    \STATE 评估鲁棒性，决定是否进入下一阶段
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsection{课程强化推理学习（CRRL）}

\subsubsection{核心思想}

CRRL（Curriculum Reinforcement Reasoning Learning）设计多维度课程，引导模型从简单到复杂逐步学习推理能力。

\subsubsection{多维度课程设计}

\textbf{难度维度}：
\begin{equation}
\text{difficulty}(q) = f(\text{steps}, \text{concepts}, \text{complexity}, \text{novelty})
\end{equation}

\textbf{领域维度}：
\begin{itemize}
    \item 算术 → 代数 → 几何 → 组合 → 数论
    \item 基础编程 → 算法 → 系统设计
\end{itemize}

\textbf{技能维度}：
\begin{itemize}
    \item 单步推理 → 多步推理 → 分支推理 → 递归推理
\end{itemize}

\subsubsection{自适应课程调度}

\begin{equation}
P(q | \theta) \propto \exp\left(-\frac{(\text{difficulty}(q) - \text{capability}(\theta))^2}{2\sigma^2}\right)
\end{equation}

选择难度与当前能力匹配的问题：
\begin{itemize}
    \item 过简单：学习信号弱
    \item 过困难：无法获得正向奖励
    \item 匹配：在最近发展区内学习
\end{itemize}

\textbf{能力估计}：
\begin{equation}
\text{capability}(\theta) = \mathbb{E}_{q \sim \mathcal{D}_{eval}}[\mathbf{1}[\pi_\theta \text{ solves } q]]
\end{equation}

使用滑动窗口估计当前能力水平。

\subsubsection{里程碑机制}

设置能力里程碑，达到后解锁新内容：
\begin{itemize}
    \item \textbf{Milestone 1}：单步算术（准确率 > 90\%）
    \item \textbf{Milestone 2}：多步算术（准确率 > 85\%）
    \item \textbf{Milestone 3}：基础代数（准确率 > 80\%）
    \item ...
\end{itemize}

\subsection{推理能力迁移学习框架（RCTL）}

\subsubsection{动机}

不同推理任务间存在共享的底层能力，RCTL（Reasoning Capability Transfer Learning）旨在高效地在任务间迁移推理能力。

\subsubsection{推理能力分解}

将推理能力分解为原子能力：
\begin{itemize}
    \item \textbf{抽象能力}：从具体到抽象
    \item \textbf{分解能力}：问题分解
    \item \textbf{组合能力}：信息整合
    \item \textbf{类比能力}：模式识别与迁移
    \item \textbf{验证能力}：结果检验
    \item \textbf{规划能力}：多步规划
\end{itemize}

\subsubsection{能力表示学习}

\begin{equation}
\mathbf{c}_i = \text{CapabilityEncoder}(\{(q_j, o_j)\}_{j \in \text{examples of capability } i})
\end{equation}

学习每种原子能力的向量表示。

\subsubsection{迁移机制}

\textbf{能力组合}：新任务 = 原子能力的组合
\begin{equation}
\mathbf{c}_{task} = \sum_i \alpha_i \mathbf{c}_i
\end{equation}

\textbf{快速适应}：仅调整组合权重$\{\alpha_i\}$，而非整个模型

\textbf{元学习}：学习如何快速学习新组合
\begin{equation}
\theta^* = \arg\min_\theta \mathbb{E}_{task}\left[\mathcal{L}(\theta', \mathcal{D}_{test}^{task})\right]
\end{equation}
其中$\theta' = \theta - \alpha \nabla_\theta \mathcal{L}(\theta, \mathcal{D}_{train}^{task})$

\subsection{因果推理增强学习（CREL）}

\subsubsection{核心思想}

CREL（Causal Reasoning Enhanced Learning）将因果推理显式地融入LLM训练，提高模型对因果关系的理解和利用能力。

\subsubsection{因果图建模}

对于每个问题，构建因果图$G = (V, E)$：
\begin{itemize}
    \item $V$：变量节点（已知量、未知量、中间变量）
    \item $E$：因果边（因果关系）
\end{itemize}

\subsubsection{因果奖励设计}

\textbf{因果正确性奖励}：
\begin{equation}
R_{causal} = \text{accuracy}(\text{CausalGraph}_{pred}, \text{CausalGraph}_{true})
\end{equation}

\textbf{干预一致性奖励}：
\begin{equation}
R_{intervention} = \mathbb{E}_{do(X=x)}[\mathbf{1}[\hat{Y} = Y]]
\end{equation}
在进行假设干预后，预测结果应保持一致。

\subsubsection{反事实推理训练}

生成反事实问题进行数据增强：
\begin{verbatim}
原问题：如果小明有5个苹果，买了3个，他有几个？
反事实：如果小明有5个苹果，买了5个，他有几个？
\end{verbatim}

模型应能正确处理反事实变体。

\subsection{分布式自博弈推理优化（DSPRO）}

\subsubsection{大规模自博弈框架}

DSPRO（Distributed Self-Play Reasoning Optimization）设计了可扩展的分布式自博弈系统：

\begin{itemize}
    \item \textbf{Generator Pool}：多个生成器并行采样
    \item \textbf{Evaluator Pool}：多个评估器并行打分
    \item \textbf{Aggregator}：聚合评估结果
    \item \textbf{Trainer}：异步更新模型
\end{itemize}

\subsubsection{多样性促进机制}

\textbf{Population-based Training}：
维护多个策略变体，各自演化后选择最优：
\begin{equation}
\pi_{next} = \text{select}(\{\pi_1', \pi_2', \ldots, \pi_K'\})
\end{equation}

\textbf{Novelty Reward}：
\begin{equation}
R_{novelty}(o) = \min_{o' \in \text{Archive}} d(o, o')
\end{equation}
奖励产生新颖解法的回答。

\subsubsection{异步训练协议}

\begin{algorithm}[H]
\caption{DSPRO异步训练}
\begin{algorithmic}[1]
\STATE 初始化参数服务器、Generator Pool、Evaluator Pool
\WHILE{not converged}
    \STATE \textbf{Generators}（并行）：
    \STATE \quad 拉取最新参数
    \STATE \quad 采样问题，生成回答
    \STATE \quad 推送到评估队列
    \STATE
    \STATE \textbf{Evaluators}（并行）：
    \STATE \quad 从队列获取(问题, 回答)对
    \STATE \quad 计算奖励
    \STATE \quad 推送到训练队列
    \STATE
    \STATE \textbf{Trainer}：
    \STATE \quad 从训练队列聚合数据
    \STATE \quad 计算梯度并更新
    \STATE \quad 推送到参数服务器
\ENDWHILE
\end{algorithmic}
\end{algorithm}

%===========================================
\section{实现细节与训练技巧}
%===========================================

\subsection{数据准备}

\subsubsection{数学数据来源}

\begin{itemize}
    \item \textbf{GSM8K}：8500道小学数学题，包含详细解答步骤
    \item \textbf{MATH}：12500道竞赛数学题，涵盖7个难度级别
    \item \textbf{AIME/AMC}：美国数学竞赛历年真题，高难度
    \item \textbf{Olympiad}：国际数学奥林匹克题目
    \item \textbf{合成数据}：程序生成的数学问题
    \item \textbf{网络数据}：爬取的数学问答，如Math StackExchange
    \item \textbf{教科书}：数学教材中的例题和习题
\end{itemize}

\subsubsection{代码数据来源}

\begin{itemize}
    \item \textbf{HumanEval}：164道编程题，测试函数实现
    \item \textbf{MBPP}：974道Python编程题
    \item \textbf{LeetCode}：算法竞赛题，难度分级
    \item \textbf{Codeforces}：编程竞赛题，含测试用例
    \item \textbf{GitHub}：开源代码库，提取函数和类
    \item \textbf{CodeContests}：Google发布的竞赛题集
    \item \textbf{APPS}：大规模编程问题集
\end{itemize}

\subsubsection{数据质量控制}

\textbf{过滤策略}：
\begin{enumerate}
    \item 去重：基于问题文本和答案的去重
    \item 长度过滤：移除过短或过长的样本
    \item 质量过滤：使用质量分类器筛选
    \item 毒性过滤：移除有害内容
    \item 平衡采样：确保难度和类型的平衡
\end{enumerate}

\textbf{数据验证}：
\begin{itemize}
    \item 数学：符号验证、数值验证
    \item 代码：执行测试、静态分析
    \item 人工抽检：定期人工审核
\end{itemize}

\subsubsection{数据增强}

\textbf{问题变换}：
\begin{itemize}
    \item \textbf{数值替换}：更换问题中的具体数字
    \item \textbf{情境替换}：更换问题背景（苹果→橙子）
    \item \textbf{复杂化}：添加额外条件或步骤
    \item \textbf{简化}：移除干扰信息
    \item \textbf{逆向构造}：给定答案生成问题
\end{itemize}

\textbf{推理链增强}：
\begin{itemize}
    \item 多解法生成
    \item 错误推理链生成（作为负样本）
    \item 推理链改写（不同表述方式）
\end{itemize}

\subsection{采样策略}

\subsubsection{温度采样}

\begin{equation}
P(token) = \frac{\exp(\text{logit}/T)}{\sum_{v} \exp(\text{logit}_v/T)}
\end{equation}

\textbf{温度选择指南}：
\begin{table}[H]
\centering
\begin{tabular}{lll}
\toprule
\textbf{场景} & \textbf{温度} & \textbf{说明} \\
\midrule
RL探索 & 0.8-1.2 & 增加多样性 \\
推理生成 & 0.6-0.8 & 平衡多样性和质量 \\
最终推理 & 0.1-0.3 & 更确定性的输出 \\
贪婪解码 & 0.0 & 确定性最高 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Top-p采样（Nucleus Sampling）}

只考虑累积概率达到$p$的tokens：
\begin{equation}
V_p = \min\left\{V' \subseteq V : \sum_{v \in V'} P(v) \geq p\right\}
\end{equation}

\textbf{动态Top-p}：
\begin{equation}
p_t = p_{base} + \alpha \cdot \text{entropy}(P_t)
\end{equation}
高熵时增加$p$以保持多样性。

\subsubsection{Top-k采样}

只考虑概率最高的$k$个tokens：
\begin{equation}
V_k = \text{argtop}_k P(v)
\end{equation}

\subsubsection{Best-of-N（Rejection Sampling）}

生成$N$个候选，用奖励模型选择最佳：
\begin{equation}
o^* = \arg\max_{o \in \{o_1, \ldots, o_N\}} R(q, o)
\end{equation}

\textbf{理论分析}：
\begin{equation}
\mathbb{E}[R(o^*)] = \mathbb{E}\left[\max_{i=1}^N R(o_i)\right]
\end{equation}

对于奖励服从某分布的情况，可以用极值理论分析改进幅度。

\textbf{实现优化}：
\begin{itemize}
    \item 批量并行生成
    \item 早停：低质量候选提前终止
    \item 分层采样：先粗筛再精选
\end{itemize}

\subsubsection{束搜索变体}

\textbf{标准束搜索}：
\begin{equation}
\mathcal{B}_{t+1} = \text{top}_k\left(\bigcup_{b \in \mathcal{B}_t} \{b \oplus v : v \in V\}\right)
\end{equation}

\textbf{多样束搜索}：
\begin{equation}
\text{score}(b) = \log P(b) - \lambda \cdot \text{similarity}(b, \mathcal{B}_{selected})
\end{equation}

\textbf{长度归一化束搜索}：
\begin{equation}
\text{score}(b) = \frac{\log P(b)}{|b|^\alpha}
\end{equation}

\subsection{训练稳定性}

\subsubsection{梯度管理}

\textbf{梯度裁剪}：
\begin{equation}
g \leftarrow \min\left(1, \frac{c}{\|g\|}\right) \cdot g
\end{equation}

\textbf{梯度累积}：
当内存不足时，累积多个小batch的梯度：
\begin{equation}
g_{accumulated} = \frac{1}{K}\sum_{k=1}^{K} g_k
\end{equation}

\textbf{梯度缩放（Mixed Precision）}：
\begin{equation}
g_{scaled} = g \cdot \text{scale\_factor}
\end{equation}
使用FP16训练时需要缩放以防止下溢。

\subsubsection{学习率调度}

\textbf{Warmup}：
\begin{equation}
\text{lr}(t) = \text{lr}_{max} \cdot \min\left(1, \frac{t}{t_{warmup}}\right)
\end{equation}

\textbf{Cosine Decay}：
\begin{equation}
\text{lr}(t) = \text{lr}_{min} + \frac{1}{2}(\text{lr}_{max} - \text{lr}_{min})\left(1 + \cos\left(\frac{t - t_{warmup}}{t_{total} - t_{warmup}}\pi\right)\right)
\end{equation}

\textbf{RL特定调度}：
\begin{itemize}
    \item RL学习率通常比SFT小10-100倍
    \item 可以使用自适应学习率（基于KL散度或奖励变化）
\end{itemize}

\textbf{自适应学习率}：
\begin{equation}
\text{lr}_{t+1} = \begin{cases}
\text{lr}_t \cdot \alpha_{up} & \text{if } \Delta R > \tau_{up} \\
\text{lr}_t \cdot \alpha_{down} & \text{if } \Delta R < \tau_{down} \\
\text{lr}_t & \text{otherwise}
\end{cases}
\end{equation}

\subsubsection{奖励归一化}

\textbf{批内归一化}：
\begin{equation}
r_{norm} = \frac{r - \mu_{batch}}{\sigma_{batch} + \epsilon}
\end{equation}

\textbf{移动平均归一化}：
\begin{equation}
\mu_t = \beta \mu_{t-1} + (1-\beta) r_t
\end{equation}
\begin{equation}
\sigma_t^2 = \beta \sigma_{t-1}^2 + (1-\beta) (r_t - \mu_t)^2
\end{equation}

\textbf{分位数归一化}：
\begin{equation}
r_{norm} = \frac{\text{rank}(r)}{N} - 0.5
\end{equation}
将奖励映射到$[-0.5, 0.5]$。

\subsubsection{KL散度控制}

\textbf{硬约束}：
\begin{equation}
\mathcal{L} = \mathcal{L}_{RL} \quad \text{s.t.} \quad \mathbb{D}_{KL}[\pi_\theta \| \pi_{ref}] \leq \delta
\end{equation}

\textbf{软约束（惩罚）}：
\begin{equation}
\mathcal{L} = \mathcal{L}_{RL} - \beta \cdot \mathbb{D}_{KL}[\pi_\theta \| \pi_{ref}]
\end{equation}

\textbf{自适应$\beta$}：
\begin{equation}
\beta_{t+1} = \begin{cases}
\beta_t \cdot 2 & \text{if } \mathbb{D}_{KL} > d_{target} \cdot 1.5 \\
\beta_t / 2 & \text{if } \mathbb{D}_{KL} < d_{target} / 1.5 \\
\beta_t & \text{otherwise}
\end{cases}
\end{equation}

\textbf{Token级KL}：
\begin{equation}
\mathbb{D}_{KL}^{token} = \sum_t \sum_v \pi_\theta(v|s_t) \log \frac{\pi_\theta(v|s_t)}{\pi_{ref}(v|s_t)}
\end{equation}

\subsection{分布式训练}

大规模RL训练需要分布式策略：

\subsubsection{数据并行}

将数据分配到多个GPU，每个GPU处理一部分样本：
\begin{equation}
g_{global} = \frac{1}{N}\sum_{i=1}^{N} g_i
\end{equation}

\textbf{All-Reduce实现}：
\begin{itemize}
    \item Ring All-Reduce
    \item Tree All-Reduce
    \item NCCL优化
\end{itemize}

\subsubsection{张量并行（Tensor Parallelism）}

将模型参数分割到多个GPU：

\textbf{列并行}：
\begin{equation}
Y = XW = X[W_1, W_2, \ldots, W_p] = [XW_1, XW_2, \ldots, XW_p]
\end{equation}

\textbf{行并行}：
\begin{equation}
Y = XW = [X_1, X_2, \ldots, X_p] \begin{bmatrix} W_1 \\ W_2 \\ \vdots \\ W_p \end{bmatrix} = \sum_{i=1}^{p} X_i W_i
\end{equation}

\subsubsection{流水线并行（Pipeline Parallelism）}

将模型层分配到不同GPU，形成流水线：

\textbf{GPipe}：微批次流水线
\begin{itemize}
    \item 将batch分成多个micro-batch
    \item 流水线式处理，减少气泡
\end{itemize}

\textbf{1F1B（One Forward One Backward）}：
交替执行前向和后向，进一步减少气泡。

\subsubsection{采样与训练分离}

\textbf{Actor-Learner架构}：
\begin{itemize}
    \item \textbf{Actor节点}：使用旧策略采样，不更新参数
    \item \textbf{Learner节点}：聚合经验，计算梯度，更新参数
    \item \textbf{参数服务器}：管理参数版本和同步
\end{itemize}

\textbf{异步更新}：
Actor使用稍旧的策略采样，Learner持续更新。
需要处理策略滞后问题：
\begin{equation}
\text{IS correction} = \prod_{t} \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}
\end{equation}

\subsection{内存优化}

\subsubsection{梯度检查点（Gradient Checkpointing）}

只保存部分激活值，其他在反向传播时重新计算：
\begin{itemize}
    \item 内存减少：$O(n) \rightarrow O(\sqrt{n})$
    \item 计算增加：约33\%
\end{itemize}

\subsubsection{混合精度训练}

\begin{itemize}
    \item 前向/后向：FP16/BF16
    \item 参数更新：FP32（master weights）
    \item 内存减少约50\%
\end{itemize}

\subsubsection{ZeRO优化}

\textbf{ZeRO Stage 1}：分割优化器状态
\textbf{ZeRO Stage 2}：+ 分割梯度
\textbf{ZeRO Stage 3}：+ 分割参数

\subsubsection{LoRA/QLoRA}

低秩适应，只训练少量参数：
\begin{equation}
W' = W + \Delta W = W + BA
\end{equation}
其中$B \in \mathbb{R}^{d \times r}$，$A \in \mathbb{R}^{r \times k}$，$r \ll \min(d, k)$。

\textbf{QLoRA}：在4-bit量化基础上使用LoRA。

\subsection{超参数配置指南}

\subsubsection{GRPO推荐配置}

\begin{table}[H]
\centering
\caption{GRPO超参数推荐值}
\begin{tabular}{lll}
\toprule
\textbf{参数} & \textbf{推荐范围} & \textbf{说明} \\
\midrule
组大小 $G$ & 16-64 & 越大越稳定，但成本更高 \\
裁剪参数 $\epsilon$ & 0.1-0.3 & 通常0.2效果较好 \\
KL系数 $\beta$ & 0.001-0.1 & 根据KL散度动态调整 \\
学习率 & 1e-7 - 1e-5 & 远小于SFT \\
批大小 & 256-1024 & 越大越稳定 \\
温度 & 0.7-1.0 & 采样温度 \\
最大长度 & 4096-32768 & 根据任务调整 \\
更新轮数 $K$ & 1-4 & 每批数据更新次数 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{PPO推荐配置}

\begin{table}[H]
\centering
\caption{PPO超参数推荐值}
\begin{tabular}{lll}
\toprule
\textbf{参数} & \textbf{推荐范围} & \textbf{说明} \\
\midrule
裁剪参数 $\epsilon$ & 0.1-0.2 & 标准值0.2 \\
GAE $\lambda$ & 0.95-0.99 & 偏差-方差权衡 \\
折扣因子 $\gamma$ & 0.99-1.0 & LLM通常用1.0 \\
值函数系数 $c_1$ & 0.5-1.0 & 值损失权重 \\
熵系数 $c_2$ & 0.01-0.1 & 探索鼓励 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{监控与调试}

\subsubsection{关键指标监控}

\begin{enumerate}
    \item \textbf{奖励统计}：均值、方差、分布
    \item \textbf{KL散度}：与参考模型的距离
    \item \textbf{策略熵}：探索程度
    \item \textbf{梯度范数}：训练稳定性
    \item \textbf{生成长度}：是否出现退化
    \item \textbf{准确率}：在验证集上的表现
    \item \textbf{重复率}：生成内容的重复程度
\end{enumerate}

\subsubsection{异常检测}

\begin{itemize}
    \item \textbf{奖励突变}：可能是reward hacking
    \item \textbf{KL发散}：策略变化过大
    \item \textbf{熵坍塌}：模式崩塌征兆
    \item \textbf{梯度爆炸/消失}：训练不稳定
    \item \textbf{长度异常}：过长或过短
\end{itemize}

\subsubsection{调试技巧}

\begin{enumerate}
    \item \textbf{小规模验证}：先在小模型、小数据上验证
    \item \textbf{固定随机种子}：保证可重复性
    \item \textbf{渐进式扩展}：逐步增加规模
    \item \textbf{消融实验}：隔离各组件影响
    \item \textbf{可视化生成}：定期查看生成样本
\end{enumerate}

%===========================================
\section{失败尝试与经验教训}
%===========================================

\subsection{DeepSeek报告的失败尝试}

\subsubsection{过程奖励模型（PRM）}

DeepSeek尝试使用PRM但效果不佳：

\textbf{困难}：
\begin{itemize}
    \item 需要大量步骤级标注，成本极高（每道题需要标注每个步骤）
    \item 推理步骤的"正确性"难以定义（中间步骤可能有多种表述方式）
    \item PRM泛化能力有限，易过拟合到特定格式
    \item 与RL训练的配合需要仔细调节（奖励信号时机）
    \item 错误步骤的识别本身就是一个困难的问题
\end{itemize}

\textbf{定量分析}：
\begin{itemize}
    \item 步骤级标注成本约为结果级的5-10倍
    \item PRM在训练域外问题上准确率下降30-50\%
    \item 与ORM相比，PRM训练时间增加2-3倍
\end{itemize}

\textbf{结论}：简单的结果奖励在实践中更有效。当数据和资源充足时，PRM可能有优势，但边际收益有限。

\subsubsection{蒙特卡洛树搜索（MCTS）}

尝试将MCTS应用于LLM生成：

\textbf{困难}：
\begin{itemize}
    \item 搜索空间巨大（词表大小$|\mathcal{V}| \approx 50000$，序列长度可达数千）
    \item 中间状态的价值估计不准确（partial sequence的价值难以评估）
    \item 计算成本过高（每个节点需要多次rollout）
    \item 难以与神经网络有效结合
    \item Token级决策的branching factor太大
\end{itemize}

\textbf{尝试的优化}：
\begin{itemize}
    \item 在句子/步骤级别进行搜索（减少branching factor）
    \item 使用神经网络估计节点价值（加速但准确率低）
    \item 启发式剪枝（基于概率阈值）
\end{itemize}

\textbf{结论}：简单的采样策略（如Best-of-N）可能更实用。MCTS在棋类游戏中成功，但LLM生成的结构性差异导致直接迁移困难。

\subsubsection{直接从Base模型RL}

R1-Zero实验表明，直接从Base模型开始RL虽然可行，但存在问题：

\textbf{问题}：
\begin{itemize}
    \item 输出可读性差：推理过程冗长、结构混乱
    \item 语言混杂：英文、中文、代码、数学符号无序混合
    \item 格式不稳定：有时不遵循指定格式
    \item 训练初期不稳定：奖励方差大，收敛慢
    \item 可能出现奇怪的"思考模式"：无意义的重复、自言自语
\end{itemize}

\textbf{观察到的有趣现象}：
\begin{itemize}
    \item 模型自发产生了一些推理元素（如"wait", "hmm"）
    \item 出现了自我验证行为
    \item 但格式和风格高度不稳定
\end{itemize}

\textbf{解决方案}：少量SFT冷启动 + 后续SFT修正。

\subsubsection{复杂奖励工程}

尝试设计复杂的奖励函数：

\textbf{尝试的复杂奖励}：
\begin{itemize}
    \item 步骤数量奖励（鼓励详细推理）
    \item 关键词奖励（包含特定词汇）
    \item 结构奖励（特定格式）
    \item 长度奖励（避免过长/过短）
\end{itemize}

\textbf{问题}：
\begin{itemize}
    \item 各奖励成分权重难以平衡
    \item 模型容易针对某个奖励成分进行优化
    \item 导致reward hacking
\end{itemize}

\textbf{结论}：简单、稀疏的奖励（正确性）往往效果更好。

\subsection{常见问题与解决}

\subsubsection{奖励黑客（Reward Hacking）}

\textbf{现象}：模型找到获取高奖励的"捷径"，而非真正解决问题。

\textbf{具体示例}：
\begin{itemize}
    \item 输出格式正确但内容错误："The answer is \textbackslash boxed\{42\}"（任意数字）
    \item 生成冗长但无意义的推理：大量重复、填充内容
    \item 利用评估器的漏洞：发现答案提取正则的bug
    \item 抄袭问题中的数字：当问题包含数字时，直接输出
    \item 过度自信的错误答案：用确定性语气说错话
\end{itemize}

\textbf{检测方法}：
\begin{itemize}
    \item 监控奖励分布的异常变化
    \item 人工抽检高奖励样本
    \item 对抗性测试集
    \item 分析生成内容的多样性
\end{itemize}

\textbf{解决方案}：
\begin{itemize}
    \item \textbf{KL散度约束}：防止策略偏离过远
    \item \textbf{多样化奖励函数}：使用多个独立的奖励信号
    \item \textbf{对抗性测试}：专门设计检测捷径的测试
    \item \textbf{人工审核}：定期人工检查
    \item \textbf{早停}：在奖励突然提升时警觉
    \item \textbf{奖励模型集成}：使用多个奖励模型投票
\end{itemize}

\subsubsection{模式崩塌（Mode Collapse）}

\textbf{现象}：模型输出变得单一，缺乏多样性。

\textbf{具体表现}：
\begin{itemize}
    \item 对不同问题给出相似的回答模板
    \item 推理路径高度相似
    \item 词汇使用范围变窄
    \item 熵急剧下降
\end{itemize}

\textbf{原因分析}：
\begin{itemize}
    \item 高奖励样本被过度学习
    \item 探索不足
    \item KL惩罚不够
\end{itemize}

\textbf{解决方案}：
\begin{itemize}
    \item \textbf{熵正则化}：$\mathcal{L} = \mathcal{L}_{RL} + \lambda H(\pi_\theta)$
    \item \textbf{温度控制}：适当提高采样温度
    \item \textbf{多样性奖励}：奖励与历史输出不同的生成
    \item \textbf{Population-based训练}：维护多个策略变体
    \item \textbf{重放缓冲区多样性}：确保训练数据多样
\end{itemize}

\subsubsection{遗忘问题（Catastrophic Forgetting）}

\textbf{现象}：RL训练后，模型失去原有的通用能力。

\textbf{具体表现}：
\begin{itemize}
    \item 在非RL任务上性能下降
    \item 丢失某些语言能力
    \item 格式和风格退化
    \item 常识问答能力下降
\end{itemize}

\textbf{解决方案}：
\begin{itemize}
    \item \textbf{混合训练数据}：RL数据 + 通用数据
    \item \textbf{KL约束}：保持与基础模型的距离
    \item \textbf{分阶段训练}：先RL后恢复性SFT
    \item \textbf{弹性权重巩固（EWC）}：保护重要参数
    \item \textbf{定期评估}：监控通用能力指标
\end{itemize}

\subsubsection{训练不稳定}

\textbf{现象}：损失震荡、性能波动大、奖励不收敛。

\textbf{原因分析}：
\begin{itemize}
    \item 学习率过高
    \item 批大小过小
    \item 奖励噪声大
    \item 策略变化过快
    \item 值函数估计不准
\end{itemize}

\textbf{解决方案}：
\begin{itemize}
    \item \textbf{降低学习率}：通常需要比SFT低1-2个数量级
    \item \textbf{增加批大小}：更稳定的梯度估计
    \item \textbf{梯度裁剪}：防止梯度爆炸
    \item \textbf{更保守的裁剪参数}：$\epsilon = 0.1$而非$0.2$
    \item \textbf{奖励归一化}：减少奖励方差
    \item \textbf{Warmup}：逐步增加学习率
    \item \textbf{检查点}：保存多个检查点以便回滚
\end{itemize}

\subsubsection{长度退化}

\textbf{现象}：生成内容越来越长或越来越短。

\textbf{长度爆炸原因}：
\begin{itemize}
    \item 无长度惩罚时，长回答可能获得更高奖励
    \item 模型学会用冗长内容"填充"
\end{itemize}

\textbf{长度塌缩原因}：
\begin{itemize}
    \item 过强的长度惩罚
    \item 短回答容易获得格式奖励
\end{itemize}

\textbf{解决方案}：
\begin{itemize}
    \item 适度的长度惩罚/奖励
    \item 使用参考长度进行归一化
    \item 监控长度分布变化
\end{itemize}

\subsection{规模化中的挑战}

\subsubsection{计算资源管理}

\textbf{挑战}：
\begin{itemize}
    \item RL需要大量采样，GPU利用率低
    \item 采样和训练的异步协调
    \item 参数同步开销
    \item 内存管理复杂
\end{itemize}

\textbf{经验}：
\begin{itemize}
    \item 采样使用较低精度（FP16/INT8）
    \item 训练使用混合精度
    \item 分离采样和训练GPU资源
    \item 使用高效的参数同步策略
\end{itemize}

\subsubsection{超参数敏感性}

\textbf{发现}：
\begin{itemize}
    \item 不同规模模型需要不同超参数
    \item 最优学习率随模型大小变化
    \item KL系数需要根据任务调整
\end{itemize}

\textbf{建议}：
\begin{itemize}
    \item 从小模型开始调参
    \item 使用自适应调度
    \item 保守起步，逐步激进
\end{itemize}

\textbf{现象}：模型找到获取高奖励的"捷径"，而非真正解决问题。

\textbf{示例}：
\begin{itemize}
    \item 输出格式正确但内容错误
    \item 生成冗长但无意义的推理
    \item 利用评估器的漏洞
\end{itemize}

\textbf{解决方案}：
\begin{itemize}
    \item KL散度约束
    \item 多样化奖励函数
    \item 对抗性测试
    \item 人工审核
\end{itemize}

\subsubsection{模式崩塌（Mode Collapse）}

\textbf{现象}：模型输出变得单一，缺乏多样性。

\textbf{解决方案}：
\begin{itemize}
    \item 熵正则化
    \item 温度控制
    \item 多样性奖励
\end{itemize}

\subsubsection{遗忘问题}

\textbf{现象}：RL训练后，模型失去原有的通用能力。

\textbf{解决方案}：
\begin{itemize}
    \item 混合训练数据（包含通用任务）
    \item KL约束保持与基础模型的距离
    \item 分阶段训练
\end{itemize}

\subsubsection{训练不稳定}

\textbf{现象}：损失震荡、性能波动大。

\textbf{解决方案}：
\begin{itemize}
    \item 降低学习率
    \item 增加批大小
    \item 梯度裁剪
    \item 更保守的裁剪参数
\end{itemize}

%===========================================
\section{未来研究方向}
%===========================================

\subsection{算法改进}

\subsubsection{更高效的RL算法}

\begin{itemize}
    \item \textbf{减少样本复杂度}：
    \begin{itemize}
        \item 模型预测环境动态
        \item 数据重用和重要性采样改进
        \item 离线RL方法
    \end{itemize}
    
    \item \textbf{更稳定的训练}：
    \begin{itemize}
        \item 自适应信任域
        \item 鲁棒优化目标
        \item 元学习超参数
    \end{itemize}
    
    \item \textbf{更低的计算成本}：
    \begin{itemize}
        \item 稀疏激活
        \item 参数高效微调
        \item 知识蒸馏
    \end{itemize}
\end{itemize}

\subsubsection{更好的奖励设计}

\begin{itemize}
    \item \textbf{自动奖励发现}：
    \begin{itemize}
        \item 从人类偏好中学习
        \item 逆强化学习
        \item 奖励函数搜索
    \end{itemize}
    
    \item \textbf{多目标优化}：
    \begin{itemize}
        \item 帕累托最优
        \item 约束优化
        \item 动态目标平衡
    \end{itemize}
    
    \item \textbf{可解释的奖励}：
    \begin{itemize}
        \item 奖励分解
        \item 可视化分析
        \item 因果归因
    \end{itemize}
\end{itemize}

\subsubsection{混合方法}

\begin{itemize}
    \item RL + 符号推理结合
    \item RL + 检索增强
    \item RL + 工具使用
    \item RL + 多Agent协作
\end{itemize}

\subsection{推理时Scaling}

\subsubsection{高效搜索算法}

\begin{itemize}
    \item \textbf{自适应搜索深度}：
    \begin{itemize}
        \item 基于置信度的早停
        \item 动态资源分配
        \item 问题难度预估
    \end{itemize}
    
    \item \textbf{剪枝策略}：
    \begin{itemize}
        \item 基于价值的剪枝
        \item 基于多样性的剪枝
        \item 启发式剪枝
    \end{itemize}
    
    \item \textbf{并行搜索}：
    \begin{itemize}
        \item 投机解码
        \item 并行候选评估
        \item 分布式搜索
    \end{itemize}
\end{itemize}

\subsubsection{动态计算分配}

\begin{itemize}
    \item \textbf{Adaptive Computation}：
    \begin{equation}
    \text{compute}(q) = f(\text{difficulty}(q), \text{confidence}(\pi, q))
    \end{equation}
    
    \item \textbf{早退机制}：简单问题快速回答
    
    \item \textbf{分层推理}：先粗后细
    
    \item \textbf{置信度校准}：准确估计何时需要更多计算
\end{itemize}

\subsubsection{推理优化方法}

\begin{itemize}
    \item \textbf{思维链压缩}：保持推理质量的同时减少token
    \item \textbf{增量推理}：复用之前的计算
    \item \textbf{推理缓存}：相似问题共享推理
\end{itemize}

\subsection{多模态推理}

\subsubsection{视觉推理}

将推理能力扩展到视觉任务：
\begin{itemize}
    \item \textbf{图表理解}：图表数据提取和分析
    \item \textbf{几何问题}：视觉几何推理
    \item \textbf{视觉问答}：需要复杂推理的VQA
    \item \textbf{科学图像}：医学影像、天文图像分析
\end{itemize}

\textbf{技术挑战}：
\begin{itemize}
    \item 视觉信息的符号化
    \item 跨模态对齐
    \item 视觉注意力机制
\end{itemize}

\subsubsection{跨模态推理}

结合多种模态进行推理：
\begin{itemize}
    \item \textbf{图文数学}：图表+文字的数学问题
    \item \textbf{代码+文档}：代码理解与生成
    \item \textbf{多模态科学}：结合实验数据、图表、文献
    \item \textbf{具身推理}：结合物理世界交互
\end{itemize}

\subsection{Agent与工具使用}

\subsubsection{工具增强推理}

\begin{itemize}
    \item \textbf{计算器}：精确数值计算
    \item \textbf{符号引擎}：SymPy等符号计算
    \item \textbf{搜索引擎}：获取外部知识
    \item \textbf{代码执行}：Python解释器
    \item \textbf{数据库}：结构化数据查询
\end{itemize}

\textbf{RL用于工具选择}：
\begin{equation}
\pi_{tool}(t | s) = \text{softmax}(f_{tool}(s, t))
\end{equation}

\subsubsection{多Agent推理}

\begin{itemize}
    \item \textbf{辩论}：多个模型辩论得出结论
    \item \textbf{分工}：不同模型负责不同步骤
    \item \textbf{验证}：一个模型生成，另一个验证
    \item \textbf{集成}：多模型投票
\end{itemize}

\subsection{理论研究}

\subsubsection{推理能力的本质}

\begin{itemize}
    \item 模型真的在"推理"还是在模式匹配？
    \item 涌现能力的机制是什么？
    \item 推理能力的可迁移性边界？
    \item 训练数据如何影响推理能力？
\end{itemize}

\subsubsection{Scaling Laws}

\begin{itemize}
    \item 推理能力如何随模型大小scaling？
    \item 推理时计算的scaling law？
    \item RL训练的样本效率scaling？
    \item 最优资源分配策略？
\end{itemize}

\subsubsection{泛化理论}

\begin{itemize}
    \item OOD推理能力的理论保证
    \item 组合泛化的条件
    \item 长度泛化的机制
\end{itemize}

\subsection{安全与对齐}

\subsubsection{安全推理}

\begin{itemize}
    \item 推理模型可能更擅长欺骗
    \item 如何验证推理过程的真实性？
    \item 隐藏推理的风险
    \item 推理过程的可监控性
\end{itemize}

\subsubsection{对齐挑战}

\begin{itemize}
    \item 复杂推理中的价值对齐
    \item 推理目标与人类意图的一致性
    \item 长程推理中的对齐漂移
\end{itemize}

\subsection{开放问题}

\begin{enumerate}
    \item \textbf{泛化性}：如何让推理能力迁移到新领域？当前的推理模型在训练域外问题上表现如何？
    
    \item \textbf{可解释性}：模型真的在"推理"还是在模式匹配？如何区分真正的推理和sophisticated的模式匹配？
    
    \item \textbf{长程规划}：如何进行更长horizon的推理？现有方法在需要数百步推理的问题上表现如何？
    
    \item \textbf{世界模型}：是否需要显式的世界知识？内隐知识vs外显知识的权衡？
    
    \item \textbf{安全性}：推理模型可能更擅长欺骗？如何确保推理过程是透明和可信的？
    
    \item \textbf{效率}：如何在保持性能的同时降低推理成本？推理时间和准确率的最优权衡？
    
    \item \textbf{理论基础}：RL涌现推理能力的理论解释是什么？是否存在必要条件？
    
    \item \textbf{数据效率}：需要多少高质量问题才能训练出强推理能力？数据质量vs数量的权衡？
    
    \item \textbf{能力组合}：不同推理能力如何组合？是否存在基本的推理原语？
    
    \item \textbf{持续学习}：如何让模型持续学习新的推理能力而不遗忘旧能力？
\end{enumerate}

%===========================================
\section{BitNet：1比特大语言模型}
%===========================================

随着LLM规模不断扩大，模型的部署成本、能耗问题日益严峻。微软研究院提出的BitNet系列为大语言模型带来了革命性的效率提升，通过极端量化技术实现了性能与效率的最优平衡。本节将详细介绍BitNet的技术原理、训练推理流程，以及与传统LLM的全面对比。

\subsection{BitNet技术概述}

\subsubsection{发展历程}

BitNet系列经历了以下关键发展阶段：

\begin{table}[H]
\centering
\caption{BitNet发展时间线}
\begin{tabular}{llp{9cm}}
\toprule
\textbf{时间} & \textbf{版本} & \textbf{主要贡献} \\
\midrule
2023.10 & BitNet & 首次提出1-bit Transformer架构，证明可规模化训练 \\
2024.02 & BitNet b1.58 & 引入三值权重\{-1, 0, 1\}，性能匹配FP16模型 \\
2024.10 & bitnet.cpp & 发布官方CPU推理框架，实现高效本地部署 \\
2024.11 & BitNet a4.8 & 引入4-bit激活，进一步优化推理效率 \\
2025.04 & BitNet b1.58-2B-4T & 官方发布2B参数模型，训练于4T tokens \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{核心创新：BitLinear层}

BitNet的核心是用\textbf{BitLinear}层替代传统的\texttt{nn.Linear}层。BitLinear的设计原则是：

\begin{enumerate}
    \item \textbf{权重量化}：将权重量化为极低比特表示
    \item \textbf{激活量化}：对激活值进行量化以提高计算效率
    \item \textbf{缩放因子}：使用可学习或统计的缩放因子恢复数值范围
\end{enumerate}

\paragraph{原始BitNet（1-bit）}

原始BitNet使用二值权重$\{-1, +1\}$：
\begin{equation}
\tilde{W} = \text{Sign}(W - \mathbb{E}[W])
\end{equation}

其中Sign函数定义为：
\begin{equation}
\text{Sign}(x) = \begin{cases}
+1, & \text{if } x > 0 \\
-1, & \text{if } x \leq 0
\end{cases}
\end{equation}

\paragraph{BitNet b1.58（1.58-bit）}

BitNet b1.58采用三值权重$\{-1, 0, +1\}$，每个权重理论上需要$\log_2(3) \approx 1.58$比特：
\begin{equation}
\tilde{W} = \text{RoundClip}\left(\frac{W}{\gamma + \epsilon}, -1, 1\right)
\end{equation}

其中：
\begin{itemize}
    \item $\gamma = \frac{1}{nm}\sum_{i,j}|W_{ij}|$ 是权重矩阵的平均绝对值
    \item RoundClip函数将值四舍五入并裁剪到$[-1, 1]$范围
    \item $\epsilon$是防止除零的小常数
\end{itemize}

\paragraph{激活量化}

激活采用absmax量化到$b$-bit整数：
\begin{equation}
\tilde{X} = \text{Quant}(X) = \text{Clip}\left(\lfloor X \cdot \frac{Q_b}{\|X\|_\infty} \rceil, -Q_b + \epsilon, Q_b - \epsilon\right)
\end{equation}

其中$Q_b = 2^{b-1}$，BitNet b1.58默认使用8-bit激活。

\subsubsection{BitLinear前向传播}

BitLinear层的完整前向传播包含以下步骤：

\begin{algorithm}[H]
\caption{BitLinear前向传播}
\begin{algorithmic}[1]
\REQUIRE 输入$X \in \mathbb{R}^{B \times T \times d_{in}}$，权重$W \in \mathbb{R}^{d_{out} \times d_{in}}$
\STATE \textbf{Layer Normalization}: $X' = \text{LN}(X)$
\STATE \textbf{激活量化}: $\tilde{X} = \text{Quant}(X')$，记录$\gamma_X = \|X'\|_\infty$
\STATE \textbf{权重量化}: $\tilde{W} = \text{RoundClip}(W/\gamma_W, -1, 1)$，$\gamma_W = \text{mean}(|W|)$
\STATE \textbf{整数矩阵乘法}: $Y = \tilde{X} \cdot \tilde{W}^T$ \quad (无乘法运算！)
\STATE \textbf{反量化}: $\hat{Y} = Y \cdot \frac{\gamma_W \gamma_X}{Q_b}$
\RETURN $\hat{Y}$
\end{algorithmic}
\end{algorithm}

\textbf{关键优势}：由于权重只有$\{-1, 0, +1\}$三个值，矩阵乘法可以完全用加减法实现：
\begin{itemize}
    \item $+1 \times x = x$（直接取值）
    \item $-1 \times x = -x$（符号翻转）
    \item $0 \times x = 0$（跳过）
\end{itemize}

\subsection{训练与推理流程}

\subsubsection{训练流程}

\textbf{重要说明}：BitNet模型\textbf{必须从头训练}，无法通过后训练量化（Post-Training Quantization, PTQ）从现有FP16模型转换。

\paragraph{直通估计器（Straight-Through Estimator, STE）}

由于量化函数不可微，训练时使用STE进行梯度传播：

\begin{equation}
\frac{\partial \mathcal{L}}{\partial W} \approx \frac{\partial \mathcal{L}}{\partial \tilde{W}}
\end{equation}

即梯度直接传递穿过量化操作。

\paragraph{训练时的计算}

训练时需要维护两套权重：
\begin{itemize}
    \item \textbf{高精度主权重（Latent Weights）}：FP32/BF16精度，用于梯度累积
    \item \textbf{量化权重}：训练前向和后向传播时动态量化生成
\end{itemize}

训练过程如图~\ref{fig:bitnet_training}所示：

\begin{figure}[H]
\centering
\begin{tikzpicture}[
    node distance=1.5cm,
    box/.style={rectangle, draw, rounded corners, minimum width=3cm, minimum height=0.8cm, align=center},
    arrow/.style={->, thick}
]
\node[box, fill=blue!20] (latent) {高精度主权重\\$W^{(fp)}$};
\node[box, fill=green!20, right=2cm of latent] (quant) {量化权重\\$\tilde{W} \in \{-1,0,1\}$};
\node[box, fill=orange!20, below=1cm of quant] (forward) {前向传播\\（使用$\tilde{W}$）};
\node[box, fill=red!20, below=1cm of forward] (backward) {后向传播\\（STE梯度）};
\node[box, fill=purple!20, left=2cm of backward] (update) {梯度更新\\$W^{(fp)} \leftarrow W^{(fp)} - \eta\nabla$};

\draw[arrow] (latent) -- node[above] {量化} (quant);
\draw[arrow] (quant) -- (forward);
\draw[arrow] (forward) -- (backward);
\draw[arrow] (backward) -- node[below] {STE} (update);
\draw[arrow] (update) -- (latent);
\end{tikzpicture}
\caption{BitNet训练流程}
\label{fig:bitnet_training}
\end{figure}

\paragraph{训练硬件要求}

\begin{itemize}
    \item \textbf{训练仍需GPU}：由于需要高精度梯度计算和反向传播，训练过程仍需要传统GPU（NVIDIA A100/H100等）
    \item \textbf{内存需求}：训练时内存需求与FP16模型相近，因为需要维护高精度主权重和优化器状态
    \item \textbf{训练速度}：训练速度与FP16模型相当，主要优势在推理阶段
\end{itemize}

\subsubsection{推理流程}

推理时的显著优势：

\begin{enumerate}
    \item \textbf{权重存储}：只存储量化后的三值权重，大幅减少内存占用
    \item \textbf{无乘法计算}：矩阵运算只需加减法
    \item \textbf{CPU高效}：可在普通CPU上实现高效推理
\end{enumerate}

\paragraph{bitnet.cpp推理框架}

微软发布的官方推理框架bitnet.cpp针对1.58-bit模型进行了深度优化：

\begin{itemize}
    \item \textbf{查表法（LUT）}：将权重编码为查表索引，避免条件分支
    \item \textbf{SIMD优化}：充分利用x86 AVX2/AVX-512和ARM NEON指令
    \item \textbf{内存布局}：优化的权重打包格式，减少内存访问
\end{itemize}

\subsection{性能对比分析}

\subsubsection{模型大小与内存消耗}

\begin{table}[H]
\centering
\caption{BitNet与FP16模型内存占用对比}
\begin{tabular}{lcccc}
\toprule
\textbf{模型规模} & \textbf{FP16内存} & \textbf{BitNet b1.58内存} & \textbf{压缩比} & \textbf{备注} \\
\midrule
700M & 1.4 GB & 0.4 GB & 3.5x & Embedding未量化 \\
1.3B & 2.6 GB & 0.7 GB & 3.7x & \\
3B & 6 GB & 1.5 GB & 4.0x & \\
7B & 14 GB & 2.5 GB & 5.6x & \\
13B & 26 GB & 4.0 GB & 6.5x & \\
70B & 140 GB & 20 GB & 7.0x & 理论估计值 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{注意}：Embedding层和LM Head保持高精度（FP16），因此小模型的压缩比相对较低。随着模型增大，线性层占比增加，压缩比提升。

\subsubsection{推理延迟}

基于bitnet.cpp的实测性能数据：

\begin{table}[H]
\centering
\caption{BitNet b1.58推理加速比（相对于llama.cpp FP16）}
\begin{tabular}{lccc}
\toprule
\textbf{模型大小} & \textbf{x86 CPU加速} & \textbf{ARM CPU加速} & \textbf{备注} \\
\midrule
700M & 2.37x & 1.37x & Intel/AMD vs Apple M系列 \\
1.3B & 2.99x & 2.02x & \\
3B & 4.08x & 3.25x & \\
7B & 5.02x & 4.28x & \\
70B（理论） & 6.17x & 5.07x & 模型越大加速越明显 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{100B模型CPU运行}

bitnet.cpp的一个标志性成就是能够在\textbf{单个CPU}上运行100B参数的BitNet模型，达到\textbf{5-7 tokens/秒}的生成速度——接近人类阅读速度。这使得超大模型的本地部署成为可能。

\subsubsection{能耗对比}

能耗是BitNet最显著的优势之一：

\begin{table}[H]
\centering
\caption{BitNet b1.58能耗降低比例}
\begin{tabular}{lcc}
\toprule
\textbf{平台} & \textbf{能耗降低} & \textbf{测试条件} \\
\midrule
x86 CPU & 71.9\% - 82.2\% & Intel/AMD服务器CPU \\
ARM CPU & 55.4\% - 70.0\% & Apple M2芯片 \\
理论GPU & >90\% & 消除乘法运算 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{能耗优势来源}

\begin{itemize}
    \item \textbf{消除浮点乘法}：乘法运算能耗远高于加法
    \item \textbf{减少内存访问}：权重压缩降低内存带宽需求
    \item \textbf{缓存友好}：更多权重可驻留在高速缓存中
\end{itemize}

\subsubsection{模型性能对比}

BitNet b1.58在语言建模任务上达到与FP16模型相当的性能：

\begin{table}[H]
\centering
\caption{BitNet b1.58与FP16基线模型性能对比（相同参数量和训练tokens）}
\begin{tabular}{lcccc}
\toprule
\textbf{模型} & \textbf{参数量} & \textbf{训练Tokens} & \textbf{PPL} & \textbf{备注} \\
\midrule
LLaMA FP16 & 700M & 100B & 12.33 & 基线 \\
BitNet b1.58 & 700M & 100B & 12.87 & 略高0.54 \\
\midrule
LLaMA FP16 & 1.3B & 100B & 11.25 & 基线 \\
BitNet b1.58 & 1.3B & 100B & 11.29 & 几乎相同 \\
\midrule
LLaMA FP16 & 3B & 100B & 10.04 & 基线 \\
BitNet b1.58 & 3B & 100B & 9.91 & \textbf{更好} \\
\midrule
StableLM & 3B & 2T & 9.22 & 参考 \\
BitNet b1.58 & 3B & 2T & 8.21 & \textbf{显著更好} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{关键发现}：
\begin{enumerate}
    \item 模型规模越大，BitNet相对于FP16的差距越小，甚至可能反超
    \item 大规模训练（更多tokens）时，BitNet表现更好
    \item 三值权重可能起到正则化作用，提高泛化能力
\end{enumerate}

\subsubsection{零样本任务性能}

BitNet b1.58 3B模型在零样本任务上的表现：

\begin{table}[H]
\centering
\caption{BitNet b1.58 3B零样本性能}
\begin{tabular}{lcc}
\toprule
\textbf{任务} & \textbf{BitNet b1.58} & \textbf{LLaMA FP16} \\
\midrule
ARC-Easy & 66.9 & 67.0 \\
ARC-Challenge & 34.6 & 34.0 \\
HellaSwag & 59.4 & 59.7 \\
Winogrande & 61.2 & 61.0 \\
PIQA & 74.5 & 74.6 \\
OpenbookQA & 34.0 & 33.2 \\
\textbf{平均} & \textbf{55.1} & \textbf{54.9} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{与传统量化方法对比}

\subsubsection{后训练量化（PTQ）vs BitNet}

\begin{table}[H]
\centering
\caption{BitNet与后训练量化方法对比}
\begin{tabular}{lcccc}
\toprule
\textbf{方法} & \textbf{精度损失} & \textbf{是否需重训} & \textbf{压缩比} & \textbf{推理优化} \\
\midrule
GPTQ (4-bit) & 中等 & 否 & 4x & 需特殊kernel \\
AWQ (4-bit) & 较小 & 否 & 4x & 需特殊kernel \\
GGUF Q4\_K\_M & 较小 & 否 & 4x & llama.cpp支持 \\
\midrule
BitNet b1.58 & \textbf{几乎无} & \textbf{是} & \textbf{7-10x} & \textbf{无乘法} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{关键区别}：BitNet不是量化方法，而是\textbf{从头训练}的新架构。

\subsubsection{为什么不能从FP16转换？}

\begin{enumerate}
    \item \textbf{信息容量不匹配}：FP16权重包含的信息无法无损压缩到1.58-bit
    \item \textbf{权重分布差异}：FP16模型的权重分布不适合直接量化为三值
    \item \textbf{训练适应性}：BitNet在训练过程中学会利用三值权重表达能力
\end{enumerate}

实验表明，对FP16模型进行三值量化会导致严重的性能下降（PPL增加数倍），而从头训练的BitNet可以达到与FP16相当甚至更好的性能。

\subsection{硬件优化与未来展望}

\subsubsection{当前硬件支持}

\begin{itemize}
    \item \textbf{CPU}：bitnet.cpp支持x86（AVX2/AVX-512）和ARM（NEON）
    \item \textbf{GPU}：2025年5月发布官方GPU kernel
    \item \textbf{NPU}：计划中，适合1-bit运算的专用加速器
\end{itemize}

\subsubsection{专用硬件前景}

BitNet为设计专用AI芯片开辟了新方向：

\begin{enumerate}
    \item \textbf{简化ALU设计}：只需加法器，无需乘法器
    \item \textbf{降低功耗}：从根本上减少能耗
    \item \textbf{提高密度}：更简单的计算单元意味着更高的集成度
    \item \textbf{内存墙突破}：权重压缩缓解内存带宽瓶颈
\end{enumerate}

\subsubsection{BitNet与RL训练的结合}

BitNet技术与RL训练的结合具有重要意义：

\begin{enumerate}
    \item \textbf{高效RL采样}：推理效率提升使RL采样更快、更便宜
    \item \textbf{本地自我博弈}：可在边缘设备进行自我博弈训练
    \item \textbf{大规模探索}：更低的推理成本支持更大规模的策略探索
    \item \textbf{绿色AI}：显著降低RL训练的碳足迹
\end{enumerate}

\begin{remark}[BitNet的局限性]
当前BitNet的主要限制包括：
\begin{enumerate}
    \item \textbf{必须从头训练}：无法利用现有预训练模型
    \item \textbf{训练成本不变}：训练阶段仍需传统GPU
    \item \textbf{生态不完善}：工具链和模型支持有限
    \item \textbf{超大模型待验证}：100B+规模的实际性能需进一步验证
\end{enumerate}
\end{remark}

\subsection{官方模型资源}

微软已开源以下BitNet模型和工具：

\begin{itemize}
    \item \textbf{模型权重}：
    \begin{itemize}
        \item BitNet-b1.58-2B-4T: \url{https://huggingface.co/microsoft/BitNet-b1.58-2B-4T}
        \item BitNet-b1.58-2B-4T-bf16（训练权重）: \url{https://huggingface.co/microsoft/bitnet-b1.58-2B-4T-bf16}
    \end{itemize}
    \item \textbf{推理框架}：\url{https://github.com/microsoft/BitNet}
    \item \textbf{第三方支持}：llama.cpp已支持BitNet模型推理
\end{itemize}

\subsection{小结}

BitNet代表了LLM效率优化的重要方向：

\begin{enumerate}
    \item \textbf{技术创新}：三值权重实现性能-效率最优平衡
    \item \textbf{训练特性}：需从头训练，训练时仍需GPU
    \item \textbf{推理优势}：内存降低7x，能耗降低70-80\%，速度提升2-6x
    \item \textbf{部署革命}：100B模型可在单CPU上运行
    \item \textbf{未来潜力}：专用硬件可进一步放大优势
\end{enumerate}

%===========================================
\section{总结}
%===========================================

\subsection{核心结论}

\begin{enumerate}
    \item \textbf{RL是提升LLM推理能力的有效方法}
    \begin{itemize}
        \item 无需大量人类标注
        \item 可以超越人类示范
        \item 涌现出复杂推理行为
        \item DeepSeek-R1证明纯RL可以训练出竞争性推理能力
    \end{itemize}
    
    \item \textbf{简单奖励可能比复杂奖励更有效}
    \begin{itemize}
        \item 结果奖励足以训练出推理能力
        \item PRM实现复杂，效果不一定更好
        \item 规则验证提供准确信号
        \item 避免奖励工程过度复杂化
    \end{itemize}
    
    \item \textbf{GRPO简化了训练流程}
    \begin{itemize}
        \item 无需Critic网络
        \item 组内相对比较更稳定
        \item 降低计算和内存成本
        \item 实现简单，效果良好
    \end{itemize}
    
    \item \textbf{推理时Scaling开辟新维度}
    \begin{itemize}
        \item 更多思考=更好结果
        \item Test-time compute很重要
        \item 为提升能力提供新途径
        \item 与模型大小scaling互补
    \end{itemize}
    
    \item \textbf{涌现现象值得深入研究}
    \begin{itemize}
        \item 自我验证、反思等行为自发出现
        \item RL压力下模型发展出新能力
        \item 理论解释仍然缺乏
    \end{itemize}
\end{enumerate}

\subsection{本报告的创新贡献}

本报告除了系统综述现有方法外，还提出了以下创新算法框架：

\begin{enumerate}
    \item \textbf{AHRO（自适应分层奖励优化）}：动态分层奖励结构，根据问题特性自适应调整奖励分配
    
    \item \textbf{CAPL（认知架构引导策略学习）}：借鉴双过程理论，设计显式认知架构
    
    \item \textbf{DCoT-SE（动态思维链自演化）}：可演化的推理模式库，自主演化推理策略
    
    \item \textbf{MGRCR（多粒度推理一致性强化）}：多层次一致性约束，提高推理可靠性
    
    \item \textbf{MFLO（元认知反馈回路优化）}：引入元认知层，监控和调节认知过程
    
    \item \textbf{ARRE（对抗推理鲁棒性增强）}：对抗训练提高推理鲁棒性
    
    \item \textbf{CRRL（课程强化推理学习）}：多维度课程设计，渐进式能力提升
    
    \item \textbf{RCTL（推理能力迁移学习框架）}：原子能力分解与迁移
    
    \item \textbf{CREL（因果推理增强学习）}：显式因果建模
    
    \item \textbf{DSPRO（分布式自博弈推理优化）}：大规模自博弈训练系统
\end{enumerate}

\subsection{实践建议}

对于希望训练推理模型的研究者，建议：

\begin{enumerate}
    \item \textbf{从简单开始}：
    \begin{itemize}
        \item 先尝试规则奖励和GRPO
        \item 不要一开始就使用复杂的奖励设计
        \item 验证基本流程后再增加复杂度
    \end{itemize}
    
    \item \textbf{数据质量优先}：
    \begin{itemize}
        \item 高质量的问题集比数量更重要
        \item 确保答案验证的准确性
        \item 覆盖多种难度级别
    \end{itemize}
    
    \item \textbf{渐进式训练}：
    \begin{itemize}
        \item 少量SFT → RL → 精调
        \item 考虑使用课程学习
        \item 保存多个检查点
    \end{itemize}
    
    \item \textbf{监控关键指标}：
    \begin{itemize}
        \item KL散度、奖励分布、生成质量
        \item 定期人工抽检
        \item 设置异常警报
    \end{itemize}
    
    \item \textbf{重视失败案例}：
    \begin{itemize}
        \item 分析失败模式指导改进
        \item 建立错误案例库
        \item 针对性改进
    \end{itemize}
    
    \item \textbf{资源规划}：
    \begin{itemize}
        \item RL训练需要大量计算资源
        \item 合理分配采样和训练资源
        \item 考虑使用参数高效方法
    \end{itemize}
\end{enumerate}

\subsection{展望}

LLM的RL训练正处于快速发展期，以下趋势值得关注：

\begin{enumerate}
    \item \textbf{规模继续扩大}：更大的模型、更多的计算
    \item \textbf{多模态融合}：推理能力扩展到视觉等模态
    \item \textbf{Agent化}：与工具、环境交互的推理
    \item \textbf{安全对齐}：确保强推理能力的安全使用
    \item \textbf{理论突破}：理解推理涌现的本质
    \item \textbf{效率提升}：更高效的训练和推理
    \item \textbf{开源生态}：更多开源模型和工具
\end{enumerate}

%===========================================
\section*{参考文献}
%===========================================

\begin{enumerate}[label={[\arabic*]}]
    \item DeepSeek-AI. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv:2501.12948, 2025.
    
    \item DeepSeek-AI. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. arXiv:2402.03300, 2024.
    
    \item Lightman, H., et al. Let's Verify Step by Step. arXiv:2305.20050, 2023.
    
    \item Bai, Y., et al. Constitutional AI: Harmlessness from AI Feedback. arXiv:2212.08073, 2022.
    
    \item Ouyang, L., et al. Training Language Models to Follow Instructions with Human Feedback. NeurIPS, 2022.
    
    \item OpenAI. Learning to Reason with LLMs. OpenAI Blog, 2024.
    
    \item Schulman, J., et al. Proximal Policy Optimization Algorithms. arXiv:1707.06347, 2017.
    
    \item Wei, J., et al. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. NeurIPS, 2022.
    
    \item Wang, X., et al. Self-Consistency Improves Chain of Thought Reasoning in Language Models. ICLR, 2023.
    
    \item Cobbe, K., et al. Training Verifiers to Solve Math Word Problems. arXiv:2110.14168, 2021.
    
    \item Gao, L., et al. Scaling Laws for Reward Model Overoptimization. ICML, 2023.
    
    \item Touvron, H., et al. LLaMA: Open and Efficient Foundation Language Models. arXiv:2302.13971, 2023.
    
    \item Zelikman, E., et al. STaR: Self-Taught Reasoner. NeurIPS, 2022.
    
    \item Zelikman, E., et al. Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking. arXiv:2403.09629, 2024.
    
    \item Rafailov, R., et al. Direct Preference Optimization: Your Language Model is Secretly a Reward Model. NeurIPS, 2023.
    
    \item Lee, H., et al. RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback. arXiv:2309.00267, 2023.
    
    \item Chen, X., et al. Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models. arXiv:2401.01335, 2024.
    
    \item Kojima, T., et al. Large Language Models are Zero-Shot Reasoners. NeurIPS, 2022.
    
    \item Yao, S., et al. Tree of Thoughts: Deliberate Problem Solving with Large Language Models. NeurIPS, 2023.
    
    \item Silver, D., et al. Mastering the Game of Go with Deep Neural Networks and Tree Search. Nature, 2016.
    
    \item Hu, E., et al. LoRA: Low-Rank Adaptation of Large Language Models. ICLR, 2022.
    
    \item Dettmers, T., et al. QLoRA: Efficient Finetuning of Quantized LLMs. NeurIPS, 2023.
    
    \item Rajbhandari, S., et al. ZeRO: Memory Optimizations Toward Training Trillion Parameter Models. SC, 2020.
    
    \item Shinn, N., et al. Reflexion: Language Agents with Verbal Reinforcement Learning. NeurIPS, 2023.
    
    \item Anthropic. Claude 3 Model Card. Anthropic, 2024.
    
    \item Google. Gemini: A Family of Highly Capable Multimodal Models. arXiv:2312.11805, 2023.
    
    \item Yuan, W., et al. Self-Rewarding Language Models. arXiv:2401.10020, 2024.
    
    \item Wang, P., et al. Self-Instruct: Aligning Language Models with Self-Generated Instructions. ACL, 2023.
    
    \item Havrilla, A., et al. Teaching Large Language Models to Self-Debug. arXiv:2304.05128, 2023.
    
    \item Madaan, A., et al. Self-Refine: Iterative Refinement with Self-Feedback. NeurIPS, 2023.
    
    \item Hendrycks, D., et al. Measuring Mathematical Problem Solving With the MATH Dataset. NeurIPS, 2021.
    
    \item Cobbe, K., et al. GSM8K: Training Verifiers to Solve Math Word Problems. arXiv:2110.14168, 2021.
    
    \item Chen, M., et al. Evaluating Large Language Models Trained on Code. arXiv:2107.03374, 2021.
    
    \item Austin, J., et al. Program Synthesis with Large Language Models. arXiv:2108.07732, 2021.
    
    \item Kaplan, J., et al. Scaling Laws for Neural Language Models. arXiv:2001.08361, 2020.
    
    \item Hoffmann, J., et al. Training Compute-Optimal Large Language Models. NeurIPS, 2022.
    
    \item Wang, H., et al. BitNet: Scaling 1-bit Transformers for Large Language Models. arXiv:2310.11453, 2023.
    
    \item Ma, S., et al. The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits. arXiv:2402.17764, 2024.
    
    \item Wang, J., et al. 1-bit AI Infra: Part 1.1, Fast and Lossless BitNet b1.58 Inference on CPUs. arXiv:2410.16144, 2024.
    
    \item Ma, S., et al. BitNet a4.8: 4-bit Activations for 1-bit LLMs. arXiv:2411.04965, 2024.
    
    \item Microsoft. Bitnet.cpp: Efficient Edge Inference for Ternary LLMs. arXiv:2502.11880, 2025.
\end{enumerate}

%===========================================
\section*{附录A：代码示例}
%===========================================

\subsection*{A.1 GRPO实现伪代码}

\begin{lstlisting}[language=Python, caption=GRPO Training Loop]
import torch
import torch.nn.functional as F

class GRPOTrainer:
    def __init__(self, model, ref_model, tokenizer, config):
        self.model = model
        self.ref_model = ref_model
        self.tokenizer = tokenizer
        self.config = config
        
    def compute_advantages(self, rewards):
        """Compute advantages using group normalization"""
        mean = rewards.mean()
        std = rewards.std() + 1e-8
        advantages = (rewards - mean) / std
        return advantages
    
    def compute_kl_penalty(self, logits, ref_logits):
        """Compute KL divergence penalty"""
        log_probs = F.log_softmax(logits, dim=-1)
        ref_log_probs = F.log_softmax(ref_logits, dim=-1)
        kl = (torch.exp(log_probs) * (log_probs - ref_log_probs)).sum(-1)
        return kl.mean()
    
    def train_step(self, questions, group_size=16):
        """Single training step"""
        all_responses = []
        all_rewards = []
        
        # Sampling phase
        for q in questions:
            responses = self.sample_group(q, group_size)
            rewards = self.compute_rewards(q, responses)
            all_responses.extend(responses)
            all_rewards.extend(rewards)
        
        # Compute advantages
        rewards_tensor = torch.tensor(all_rewards)
        advantages = self.compute_advantages(rewards_tensor)
        
        # Policy update
        for epoch in range(self.config.num_epochs):
            loss = self.compute_loss(
                questions, all_responses, advantages
            )
            loss.backward()
            self.optimizer.step()
            self.optimizer.zero_grad()
        
        return loss.item()
\end{lstlisting}

\subsection*{A.2 奖励函数实现}

\begin{lstlisting}[language=Python, caption=Math Problem Reward Function]
import re
from sympy import simplify, sympify

def extract_answer(response):
    """Extract answer from response"""
    # Try to match \boxed{} format
    match = re.search(r'\\boxed\{([^}]+)\}', response)
    if match:
        return match.group(1)
    
    # Try to match "The answer is" format
    match = re.search(r'answer\s*(?:is|:)\s*([^\s]+)', response)
    if match:
        return match.group(1)
    
    return None

def compare_answers(pred, gold, tolerance=1e-6):
    """Compare if answers are correct"""
    # Try numerical comparison
    try:
        pred_val = float(pred)
        gold_val = float(gold)
        return abs(pred_val - gold_val) < tolerance
    except:
        pass
    
    # Try symbolic comparison
    try:
        pred_expr = sympify(pred)
        gold_expr = sympify(gold)
        return simplify(pred_expr - gold_expr) == 0
    except:
        pass
    
    # String comparison
    return pred.strip() == gold.strip()

def compute_math_reward(question, response, gold_answer):
    """Compute math problem reward"""
    pred_answer = extract_answer(response)
    
    if pred_answer is None:
        return -0.5  # Format error penalty
    
    if compare_answers(pred_answer, gold_answer):
        return 1.0  # Correct
    else:
        return 0.0  # Incorrect
\end{lstlisting}

%===========================================
\section*{附录B：超参数详细说明}
%===========================================

\begin{table}[H]
\centering
\caption{完整超参数配置表}
\begin{tabular}{llp{6cm}}
\toprule
\textbf{参数} & \textbf{典型值} & \textbf{说明} \\
\midrule
\multicolumn{3}{l}{\textbf{模型相关}} \\
max\_length & 4096-32768 & 最大生成长度 \\
temperature & 0.7-1.0 & 采样温度 \\
top\_p & 0.9-0.95 & Nucleus采样参数 \\
\midrule
\multicolumn{3}{l}{\textbf{GRPO相关}} \\
group\_size & 16-64 & 每个问题采样的回答数 \\
clip\_epsilon & 0.1-0.3 & PPO裁剪参数 \\
kl\_coef & 0.001-0.1 & KL惩罚系数 \\
\midrule
\multicolumn{3}{l}{\textbf{训练相关}} \\
learning\_rate & 1e-7-1e-5 & 学习率 \\
batch\_size & 256-1024 & 批大小 \\
num\_epochs & 1-4 & 每批数据训练轮数 \\
warmup\_steps & 100-500 & 预热步数 \\
grad\_clip & 1.0 & 梯度裁剪阈值 \\
\midrule
\multicolumn{3}{l}{\textbf{优化器相关}} \\
optimizer & AdamW & 优化器类型 \\
weight\_decay & 0.01-0.1 & 权重衰减 \\
beta1 & 0.9 & Adam beta1 \\
beta2 & 0.95-0.999 & Adam beta2 \\
\bottomrule
\end{tabular}
\end{table}

%===========================================
\section*{附录C：算法复杂度分析}
%===========================================

\begin{table}[H]
\centering
\caption{各算法复杂度对比}
\begin{tabular}{lccc}
\toprule
\textbf{算法} & \textbf{时间复杂度} & \textbf{空间复杂度} & \textbf{采样需求} \\
\midrule
PPO & $O(NK \cdot T \cdot M)$ & $O(M + M')$ & 在线 \\
GRPO & $O(NGK \cdot T \cdot M)$ & $O(M)$ & 在线 \\
DPO & $O(N \cdot T \cdot M)$ & $O(M)$ & 离线 \\
RLAIF & $O(NK \cdot T \cdot (M + M_j))$ & $O(M + M_j)$ & 在线 \\
\bottomrule
\end{tabular}
\end{table}

其中：
\begin{itemize}
    \item $N$：批大小
    \item $K$：更新轮数
    \item $T$：序列长度
    \item $M$：策略模型参数量
    \item $M'$：值网络参数量
    \item $M_j$：评判模型参数量
    \item $G$：组大小（GRPO）
\end{itemize}

%===========================================
\section*{附录D：常见问题解答（FAQ）}
%===========================================

\textbf{Q1：GRPO和PPO哪个更好？}

A：取决于具体场景。GRPO实现简单、内存效率高，适合资源受限或快速迭代场景。PPO理论基础更扎实，在有足够资源时可能表现更稳定。建议先尝试GRPO，如有问题再考虑PPO。

\textbf{Q2：需要多少数据才能训练出推理能力？}

A：根据DeepSeek-R1的经验，冷启动SFT只需要几千条高质量数据，RL阶段使用的问题集可以更大（数万到数十万）。关键是数据质量和答案验证的准确性。

\textbf{Q3：如何判断训练是否正常？}

A：关注以下指标：
\begin{itemize}
    \item 奖励应该稳步上升（但不应该突然跳跃）
    \item KL散度保持在合理范围（通常$<10$）
    \item 生成内容质量人工抽检
    \item 在验证集上的准确率
\end{itemize}

\textbf{Q4：小模型也能训练推理能力吗？}

A：可以，但效果与模型大小相关。DeepSeek的蒸馏实验表明，7B模型也能获得相当的推理能力。小模型可能需要更长的训练和更高质量的数据。

\textbf{Q5：如何避免reward hacking？}

A：
\begin{itemize}
    \item 使用多个独立的奖励信号
    \item KL散度约束
    \item 定期人工审核高奖励样本
    \item 对抗性测试
    \item 设置合理的奖励上限
\end{itemize}

\textbf{Q6：RL训练需要多少计算资源？}

A：取决于模型大小和训练规模。7B模型的RL训练通常需要8-16张A100 GPU，671B模型可能需要数百张。采样是主要瓶颈，可以通过并行化加速。

\textbf{Q7：可以在消费级GPU上训练吗？}

A：通过LoRA/QLoRA等参数高效方法，可以在单张24GB显存GPU上对7B模型进行RL训练，但速度会较慢。建议至少使用40GB显存的GPU以获得合理的训练效率。

\textbf{Q8：BitNet模型可以通过量化现有FP16模型得到吗？}

A：不可以。BitNet模型必须从头训练，无法通过后训练量化（PTQ）从现有FP16/BF16模型转换得到。这是因为：(1) FP16权重包含的信息无法无损压缩到1.58-bit；(2) FP16模型的权重分布不适合直接量化为三值；(3) BitNet在训练过程中学会利用三值权重的表达能力。实验表明，对FP16模型进行三值量化会导致严重的性能下降。

\textbf{Q9：BitNet训练需要什么硬件？推理呢？}

A：训练时，BitNet仍需要传统GPU（如NVIDIA A100/H100），因为需要维护高精度主权重和进行梯度计算，内存需求与FP16模型相近。但推理时，BitNet可以在普通CPU上高效运行，内存占用降低7倍，能耗降低70-80\%，100B参数模型可在单CPU上以人类阅读速度运行。

%===========================================
\section*{附录E：术语表}
%===========================================

\begin{table}[H]
\centering
\begin{tabular}{lp{10cm}}
\toprule
\textbf{术语} & \textbf{定义} \\
\midrule
RLHF & Reinforcement Learning from Human Feedback，基于人类反馈的强化学习 \\
RLAIF & RL from AI Feedback，基于AI反馈的强化学习 \\
PPO & Proximal Policy Optimization，近端策略优化 \\
GRPO & Group Relative Policy Optimization，组相对策略优化 \\
DPO & Direct Preference Optimization，直接偏好优化 \\
PRM & Process Reward Model，过程奖励模型 \\
ORM & Outcome Reward Model，结果奖励模型 \\
CoT & Chain-of-Thought，思维链 \\
GAE & Generalized Advantage Estimation，广义优势估计 \\
KL & Kullback-Leibler散度 \\
SFT & Supervised Fine-Tuning，监督微调 \\
MDP & Markov Decision Process，马尔可夫决策过程 \\
BitNet & 1比特/1.58比特大语言模型架构，使用三值权重\{-1, 0, 1\} \\
BitLinear & BitNet中替代nn.Linear的量化线性层 \\
STE & Straight-Through Estimator，直通估计器，用于量化训练 \\
PTQ & Post-Training Quantization，后训练量化 \\
\bottomrule
\end{tabular}
\end{table}

\end{document}