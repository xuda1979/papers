@article{dettmers2022gptq,
  title={GPTQ: Accurate post-training quantization for generative pre-trained transformers},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2210.17323},
  year={2022}
}

@article{wang2023bitnet,
  title={BitNet: Scaling 1-bit Transformers for Large Language Models},
  author={Wang, Hongyu and Ma, Shuming and Dong, Li and Huang, Shaohan and Zhang, Dong and others},
  journal={arXiv preprint arXiv:2310.11453},
  year={2023}
}

@article{frantar2022optimal,
  title={Optimal brain compression: A framework for accurate post-training quantization and pruning},
  author={Frantar, Elias and Alistarh, Dan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={4475--4488},
  year={2022}
}

@article{huang2024billm,
  title={BiLLM: Pushing the Limit of Post-Training Quantization for LLMs},
  author={Huang, Wei and others},
  journal={arXiv preprint arXiv:2402.04291},
  year={2024}
}
