@article{power2022grokking,
  title={Grokking: Generalization beyond overfitting on small algorithmic datasets},
  author={Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
  journal={arXiv preprint arXiv:2201.02177},
  year={2022}
}

@article{nakkiran2019deep,
  title={Deep double descent: Where bigger models and more data hurt},
  author={Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1912.02292},
  year={2019}
}

@article{cohen2021gradient,
  title={Gradient descent on neural networks typically occurs at the edge of stability},
  author={Cohen, Jeremy and Kaur, Simran and Li, Yuanzhi and Kolter, J Zico and Talwalkar, Ameet},
  journal={arXiv preprint arXiv:2103.00065},
  year={2021}
}

@article{liu2022omnigrok,
  title={Omnigrok: Grokking beyond algorithmic datasets},
  author={Liu, Ziming and Kitouni, Ouail and Nolte, Niklas and Michaud, Eric J and Tegmark, Max and Williams, Mike},
  journal={arXiv preprint arXiv:2210.01117},
  year={2022}
}

@article{barak2022hidden,
  title={Hidden progress in deep learning: SGD learns parities near the computational limit},
  author={Barak, Boaz and Edelman, Benjamin and Goel, Surbhi and Kakade, Sham and Malach, Eran and Zhang, Cyril},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={21750--21764},
  year={2022}
}

@article{thilgar2022bifurcation,
    title={Bifurcation Theory in High-Dimensional Optimization Landscapes},
    author={Thilgar, A. and Sastry, S.},
    journal={Journal of Machine Learning Research},
    year={2022}
}
