\documentclass[11pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{hyperref}

\geometry{margin=1in}

\title{The Embryology of Intelligence: Tracking the Topological Evolution of Knowledge Circuits During Training}
\author{Research Group on Theoretical AI}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Mechanistic interpretability has traditionally focused on reverse-engineering fully trained models. However, the static analysis of "adult" networks obscures the complex dynamics of capability acquisition. This paper introduces "Developmental Interpretability," a framework for tracking the emergence and evolution of neural circuits throughout the training trajectory. We focus on the "grokking" phenomenon—the delayed phase transition from memorization to generalization—and investigate whether knowledge circuits evolve linearly or undergo distinct topological rewiring. By applying dynamic circuit discovery techniques and network centrality metrics to small transformers trained on algorithmic tasks, we aim to characterize the "embryology" of intelligence, identifying precursor circuits that predict the emergence of advanced capabilities.
\end{abstract}

\section{Introduction}
Deep learning models are typically treated as black boxes that are "found" rather than "made." We initialize them, train them, and then attempt to understand the final artifact. This approach misses the critical temporal dimension of learning. Just as developmental biology explains the structure of an organism through its embryogenesis, understanding the training dynamics of neural networks is crucial for explaining their final behavior \citep{nanda2023progress}.

A key mystery in this domain is "grokking," where a model achieves perfect training accuracy long before it generalizes to the validation set \citep{power2022grokking}. This suggests a hidden reorganization of internal representations. We propose that this reorganization involves a topological shift in the attention head interaction graph, moving from a dense, memorization-heavy structure to a sparse, algorithm-implementing structure.

\section{Methodology: The Embryological Approach}

\subsection{High-Frequency Checkpointing}
We train 1-layer and 2-layer transformers on modular arithmetic tasks ($a + b \pmod p$). Unlike standard training runs where checkpoints are sparse, we save the model state every 10 gradient steps during critical phase transitions. This high temporal resolution allows us to observe the continuous evolution of weights and activations.

\subsection{Dynamic Circuit Discovery}
We employ Sparse Autoencoders (SAEs) \citep{bricken2023monosemanticity} to decompose the activation space of the MLP layers into interpretable features. We track these features across time:
\begin{itemize}
    \item \textbf{Feature Persistence}: Do features found at step $t$ persist to step $t+k$?
    \item \textbf{Polysemanticity Evolution}: Does the number of concepts activated by a single neuron decrease over time (differentiation)?
\end{itemize}

\subsection{Network Centrality Analysis}
We model the transformer as a directed graph where nodes are attention heads and edges represent the composition of information (Q-K-V circuits). We calculate centrality metrics (Degree, Betweenness, Eigenvector) to identify "hub" heads. We hypothesize that "Induction Heads" \citep{olsson2022context} emerge as central hubs exactly at the onset of the grokking phase.

\section{Theoretical Hypotheses}

\subsection{The Pruning Hypothesis}
We postulate that during the memorization phase, the network utilizes a broad, distributed set of circuits to "cram" data. During the generalization phase, the network "prunes" these inefficient circuits, settling into a low-entropy configuration that implements the ground-truth algorithm (e.g., the Fourier transform for modular addition).

\subsection{Precursor Circuits}
We aim to identify "precursor circuits"—structures that are functionally useless for the task but serve as necessary scaffolding for the final generalized circuit. Identifying these could allow for the early detection of capabilities before they are fully realized.

\section{Preliminary Observations}
Initial experiments on modular addition suggest a three-phase evolution:
\begin{enumerate}
    \item \textbf{Chaos}: Random weight updates, high loss.
    \item \textbf{Memorization}: Formation of disconnected "lookup table" circuits.
    \item \textbf{Crystallization}: Rapid formation of the specific "trig head" circuit that implements the modular addition algorithm, coincident with the collapse of the lookup circuits.
\end{enumerate}

\section{Conclusion}
Developmental Interpretability offers a new lens for AI safety and efficiency. by mapping the trajectory of circuit formation, we can move from reactive auditing of trained models to proactive monitoring of training dynamics. This "embryology of intelligence" may ultimately allow us to steer the development of models away from dangerous capabilities and towards robust, interpretable generalizations.

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
