\documentclass[11pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{listings}

\geometry{margin=1in}

\title{The Formal-Functional Competence Gap: A Neurosymbolic Analysis of Intermediate Representations in Large Language Models}
\author{Research Group on Theoretical AI}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
The debate over whether Large Language Models (LLMs) possess genuine understanding relies heavily on the distinction between formal linguistic competence (knowledge of grammar and form) and functional linguistic competence (reasoning and world modeling). This paper investigates the "Formal-Functional Competence Gap," positing that LLMs often rely on syntactic heuristics that mimic functional competence without implementing it. We introduce the "Intermediate Language Challenge," demonstrating that the choice of formal representation (e.g., natural language vs. logic vs. code) fundamentally alters a model's reasoning trajectory. Through syntactic perturbation experiments and neurosymbolic translation tasks, we derive a "Translation Distance" metric that predicts reasoning degradation based on the divergence between the problem's intermediate representation and the model's pre-training distribution.
\end{abstract}

\section{Introduction}
Large Language Models exhibit a striking paradox: they can generate fluent, grammatically complex prose (high formal competence) while often failing at simple logical deductions or causal reasoning (low functional competence) \citep{mahowald2024dissociating}. This dissociation suggests that the mechanisms for language production and thought generation are distinct, or at least only partially overlapping, in the transformer architecture.

Recent work has shown that "prompting" strategies like Chain-of-Thought (CoT) \citep{wei2022chain} bridge this gap by forcing the model to externalize its reasoning steps. However, the \textit{format} of this externalization matters. Why does reasoning in Python often outperform reasoning in English for certain tasks? We argue that this is a matter of "Prompt Linguistics"—the study of how specific formal languages align with the model's internal computational primitives.

\section{Theoretical Framework}

\subsection{The Competence Gap}
We define Formal Competence ($C_{form}$) as the probability of generating a sequence $S$ that adheres to the grammatical rules of language $L$. We define Functional Competence ($C_{func}$) as the probability that $S$ correctly solves a task $T$ defined by world state $W$.
The gap $G = C_{form} - C_{func}$ represents the degree to which the model is "bullshitting"—producing plausible but grounded-less text.

\subsection{Syntactic Heuristics}
Models often learn shortcuts, associating specific syntactic templates with semantic labels. For instance, the structure "Who is the president of [Country]?" strongly triggers a named-entity retrieval head. If we perturb the semantics while keeping the syntax ("Who is the president of the moon?"), the model may hallucinate a plausible-sounding name rather than stating the null hypothesis, revealing a reliance on the syntactic template \citep{mccoy2019right}.

\section{Methodology}

\subsection{Syntactic Perturbation Stress-Testing}
We construct a dataset of "Nonsense Reasoning" problems.
\begin{itemize}
    \item \textbf{Control}: "If it rains, the ground is wet. It rains. Is the ground wet?"
    \item \textbf{Perturbed}: "If the blorp zigs, the flim is wet. The blorp zigs. Is the flim wet?"
\end{itemize}
Ideally, a functionally competent model should solve both equally well (as the logic is identical: Modus Ponens). A gap in performance indicates reliance on semantic priors rather than abstract logical rules.

\subsection{Neurosymbolic Translation}
We test the model's ability to translate natural language problems into various formal intermediate representations (IRs):
\begin{itemize}
    \item \textbf{Logic}: First-Order Logic (FOL).
    \item \textbf{Code}: Python.
    \item \textbf{Database}: SQL.
    \item \textbf{Declarative}: Prolog.
\end{itemize}
We measure the "Execution Accuracy" of the generated IRs. We hypothesize that code (Python) serves as a superior IR for procedural reasoning because its strict syntax forces the model to resolve ambiguities that natural language leaves open.

\section{Proposed Metric: Translation Distance}
We propose a theoretical metric, $D_{trans}(L_{source}, L_{target})$, which quantifies the "effort" required for the model's attention heads to map tokens from the natural language distribution to the target formal language. This distance is a function of the cross-entropy between the model's internal activation patterns when processing $L_{source}$ versus $L_{target}$.

\section{Implications}
Understanding this gap transforms "Prompt Engineering" into a principled science. Instead of guessing, we can analytically determine the optimal IR for a given task. If a task requires strict sequential logic, we should prompt the model to translate to Python. If it requires relational inference, we should translate to SQL. This "Polyglot Prompting" strategy leverages the formal competence of the model to bootstrap its functional competence.

\section{Conclusion}
The Formal-Functional Competence Gap is a critical barrier to AGI. By rigorously mapping the boundaries of what models "know" versus what they "can say," and by utilizing appropriate intermediate languages to bridge this divide, we can build more robust and reasoning-capable AI systems.

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
