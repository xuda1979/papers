% AG-QEC: AI-Guided Quantum Error Correction Under Heterogeneous Noise
% Minimal, self-contained LaTeX that compiles on arXiv/TeXLive
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{siunitx}
\usepackage{microtype}
\usepackage{enumitem}
\usepackage{float}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue}

\title{AG-QEC: AI-Guided Quantum Error Correction Under Heterogeneous Noise}
\author{David Xu}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
We study \emph{AI-guided quantum error correction} (AG-QEC) under heterogeneous and drifting noise.
We present a minimal but fully reproducible pipeline that (i) generates syndrome data under standard phenomenological noise models, (ii) evaluates classical baselines (e.g., majority vote on a repetition code), and (iii) compares against a lightweight AI-style decoder that is \emph{calibrated from data} rather than analytically designed. 
Our goal is to isolate and quantify the practical benefits of learned heuristics (robustness to distribution shift, label efficiency) while keeping enough structure for exact reproducibility.
\end{abstract}

\paragraph{Contributions.}
\begin{itemize}[leftmargin=*]
  \item \textbf{Reproducible simulator \& CLI.} A single-file simulator (\texttt{simulation.py}) with deterministic seeds that emits raw artifacts (config, metrics, optional syndromes) and supports bit-flip, phase-flip, and depolarizing noise with optional measurement error.
  \item \textbf{Baselines and an AI-style decoder.} Majority-vote decoding for the repetition code (baseline) and a tiny, data-calibrated threshold decoder that uses syndrome counts as a sufficient statistic.
  \item \textbf{Evaluation protocol.} Logical error vs.\ physical error at distances $d\in\{3,5,7\}$; robustness to calibration drift by testing at a mismatched noise parameter; and a lightweight threshold-estimation procedure with no heavy ML dependencies.
\end{itemize}

\section{Introduction}
Quantum error correction (QEC) protects logical information from physical noise by repeatedly extracting \emph{syndromes} and applying \emph{decoding} decisions. Classical decoders such as minimum-weight perfect matching (MWPM) are well studied for specific codes, but practical deployments face hardware-specific, drifting, and correlated noise. This paper contributes a compact, reproducible baseline to examine whether simple learned heuristics---here, an AI-style \emph{threshold} decoder---offer robustness advantages over fixed rules under modest distribution shift.

\section{Background}
\paragraph{Codes.} We focus on the repetition code, a pedagogical limit where phase or bit errors dominate, because it allows a complete, closed-form simulator and clear analysis of learned calibration. (Extending the CLI to surface/color/LDPC via external packages is future work.)

\paragraph{Noise models.} We consider independent Pauli channels per qubit per round:
\begin{itemize}[leftmargin=*]
  \item \textbf{Bit flip:} $X$ with probability $p_X$ (others zero).
  \item \textbf{Phase flip:} $Z$ with probability $p_Z$ (others zero).
  \item \textbf{Depolarizing:} $X,Y,Z$ each with probability $p/3$.
\end{itemize}
Measurement outcomes are independently flipped with probability $p_{\mathrm{meas}}$ (phenomenological model).

\section{Method: AG-QEC (data-calibrated threshold decoding)}
Given an encoded logical bit $L\in\{0,1\}$, the repetition code maps $L \mapsto L^d$ across $d$ physical qubits. After noise and measurement, we compute the \emph{syndrome vector} $s\in\{0,1\}^{d-1}$ with entries $s_i = m_i \oplus m_{i+1}$. 
The baseline \textbf{majority} decoder estimates $\hat L$ as the majority of $m_1,\dots,m_d$.
Our AI-style decoder uses the statistic $S=\sum_i s_i$ and a \emph{calibrated threshold} $t^\star$ learned on a calibration split, predicting $\hat L = \mathbb{1}[S \ge t^\star]$.\footnote{Variants include separate thresholds per distance, noise family, or temperature-like parameters controlling model confidence.}

\section{Evaluation protocol}
\paragraph{Metrics.} We report the \emph{logical error rate}, i.e., $\Pr(\hat L \ne L)$, estimated over i.i.d.\ trials with binomial confidence intervals.

\paragraph{Curves.} For each code distance $d\in\{3,5,7\}$ and noise family, sweep the physical error rate $p$ in a small grid (e.g., $p\in\{0.01,0.03,0.05,0.07,0.09\}$). For robustness-to-drift, calibrate the threshold at $p_{\text{train}}$ and evaluate at $p_{\text{test}}\ne p_{\text{train}}$.

\paragraph{Latency \& complexity.} Both decoders are $O(d)$ per round, enabling fast, CPU-only evaluation.

\section{Results (template)}
Figure~\ref{fig:pipeline} shows the pipeline. Tables~\ref{tab:thresholds}--\ref{tab:results} summarize threshold calibration and logical error rates across distances. (Populate by running the CLI; see Appendix~\ref{app:cli}.)

\begin{figure}[H]
  \centering
  % Placeholder rectangle instead of heavy TikZ/diagram assets
  \fbox{\rule{0pt}{0.30\linewidth}\rule{0.95\linewidth}{0pt}}
  \caption{AG-QEC evaluation pipeline: noise $\to$ syndrome $\to$ decoder $\to$ logical outcome.}
  \label{fig:pipeline}
\end{figure}

\begin{table}[H]
  \centering
  \caption{Example threshold calibration summary (fill with CLI outputs).}
  \begin{tabular}{llll}
  \toprule
  Distance $d$ & Noise & $p$ & Calibrated $t^\star$ \\
  \midrule
  5 & bitflip & 0.05 & 2 \\
  7 & depol   & 0.06 & 3 \\
  \bottomrule
  \end{tabular}
  \label{tab:thresholds}
\end{table}

\begin{table}[H]
  \centering
  \caption{Logical error rates (illustrative schema).}
  \begin{tabular}{lllll}
  \toprule
  $d$ & noise & $p$ & Majority & Threshold (AG-QEC) \\
  \midrule
  5 & bitflip & 0.05 & 0.071 & 0.066 \\
  7 & depol   & 0.06 & 0.092 & 0.084 \\
  \bottomrule
  \end{tabular}
  \label{tab:results}
\end{table}

\section{Related work}
Recent experimental and systems work has highlighted end-to-end latency and hardware integration for decoders, as well as software simulation toolchains. See, e.g., real-time decoding on superconducting hardware~\cite{rigetti_realtime_qec_2024}, measurement-free and hardware-aware QEC schemes~\cite{locher_mf_ft_2024}, transversal/low-overhead FT strategies~\cite{transversal_2024}, and open-source stabilizer simulators like \texttt{qecsim}~\cite{tuckett_qecsim}.

\section{Reproducibility checklist}
\begin{itemize}[leftmargin=*]
  \item \textbf{Deterministic runs:} set \texttt{--seed} (default shown in CLI).
  \item \textbf{Artifact logging:} the simulator writes \texttt{config.json}, \texttt{metrics.json}, and (optionally) \texttt{syndromes.csv} into a timestamped folder.
  \item \textbf{Exact command lines:} record CLI invocations in the paper (Appendix~\ref{app:cli}).
  \item \textbf{Environment:} Python $\ge 3.9$, NumPy only (no heavy ML deps).
\end{itemize}

\section{Conclusion}
We provided a compact, fully reproducible baseline to probe whether simple learned calibration can improve robustness of QEC decoding under mild noise mismatch, while keeping the code path transparent and auditable. The framework is intentionally minimal; adding more realistic codes and decoders (e.g., surface code + MWPM) can be layered on without changing the evaluation harness.

\appendix
\section{Notation}\label{app:notation}
\begin{table}[H]
  \centering
  \begin{tabular}{ll}
  \toprule
  Symbol & Meaning\\
  \midrule
  $d$ & Code distance (repetition length)\\
  $p_X,p_Y,p_Z$ & Pauli error probabilities\\
  $p_{\mathrm{meas}}$ & Measurement flip probability\\
  $s\in\{0,1\}^{d-1}$ & Syndrome vector ($s_i = m_i \oplus m_{i+1}$)\\
  $S=\sum_i s_i$ & Syndrome count statistic\\
  $t^\star$ & Calibrated threshold on $S$\\
  \bottomrule
  \end{tabular}
\end{table}

\section{Command-line interface}\label{app:cli}
\begin{lstlisting}[language=bash,basicstyle=\ttfamily\small]
# Example: distance 5, bit-flip noise, 100k shots, majority + threshold decoders
python simulation.py \
  --code repetition --distance 5 \
  --noise bitflip --p-x 0.05 --p-meas 0.02 \
  --n-samples 100000 --n-rounds 1 \
  --decoder threshold --calib-frac 0.1 --seed 123
\end{lstlisting}

\begingroup
\small
\begin{thebibliography}{9}
\bibitem{rigetti_realtime_qec_2024}
Y. P. Kalachev \emph{et al.}, ``Demonstrating real-time and low-latency quantum error correction with an FPGA decoder,'' \emph{arXiv:2410.05202}, 2024.
\bibitem{locher_mf_ft_2024}
S. Locher and M. M\"uller, ``Measurement-Free Fault-Tolerant Quantum Error Correction in Near-Term Devices,'' \emph{PRX Quantum}, 5:010333, 2024.
\bibitem{transversal_2024}
J. Beverland \emph{et al.}, ``Low-Overhead Transversal Fault Tolerance for Universal Quantum Computation,'' \emph{arXiv:2406.17653}, 2024.
\bibitem{tuckett_qecsim}
D. K. Tuckett, ``qecsim: a Python package for simulating quantum error correction,'' project documentation, accessed 2025, \url{https://qecsim.github.io/}.
\end{thebibliography}
\endgroup

\end{document}
