\documentclass{article}
\usepackage{amsmath, amssymb, amsthm, geometry}
\usepackage{graphicx}
\usepackage{microtype}
\usepackage{booktabs}
\usepackage{tikz-cd}
\usepackage{hyperref}
\usepackage{natbib}

\geometry{a4paper, margin=1in}

\title{Categorical Limits of Reasoning: A No-Go Theorem for Fixed-Depth Transformers}
\author{Jules the AI}
\date{\today}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\begin{document}

\maketitle

\begin{abstract}
While Transformers have achieved remarkable success in language tasks, their ability to perform compositional reasoning remains constrained by their fixed-depth architecture. We propose a rigorous framework based on Category Theory to model the reasoning capabilities of Large Language Models (LLMs). We define reasoning tasks as finding the Initial Algebra of a specific endofunctor and model the Transformer as a composition of bounded functors. We prove a ``No-Go Theorem'' demonstrating that fixed-depth Transformers cannot represent the solution to recursive problems (such as graph connectivity or transitive closure) for unbounded inputs, as they can only approximate finite truncations of the required limit. This provides a structural justification for the necessity of "Chain-of-Thought" prompting, which we formalize as iteratively applying the functor to approach the fixpoint.
\end{abstract}

\section{Introduction}
The Transformer architecture \citep{vaswani2017attention} has revolutionized Natural Language Processing. However, despite their scale, these models exhibit fundamental limitations in logical reasoning, particularly for tasks requiring recursion or multi-step deduction on inputs of arbitrary size. Theoretical analyses have linked Transformers to the circuit complexity class $\mathsf{TC}^0$ \citep{merrill2023saturation}, implying they cannot solve problems requiring logarithmic depth, such as parity or graph connectivity, on unbounded inputs.

In this work, we elevate this analysis to the level of \textbf{Category Theory}. By treating reasoning tasks as the computation of \textit{Catamorphisms} on algebraic data types (initial algebras), we show that a fixed-depth network corresponds to a functor that can only unfold the recursion a finite number of times. This creates a "categorical gap" between the model's expressivity and the problem's structure.

Our main contributions are:
\begin{enumerate}
    \item A formal categorical framework where reasoning tasks are initial algebras of endofunctors.
    \item A rigorous "No-Go Theorem" proving the impossibility of solving recursive tasks with fixed-depth composition.
    \item A theoretical justification for Chain-of-Thought (CoT) \citep{wei2022chain} as an externalized fixpoint iteration.
\end{enumerate}

\section{Categorical Framework}

We model the "reasoning space" using category theory. Let $\mathbf{Set}$ be the category of sets and functions.

\subsection{Reasoning as Initial Algebras}
Many logical tasks can be framed as finding a fixed point. Consider a recursive problem defined by a transformation.

\begin{definition}[Reasoning Endofunctor]
Let $T: \mathcal{C} \to \mathcal{C}$ be an endofunctor representing a single step of logical deduction or state transition. For a graph connectivity problem, $\mathcal{C}$ might be the category of adjacency matrices, and $T$ maps a matrix $A$ to $A + A^2$ (or similar logic).
\end{definition}

The "solution" to the recursive problem is often the \textit{Initial Algebra} $(\mu T, in)$ of the functor $T$, or the least fixed point.

\begin{definition}[Catamorphism]
Given an algebra $(A, \alpha: TA \to A)$, the unique homomorphism from the initial algebra $(\mu T, in)$ to $(A, \alpha)$ is the catamorphism $\llparenthesis \alpha \rrparenthesis$. This represents the "fold" operation that consumes the recursive structure to produce a result.
\end{definition}

\subsection{Transformers as Bounded Functors}
A Transformer of depth $L$ computes a function $f: X \to Y$ that is a composition of $L$ layers: $f = \lambda_L \circ \dots \circ \lambda_1$.

\begin{definition}[Bounded Composition]
Let $\mathbf{Transf}_L$ be the subcategory of functions computable by composing at most $L$ attention and feed-forward blocks (with appropriate non-linearities and bounded precision).
\end{definition}

Crucially, the operation of a single layer can be seen as applying a local update rule. However, computing the global fixed point requires iterating this rule a number of times that depends on the input size $N$ (specifically, the diameter of the graph).

\section{The No-Go Theorem}

We focus on the problem of \textbf{Transitive Closure} (Graph Connectivity), which is the canonical example of a problem requiring recursion depth proportional to the input size.

\begin{theorem}[No-Go Theorem for Fixed-Depth Reasoning]
\label{thm:nogo}
Let $\mathcal{P}_{conn}$ be the problem of determining if two nodes in a graph $G$ of size $N$ are connected. This problem corresponds to computing the limit of the sequence $A, A^2, A^4, \dots, A^N$ (in the boolean semiring).
A Transformer of fixed depth $L$ cannot solve $\mathcal{P}_{conn}$ for all $N > 2^L$.
\end{theorem}

\begin{proof}
We formulate the proof using the properties of the endofunctor $T(X) = I + A \cdot X$ (where $+$ and $\cdot$ are boolean matrix operations).
The transitive closure $A^*$ is given by the limit of the chain:
\[ A^{(0)} \to A^{(1)} \to A^{(2)} \to \dots \to A^{(N)} = A^* \]
where $A^{(k)}$ represents paths of length at most $k$. The minimal depth required to compute $A^{(k)}$ using standard matrix multiplication logic (which Attention approximates) grows logarithmically with $k$ if we use exponentiation by squaring, or linearly if we iterate neighbors.

Let the Transformer be modeled as a functor $F_{Tr}: \mathbf{Graph} \to \mathbf{Bool}$. If the depth is fixed at $L$, the maximum "logical distance" information can propagate is bounded by $2^L$ (assuming exponentiation behavior in the best case) or $L$ (linear behavior).

Formally, the initial algebra requires the colimit of the chain up to the graph diameter $D$. For a general graph, $D \approx O(N)$. The fixed-depth Transformer computes a truncated approximation:
\[ \text{Output} \approx \pi_L(\text{colim}_{k \to \infty} T^k(\bot)) \]
where $\pi_L$ is a projection onto the first $L$ iterations (or $2^L$ steps).
If $N > 2^L$, there exist graphs (e.g., a path graph of length $N$) where the connectivity information between endpoints cannot be resolved within the receptive field or logical depth of the network. Thus, there is no natural transformation from the problem functor to the Transformer functor that holds for all $N$.
\end{proof}

\begin{figure}[h]
    \centering
    \begin{tikzcd}
        \text{Input } G \arrow[r, "T"] \arrow[d, "F_{Tr}"'] & T(G) \arrow[r, "T"] & \dots \arrow[r] & \mu T \text{ (Solution)} \\
        \text{Layer } 1 \arrow[r] & \text{Layer } 2 \arrow[r] & \dots \arrow[r] & \text{Output} \arrow[u, "\text{Error}"', dashed, red]
    \end{tikzcd}
    \caption{Commutative diagram illustrating the failure of a fixed-depth Transformer ($L$ steps) to reach the initial algebra $\mu T$ (the fixpoint solution) for large inputs. The dashed red arrow indicates the inevitable error or "categorical gap".}
    \label{fig:diagram}
\end{figure}

\section{Discussion: Chain-of-Thought as Unrolling}

The theorem highlights a fundamental structural mismatch. How do LLMs solve these problems in practice? The answer lies in \textbf{Chain-of-Thought (CoT)} prompting.

From a categorical perspective, generating tokens sequentially allows the model to utilize the context window as an external memory tape. Each new token generated is effectively one iteration of the endofunctor $T$.
\[ \text{State}_{t+1} = \text{Model}(\text{State}_t) \]
If the reasoning chain has length $M$, the effective depth of the computation becomes $L \times M$. If $M$ is allowed to scale with $N$ (e.g., $M \propto N$), the system can compute the full limit $\mu T$.

\begin{corollary}[Sufficiency of CoT]
A fixed-depth Transformer equipped with a scratchpad (CoT) of length $O(N)$ can solve $\mathcal{P}_{conn}$ by simulating the fixed-point iteration step-by-step.
\end{corollary}

This aligns with results by \cite{wei2022chain} and formalizes the intuition that "thinking time" (context length) is computationally equivalent to depth.

\section{Conclusion}
We have presented a "No-Go Theorem" for fixed-depth Transformers using Category Theory. By identifying recursive reasoning with the computation of initial algebras, we showed that fixed-depth architectures are structurally incapable of solving problems with unbounded dependency chains. This provides a robust theoretical foundation for the necessity of adaptive computation mechanisms like Chain-of-Thought.

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
