\documentclass{article}
\usepackage{amsmath, amssymb, amsthm, geometry}
\usepackage{graphicx}
\usepackage{microtype}
\usepackage{booktabs}
\usepackage{tikz-cd}
\usepackage{hyperref}

\geometry{a4paper, margin=1in}

\title{Categorical Limits of Reasoning: A No-Go Theorem for Fixed-Depth Transformers}
\author{Jules the AI}
\date{\today}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{proposition}[theorem]{Proposition}

\begin{document}

\maketitle

\begin{abstract}
While Transformers have achieved remarkable success in language tasks, their ability to perform compositional reasoning remains debated. We apply Category Theory to model the semantic space of Large Language Models (LLMs) and the logical structure of reasoning tasks. We prove a ``No-Go Theorem'' demonstrating that fixed-depth Transformers cannot represent the solution to certain recursive logical problems, specifically those requiring the computation of functions in a topos with infinite internal logic depth, such as checking graph connectivity or parity on unbounded inputs.
\end{abstract}

\section{Introduction}
We explore the intersection of Category Theory and Deep Learning. Can a continuous function approximator (a neural network) faithfully implement discrete, compositional logic? We answer this in the negative for specific complexity classes relative to the network depth.

\section{Categorical Framework}
\begin{definition}[Semantic Functor]
Let $\mathcal{C}$ be a category representing the syntax of a formal language, and $\mathcal{S}$ be the category of vector spaces (semantic embeddings). A semantic mapping is a functor $F: \mathcal{C} \to \mathcal{S}$.
\end{definition}

\begin{definition}[Reasoning as Morphism Composition]
A reasoning step is a morphism $f: A \to B$ in $\mathcal{C}$. A model successfully reasons if its induced map on $\mathcal{S}$ commutes with the functor $F$.
\end{definition}

\section{Main Results}

\begin{theorem}[No-Go Theorem for Recursive Reasoning]
Let $\mathcal{P}$ be a recursive problem class equivalent to computing the transitive closure of a graph (e.g., connectivity). A Transformer of fixed depth $L$ cannot uniformly solve $\mathcal{P}$ for input graphs of size $N > 2^L$.
\end{theorem}

\begin{proof}
(Sketch) We use the correspondence between Transformers and the complexity class $\mathsf{TC}^0$ (constant-depth threshold circuits). In the categorical view, we model the computation as a diagram in a Topos. The recursive operation requires the existence of a specific limit (or colimit) that corresponds to the fixed point of the recursion. We show that a fixed-depth network can only approximate finite truncations of this limit. Using Algebraic Topology, we compute the Betti numbers of the solution space and show that the network's representation cannot support the required topology for $N \gg L$, leading to inevitable failure in preserving the truth values of the logical propositions.
\end{proof}

\section{Discussion}
This result mirrors results in circuit complexity but provides a structural reason: the lack of "compositionality" in the strict categorical sense for finite-depth approximators. This suggests that "Chain-of-Thought" prompting works by effectively increasing the depth of the computation graph, unrolling the recursion externally.

\section{Conclusion}
We have established categorical limits on the reasoning capabilities of fixed-depth Transformers, proving they cannot universally solve recursive problems without external scratchpads or recurrent loops.

\bibliographystyle{plain}
\bibliography{references}

\end{document}
